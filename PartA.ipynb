{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cc09d0e-76ad-40ea-8c5f-016dc0c7ad25",
   "metadata": {},
   "source": [
    "CS4001/4042 Assignment 1\n",
    "---\n",
    "Part A, Q1 (15 marks)\n",
    "---\n",
    "\n",
    ">Design a feedforward deep neural network (DNN) which consists of **three** hidden layers of 128 neurons each with ReLU activation function, and an output layer with sigmoid activation function. Apply dropout of probability **0.2** to each of the hidden layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f452e6c-4a7f-4565-b42d-28890075e2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import time\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from scipy.io import wavfile as wav\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "from common_utils import set_seed\n",
    "\n",
    "# setting seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cf9d7a-b94d-4b39-87a2-839d21f52f87",
   "metadata": {},
   "source": [
    "Define the model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac16b16f-245a-4096-990a-521d04ee437e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enter your code here\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, first_hidden_size=None, hidden_size=128, output_size=1, dropout_prob=0.2):\n",
    "        super(MLP, self).__init__()\n",
    "        if first_hidden_size == None:\n",
    "            self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "            self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        else:\n",
    "            self.fc1 = nn.Linear(input_size, first_hidden_size)\n",
    "            self.fc2 = nn.Linear(first_hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.sigmoid(self.fc4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445d9a47-d690-4992-8151-19e7ab3c68ab",
   "metadata": {},
   "source": [
    "> Divide the dataset into a 70:30 ratio for training and testing. Use **appropriate** scaling of input features. We solely assume that there are only two datasets here: training & test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5154111-ba80-4eff-8ed9-67d3451ca515",
   "metadata": {},
   "source": [
    "Split the dataset and do preprocessing. You can use the split_dataset and preprocess_dataset provided for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee8112e-7e1c-46da-a21b-a70849d6b621",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common_utils import split_dataset, preprocess_dataset\n",
    "\n",
    "# TODO: Enter your code here\n",
    "def load_csv(file_path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(file_path)\n",
    "    filename_col = df[\"filename\"]\n",
    "    lable_col = filename_col.apply(lambda x: x.split(\".\")[0])\n",
    "    df[\"label\"] = lable_col\n",
    "    df.drop([\"filename\"], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "def load_dataframe_split(df: pd.DataFrame) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    df_train, y_train, df_test, y_test = split_dataset(df, [\"label\"], 0.3, 42)\n",
    "    df_train, df_test = preprocess_dataset(df_train, df_test)\n",
    "    return df_train, y_train, df_test, y_test\n",
    "\n",
    "df = load_csv(\"./audio_gtzan.csv\")\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_dataframe_split(df)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba7da82-1a87-4415-b1fd-c7ba7b706eb6",
   "metadata": {},
   "source": [
    "> Use the training dataset to train the model for 100 epochs. Use a mini-batch gradient descent with **‘Adam’** optimizer with learning rate of **0.001**, and **batch size = 128**. Implement early stopping with patience of **3**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19d12e7-562a-4a74-9646-4d90bdeb4367",
   "metadata": {},
   "source": [
    "1. Define a Pytorch Dataset and Dataloaders.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e86f216-50ba-4670-8cfd-bf28949e7dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enter your code here\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "        if self.X.shape[0] != self.y.shape[0]:\n",
    "            raise ValueError(\"X and y must have the same length.\")\n",
    "        if self.y.ndim == 1:\n",
    "            self.y = self.y.reshape(-1, 1)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "class AudioDataLoader(DataLoader):\n",
    "    def __init__(self, dataset, batch_size, shuffle=True):\n",
    "        super(AudioDataLoader, self).__init__(dataset, batch_size, shuffle=shuffle)\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i in range(0, len(self.dataset), self.batch_size):\n",
    "            yield self.dataset[i : i + self.batch_size]\n",
    "            \n",
    "    def __str__(self):\n",
    "        dataset_info = f\"\\tDataset length: {len(self.dataset)}, \\n\\tDataset shape: {self.dataset[0][0].shape}, \\n\\tBatch size: {self.batch_size}, \\n\\tShuffle: {self.shuffle}\"\n",
    "        return f\"AudioDataLoader(\\n{dataset_info}\\n)\\n\"\n",
    "            \n",
    "train_dataset = AudioDataset(X_train, y_train)\n",
    "test_dataset = AudioDataset(X_test, y_test)\n",
    "\n",
    "train_loader = AudioDataLoader(train_dataset, 128)\n",
    "test_loader = AudioDataLoader(test_dataset, 128)\n",
    "\n",
    "print(train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c905954-db7b-46b8-b758-ac3182872854",
   "metadata": {},
   "source": [
    "2. Next, define the model, optimizer and loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e64a6b1-e29d-4b24-87f6-d67c79589427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enter your code here\n",
    "model = MLP(57)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38afed0-48b9-4512-a6d7-28181261b9fb",
   "metadata": {},
   "source": [
    "3. Train model for 100 epochs. Record down train and test accuracies. Implement early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3b26a3-07aa-4608-b865-72b261b73079",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common_utils import EarlyStopper\n",
    "\n",
    "# TODO: Enter your code here\n",
    "early_stopper = EarlyStopper(patience=3, min_delta=1e-5)\n",
    "n_epochs = 100\n",
    "acc_train = []\n",
    "acc_test = []\n",
    "loss_train = []\n",
    "loss_test = []\n",
    "f1_train = []\n",
    "f1_test = []\n",
    "precision_train = []\n",
    "precision_test = []\n",
    "recall_train = []\n",
    "recall_test = []\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    with torch.enable_grad():\n",
    "        y_pred_list = []\n",
    "        y_true_list = []\n",
    "        for batch in train_loader:\n",
    "            X, y = batch\n",
    "            y_pred = model(X)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            y_pred_list.append(y_pred)\n",
    "            y_true_list.append(y)\n",
    "        y_pred_list = torch.cat(y_pred_list)\n",
    "        y_true_list= torch.cat(y_true_list)\n",
    "        loss_train.append(loss_fn(y_pred_list, y_true_list))\n",
    "        y_pred_list = torch.round(y_pred_list)\n",
    "        acc_train.append((y_pred_list == y_true_list).sum().item() / len(y_true_list))\n",
    "        y_pred_list, y_true_list = y_pred_list.detach().numpy(), y_true_list.detach().numpy()\n",
    "        f1_train.append(f1_score(y_true_list, y_pred_list))\n",
    "        precision_train.append(precision_score(y_true_list, y_pred_list))\n",
    "        recall_train.append(recall_score(y_true_list, y_pred_list))\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred_list = []\n",
    "        y_true_list = []\n",
    "        for batch in test_loader:\n",
    "            X, y = batch\n",
    "            y_pred = model(X)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            y_pred_list.append(y_pred)\n",
    "            y_true_list.append(y)\n",
    "        y_pred_list = torch.cat(y_pred_list)\n",
    "        y_true_list= torch.cat(y_true_list)\n",
    "        loss_test.append(loss_fn(y_pred_list, y_true_list))\n",
    "        y_pred_list = torch.round(y_pred_list)\n",
    "        acc_test.append((y_pred_list == y_true_list).sum().item() / len(y_true_list))\n",
    "        y_pred_list, y_true_list = y_pred_list.detach().numpy(), y_true_list.detach().numpy()\n",
    "        f1_test.append(f1_score(y_true_list, y_pred_list))\n",
    "        precision_test.append(precision_score(y_true_list, y_pred_list))\n",
    "        recall_test.append(recall_score(y_true_list, y_pred_list))\n",
    "\n",
    "    log_info = f\"epoch: {epoch} \\t| train loss: {loss_train[-1]:.5f} \\t| test loss: {loss_test[-1]:.5f}\"\n",
    "    print(log_info)\n",
    "\n",
    "    if early_stopper.early_stop(loss_test[-1]):\n",
    "        print(f\"early stopping at epoch {epoch}\")\n",
    "        break\n",
    "    \n",
    "loss_train = torch.tensor(loss_train)\n",
    "loss_test = torch.tensor(loss_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cde204-f469-45c3-a78e-36640b9829d1",
   "metadata": {},
   "source": [
    "> Plot train and test accuracies and losses on training and test data against training epochs and comment on the line plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f33ffc-6ce7-487a-8610-d92eaeba514d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enter your code here\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, len(acc_train)+1), acc_train, label='train')\n",
    "plt.plot(range(1, len(acc_test)+1), acc_test, label='test')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, len(loss_train)+1), loss_train, label='train')\n",
    "plt.plot(range(1, len(loss_test)+1), loss_test, label='test')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fe4c0b-c63d-4354-9141-31ac94607f77",
   "metadata": {},
   "source": [
    "\\# TODO: \\<Enter your answer here\\>\n",
    "\n",
    "The train and test accuracy both increase with epochs, with train accuracy being higher than test accuracy. And the train and test loss both decrease with epochs, with train loss being lower than test loss. The model is doing well in fitting the training data, but the test accuracy is not as high as the train accuracy. Hence, the model may be overfitting the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71e5ee7-d56a-4d1c-829e-f841d0d12518",
   "metadata": {},
   "source": [
    "Part A, Q2 (10 marks)\n",
    "---\n",
    "\n",
    "In this question, we will determine the optimal batch size for mini-batch gradient descent. Find the optimal batch size for mini-batch gradient descent by training the neural network and evaluating the performances for different batch sizes. Note: Use 5-fold cross-validation on training partition to perform hyperparameter selection. You will have to reconsider the scaling of the dataset during the 5-fold cross validation.\n",
    "\n",
    "To reduce repeated code, you may need to place the network (MLP defined in QA1) in a separate file called **common_utils.py**. Import it here for Q2. You will not be repenalised for any error in QA1 here as the code in QA1 will not be remarked. The following code cell will not be marked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c76ec9-cd41-4cec-870f-1c559af5eaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enter your code here\n",
    "\n",
    "batch_sizes = [32, 64, 128, 256]\n",
    "\n",
    "model_dict = {batch_size: MLP(57) for batch_size in batch_sizes}\n",
    "\n",
    "model_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c51b77-2743-4cb2-9169-ebca88ec57c8",
   "metadata": {},
   "source": [
    "> Plot mean cross-validation accuracies on the final epoch for different batch sizes as a scatter plot. Limit search space to batch sizes {32, 64, 128, 256}. Next, create a table of time taken to train the network on the last epoch against different batch sizes. Finally, select the optimal batch size and state a reason for your selection. This might take a while to run, so plan your time carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e29cb83-8e4b-43b4-b3f3-31162354cd30",
   "metadata": {},
   "source": [
    "1. Define different folds for different batch sizes to get a dictionary of training and validation datasets. Preprocess your datasets accordingly. Please use the following name conventions:\n",
    "    - X_train_scaled_dict[batch_size] is a list of the preprocessed training matrix for the different folds. \n",
    "    - X_val_scaled_dict[batch_size] is a list of the processed validation matrix for the different folds. \n",
    "    - y_train_dict[batch_size] and y_val_dict[batch_size] is a list of labels for the different folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d677f5-01ad-43b4-bfeb-ef1f575d7730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enter your code here\n",
    "\n",
    "X, y = X_train, y_train\n",
    "\n",
    "X_train_scaled_dict = {batch_size: None for batch_size in batch_sizes}\n",
    "X_val_scaled_dict = {batch_size: None for batch_size in batch_sizes}\n",
    "y_train_dict = {batch_size: None for batch_size in batch_sizes}\n",
    "y_val_dict = {batch_size: None for batch_size in batch_sizes}\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    X_train_folds = []\n",
    "    X_val_folds = []\n",
    "    y_train_folds = []\n",
    "    y_val_folds = []\n",
    "\n",
    "    for train_index, val_index in kf.split(X):\n",
    "        X_train_fold, X_val_fold = X[train_index], X[val_index]\n",
    "        y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
    "\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        X_train_fold = scaler.fit_transform(X_train_fold)\n",
    "        X_val_fold = scaler.transform(X_val_fold)\n",
    "\n",
    "        X_train_folds.append(X_train_fold)\n",
    "        X_val_folds.append(X_val_fold)\n",
    "        y_train_folds.append(y_train_fold)\n",
    "        y_val_folds.append(y_val_fold)\n",
    "\n",
    "    X_train_scaled_dict[batch_size] = X_train_folds\n",
    "    X_val_scaled_dict[batch_size] = X_val_folds\n",
    "    y_train_dict[batch_size] = y_train_folds\n",
    "    y_val_dict[batch_size] = y_val_folds\n",
    "    \n",
    "print(X_train_scaled_dict[32][0].shape, X_val_scaled_dict[32][0].shape, y_train_dict[32][0].shape, y_val_dict[32][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52ed444-b01b-44fd-bf8f-19df1ed70d6d",
   "metadata": {},
   "source": [
    "2. Perform hyperparameter tuning for the different batch sizes with 5-fold cross validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d615b70-ef9f-4fb3-9802-0af0fce8ec01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enter your code here\n",
    "\n",
    "time_train_dict = {batch_size: [] for batch_size in batch_sizes}\n",
    "mean_cv_acc_dict = {batch_size: [] for batch_size in batch_sizes}\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    set_seed(42)\n",
    "    model = model_dict[batch_size]\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    loss_fn = nn.BCELoss()\n",
    "\n",
    "    n_splits = 5\n",
    "    n_epochs = 200       \n",
    "\n",
    "    for i in range(n_splits):\n",
    "        X_train_ndarray = X_train_scaled_dict[batch_size][i]\n",
    "        X_val_ndarray = X_val_scaled_dict[batch_size][i]\n",
    "        y_train_ndarray = y_train_dict[batch_size][i]\n",
    "        y_val_ndarray = y_val_dict[batch_size][i]\n",
    "        train_dataset = AudioDataset(X_train_ndarray, y_train_ndarray)\n",
    "        val_dataset = AudioDataset(X_val_ndarray, y_val_ndarray)\n",
    "        train_loader = AudioDataLoader(train_dataset, batch_size)\n",
    "        val_loader = AudioDataLoader(val_dataset, batch_size)\n",
    "\n",
    "        loss_train = []\n",
    "        acc_train = []\n",
    "        time_train = []\n",
    "        loss_val = []\n",
    "        acc_val = []\n",
    "\n",
    "        early_stopper = EarlyStopper(patience=3, min_delta=1e-5)\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            start_time = time.time()\n",
    "            model.train()\n",
    "            with torch.enable_grad():\n",
    "                y_pred_list = []\n",
    "                y_true_list = []\n",
    "                for batch in train_loader:\n",
    "                    X, y = batch\n",
    "                    y_pred = model(X)\n",
    "                    loss = loss_fn(y_pred, y)\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    y_pred_list.append(y_pred)\n",
    "                    y_true_list.append(y)\n",
    "                y_pred_list = torch.cat(y_pred_list)\n",
    "                y_true_list= torch.cat(y_true_list)\n",
    "                loss_train.append(loss_fn(y_pred_list, y_true_list))\n",
    "                y_pred_list = torch.round(y_pred_list)\n",
    "                acc_train.append((y_pred_list == y_true_list).sum().item() / len(y_true_list))\n",
    "            end_time = time.time()\n",
    "            time_train.append(end_time - start_time)\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                y_pred_list = []\n",
    "                y_true_list = []\n",
    "                for batch in val_loader:\n",
    "                    X, y = batch\n",
    "                    y_pred = model(X)\n",
    "                    loss = loss_fn(y_pred, y)\n",
    "                    y_pred_list.append(y_pred)\n",
    "                    y_true_list.append(y)\n",
    "                y_pred_list = torch.cat(y_pred_list)\n",
    "                y_true_list= torch.cat(y_true_list)\n",
    "                loss_val.append(loss_fn(y_pred_list, y_true_list))\n",
    "                y_pred_list = torch.round(y_pred_list)\n",
    "                acc_val.append((y_pred_list == y_true_list).sum().item() / len(y_true_list))\n",
    "\n",
    "            log_info = f\"batch_size: {batch_size} \\t| epoch: {epoch} \\t| train loss: {loss_train[-1]:.5f} \\t| val loss: {loss_val[-1]:.5f}\\t | time taken: {time_train[-1]}\"\n",
    "            print(log_info)\n",
    "\n",
    "            if early_stopper.early_stop(loss_val[-1]):\n",
    "                print(f\"early stopping at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "            if epoch == n_epochs - 1:\n",
    "                print(\"reach max number of epoch\")\n",
    "\n",
    "        time_train_dict[batch_size].append(time_train[-1])\n",
    "        mean_cv_acc_dict[batch_size].append(acc_val[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b224f8-3b69-425f-bebb-5d15d16e9596",
   "metadata": {},
   "source": [
    "3. Plot scatterplot of mean cross validation accuracies on the final epoch for the different batch sizes. Limit search space to batch sizes {32, 64, 128, 256}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f044488a-88f9-434e-8b66-f6518392daf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enter your code here\n",
    "\n",
    "mean_cv_accs_plot = [np.mean(mean_cv_acc_dict[batch_size]) for batch_size in batch_sizes]\n",
    "\n",
    "plt.scatter(batch_sizes, mean_cv_accs_plot)\n",
    "plt.xlabel(\"Batch Size\")\n",
    "plt.ylabel(\"Mean Cross Validation Accuracy\")\n",
    "plt.title(\"Mean Cross Validation Accuracy vs Batch Size\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa030c54-1812-482c-8662-289318457a3b",
   "metadata": {},
   "source": [
    "4. Create a table of time taken to train the network on the last epoch against different batch sizes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad41399d-a7fa-40b4-8d56-4f2fd500f36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enter your code here\n",
    "\n",
    "batch_sizes = list(time_train_dict.keys())\n",
    "split_numbers = [f\"Split {i+1}\" for i in range(len(time_train_dict[batch_sizes[0]]))]\n",
    "columns = split_numbers + [\"Mean\"]\n",
    "\n",
    "results = pd.DataFrame(index=batch_sizes, columns=columns)\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    times = time_train_dict[batch_size]\n",
    "    mean_time = np.mean(times)\n",
    "\n",
    "    for i, time_count in enumerate(times):\n",
    "        results.at[batch_size, f\"Split {i+1}\"] = time_count\n",
    "\n",
    "    results.at[batch_size, \"Mean\"] = mean_time\n",
    "    \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6750ff86-6c16-4522-a858-5f533d8b0353",
   "metadata": {},
   "source": [
    "5. Select the optimal batch size and state a reason for your selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238d470c-c73a-4c83-a09e-695b5669b736",
   "metadata": {},
   "source": [
    "\\# TODO: \\<Enter your answer here\\>\n",
    "\n",
    "From the experimental chart, we can see that no matter how the batch size is set, a high cross-validation accuracy can be achieved. It is also worth noting that the cross-validation accuracy tends to decrease as the batch size increases. From the perspective of time consumption, a larger batch size consumes less time. Empirically, doubling the batch size will reduce the time consumption to about three-quarters of the original. Therefore, this is a decision-making problem. If we want the model to train faster, we can choose a larger batch. If we want the model to have better accuracy, we should set a smaller batch. In the scenario of this project, the absolute value of the change in accuracy is not obvious, so we can accept the slight decrease in accuracy caused by a larger batch. Therefore, I will choose a batch size of 32, because it brings faster training time and acceptable accuracy at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2ea730-e6f9-4e9e-ad30-905d62436d37",
   "metadata": {},
   "source": [
    "Part A, Q3 (10 marks)\n",
    "---\n",
    "In this question, we will find the optimal number of hidden neurons for first hidden layer of the 4-layer network (3 hidden layers, output layer) designed in Q1 and Q2.\n",
    "\n",
    "To reduce repeated code, you may need to import the network (MLP defined in QA1) from common_utils.py. You will not be repenalised for any error in QA1 here as the code in QA1 will not be remarked. The following code cell will not be marked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f995b5b-1c8a-4a50-a615-c11b5fa66586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enter your code here\n",
    "\n",
    "first_hidden_sizes = [64, 128, 256]\n",
    "\n",
    "model_dict = {first_hidden_size: MLP(57, first_hidden_size=first_hidden_size) for first_hidden_size in first_hidden_sizes}\n",
    "\n",
    "model_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cc14b0-eb67-4b18-a38f-69a32e08a9ae",
   "metadata": {},
   "source": [
    "> Plot the mean cross-validation accuracies on the final epoch for different numbers of hidden-layer neurons using a scatter plot. Limit the search space of the number of neurons to {64, 128, 256}. Continue using 5-fold cross validation on training dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ced740-f749-4312-be7b-f747d3eea0c1",
   "metadata": {},
   "source": [
    "1. Perform hyperparameter tuning for the different neurons with 5-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a044015-bf37-4673-8456-e20dfe0b52c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enter your code here\n",
    "\n",
    "X, y = X_train, y_train\n",
    "\n",
    "\n",
    "X_train_scaled_list = []\n",
    "X_val_scaled_list = []\n",
    "y_train_list = []\n",
    "y_val_list = []\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for train_index, val_index in kf.split(X):\n",
    "    X_train_fold, X_val_fold = X[train_index], X[val_index]\n",
    "    y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
    "\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    X_train_fold = scaler.fit_transform(X_train_fold)\n",
    "    X_val_fold = scaler.transform(X_val_fold)\n",
    "\n",
    "    X_train_scaled_list.append(X_train_fold)\n",
    "    X_val_scaled_list.append(X_val_fold)\n",
    "    y_train_list.append(y_train_fold)\n",
    "    y_val_list.append(y_val_fold)\n",
    "    \n",
    "print(\n",
    "    X_train_scaled_list[0].shape,\n",
    "    X_val_scaled_list[0].shape,\n",
    "    y_train_list[0].shape,\n",
    "    y_val_list[0].shape,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaf267e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTINUE\n",
    "\n",
    "\n",
    "time_train_dict = {first_hidden_size: [] for first_hidden_size in first_hidden_sizes}\n",
    "mean_cv_acc_dict = {first_hidden_size: [] for first_hidden_size in first_hidden_sizes}\n",
    "mean_test_acc_dict = {first_hidden_size: [] for first_hidden_size in first_hidden_sizes}\n",
    "acc_train_dict = {first_hidden_size: [] for first_hidden_size in first_hidden_sizes}\n",
    "acc_val_dict = {first_hidden_size: [] for first_hidden_size in first_hidden_sizes}\n",
    "acc_test_dict = {first_hidden_size: [] for first_hidden_size in first_hidden_sizes}\n",
    "\n",
    "for first_hidden_size in first_hidden_sizes:\n",
    "    set_seed(42)\n",
    "    model = model_dict[first_hidden_size]\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    loss_fn = nn.BCELoss()\n",
    "\n",
    "    n_splits = 5\n",
    "    n_epochs = 200\n",
    "    best_batch_size = 32\n",
    "\n",
    "    for i in range(n_splits):\n",
    "        X_train_ndarray = X_train_scaled_list[i]\n",
    "        X_val_ndarray = X_val_scaled_list[i]\n",
    "        y_train_ndarray = y_train_list[i]\n",
    "        y_val_ndarray = y_val_list[i]\n",
    "        train_dataset = AudioDataset(X_train_ndarray, y_train_ndarray)\n",
    "        val_dataset = AudioDataset(X_val_ndarray, y_val_ndarray)\n",
    "        train_loader = AudioDataLoader(train_dataset, best_batch_size)\n",
    "        val_loader = AudioDataLoader(val_dataset, best_batch_size)\n",
    "\n",
    "        loss_train = []\n",
    "        acc_train = []\n",
    "        time_train = []\n",
    "        loss_val = []\n",
    "        acc_val = []\n",
    "        loss_test = []\n",
    "        acc_test = []\n",
    "\n",
    "        early_stopper = EarlyStopper(patience=3, min_delta=1e-5)\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            start_time = time.time()\n",
    "            model.train()\n",
    "            with torch.enable_grad():\n",
    "                y_pred_list = []\n",
    "                y_true_list = []\n",
    "                for batch in train_loader:\n",
    "                    X, y = batch\n",
    "                    y_pred = model(X)\n",
    "                    loss = loss_fn(y_pred, y)\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    y_pred_list.append(y_pred)\n",
    "                    y_true_list.append(y)\n",
    "                y_pred_list = torch.cat(y_pred_list)\n",
    "                y_true_list= torch.cat(y_true_list)\n",
    "                loss_train.append(loss_fn(y_pred_list, y_true_list))\n",
    "                y_pred_list = torch.round(y_pred_list)\n",
    "                acc_train.append((y_pred_list == y_true_list).sum().item() / len(y_true_list))\n",
    "            end_time = time.time()\n",
    "            time_train.append(end_time - start_time)\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                y_pred_list = []\n",
    "                y_true_list = []\n",
    "                for batch in val_loader:\n",
    "                    X, y = batch\n",
    "                    y_pred = model(X)\n",
    "                    loss = loss_fn(y_pred, y)\n",
    "                    y_pred_list.append(y_pred)\n",
    "                    y_true_list.append(y)\n",
    "                y_pred_list = torch.cat(y_pred_list)\n",
    "                y_true_list= torch.cat(y_true_list)\n",
    "                loss_val.append(loss_fn(y_pred_list, y_true_list))\n",
    "                y_pred_list = torch.round(y_pred_list)\n",
    "                acc_val.append((y_pred_list == y_true_list).sum().item() / len(y_true_list))\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                y_pred_list = []\n",
    "                y_true_list = []\n",
    "                for batch in test_loader:\n",
    "                    X, y = batch\n",
    "                    y_pred = model(X)\n",
    "                    loss = loss_fn(y_pred, y)\n",
    "                    y_pred_list.append(y_pred)\n",
    "                    y_true_list.append(y)\n",
    "                y_pred_list = torch.cat(y_pred_list)\n",
    "                y_true_list= torch.cat(y_true_list)\n",
    "                loss_test.append(loss_fn(y_pred_list, y_true_list))\n",
    "                y_pred_list = torch.round(y_pred_list)\n",
    "                acc_test.append((y_pred_list == y_true_list).sum().item() / len(y_true_list))\n",
    "\n",
    "            log_info = f\"first_hidden_size: {first_hidden_size} \\t| epoch: {epoch} \\t| train loss: {loss_train[-1]:.5f} \\t| val loss: {loss_val[-1]:.5f}\\t| test loss: {loss_test[-1]:.5f}\\t| time taken: {time_train[-1]}\"\n",
    "            print(log_info)\n",
    "\n",
    "            if early_stopper.early_stop(loss_val[-1]):\n",
    "                print(f\"early stopping at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "            if epoch == n_epochs - 1:\n",
    "                print(\"reach max number of epoch\")\n",
    "\n",
    "        time_train_dict[first_hidden_size].append(time_train[-1])\n",
    "        mean_cv_acc_dict[first_hidden_size].append(acc_val[-1])\n",
    "        mean_test_acc_dict[first_hidden_size].append(acc_test[-1])\n",
    "        acc_train_dict[first_hidden_size].append(acc_train)\n",
    "        acc_val_dict[first_hidden_size].append(acc_val)\n",
    "        acc_test_dict[first_hidden_size].append(acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad9f217-fa68-4a7f-bb37-dcaaba438bd1",
   "metadata": {},
   "source": [
    "2. Plot the mean cross-validation accuracies on the final epoch for different numbers of hidden-layer neurons using a scatter plot. Limit the search space of the number of neurons to {64, 128, 256}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6920ca9-2886-432b-914d-65f6e5d7f128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enter your code here\n",
    "\n",
    "mean_cv_accs_plot = [\n",
    "    np.mean(mean_cv_acc_dict[first_hidden_size])\n",
    "    for first_hidden_size in first_hidden_sizes\n",
    "]\n",
    "plt.scatter(first_hidden_sizes, mean_cv_accs_plot)\n",
    "plt.xlabel(\"Number of Hidden Layer Neurons\")\n",
    "plt.ylabel(\"Mean Cross Validation Accuracy\")\n",
    "plt.title(\"Mean Cross Validation Accuracy vs Number of Hidden Layer Neurons\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5787c1e-ec8f-43eb-9809-ff46ecf7077b",
   "metadata": {},
   "source": [
    "> Select the optimal number of neurons for the hidden layer. State the rationale for your selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3916a7ad-f9fd-4a0f-acf6-aa33f804e42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enter your code here\n",
    "\n",
    "n_splits = 5\n",
    "for i in range(n_splits):\n",
    "    mean_cv_accs_plot = [\n",
    "        mean_cv_acc_dict[first_hidden_size][i]\n",
    "        for first_hidden_size in first_hidden_sizes\n",
    "    ]\n",
    "    plt.scatter(first_hidden_sizes, mean_cv_accs_plot, label=f\"Fold {i+1}\")\n",
    "plt.xlabel(\"Number of Hidden Layer Neurons\")\n",
    "plt.ylabel(\"Mean Cross Validation Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Fold Accuracy vs Number of Hidden Layer Neurons\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2a1270-5499-45fe-a28c-4f3416336eeb",
   "metadata": {},
   "source": [
    "\\# TODO: \\<Enter your answer here\\>\n",
    "\n",
    "From the chart, we can see that when the number of neurons in the first hidden layer is set to 256, the model has better accuracy on the validation set than the other two settings. This may be because the characteristics of the original input data are more complex, so a more complex model has a better fitting effect. Under this parameter setting, there is no obvious overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e382726-ffc0-42bf-8b42-16448c1b9f58",
   "metadata": {},
   "source": [
    "> Plot the train and test accuracies against training epochs with the optimal number of neurons using a line plot.\n",
    "Note: use this optimal number of neurons for the rest of the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3adb95-dbeb-486c-a481-321eb166351d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enter your code here\n",
    "\n",
    "optimal_first_hidden_size = 256\n",
    "acc_train_full = []\n",
    "acc_val_full = []\n",
    "acc_test_full = []\n",
    "for i in range(n_splits):\n",
    "    plt.plot(range(1, len(acc_train_dict[optimal_first_hidden_size][i])+1), acc_train_dict[optimal_first_hidden_size][i], label='train')\n",
    "    plt.plot(range(1, len(acc_val_dict[optimal_first_hidden_size][i])+1), acc_val_dict[optimal_first_hidden_size][i], label='val')\n",
    "    plt.plot(\n",
    "        range(1, len(acc_test_dict[optimal_first_hidden_size][i]) + 1),\n",
    "        acc_test_dict[optimal_first_hidden_size][i],\n",
    "        label=\"test\",\n",
    "    )\n",
    "    plt.plot()\n",
    "    acc_train_full += acc_train_dict[optimal_first_hidden_size][i]\n",
    "    acc_val_full += acc_val_dict[optimal_first_hidden_size][i]\n",
    "    acc_test_full += acc_test_dict[optimal_first_hidden_size][i]\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title(f\"Fold {i + 1} Accuracy vs Epochs with {optimal_first_hidden_size} Hidden Layer Neurons\")\n",
    "    plt.show()\n",
    "\n",
    "plt.plot(\n",
    "    range(1, len(acc_train_full) + 1),\n",
    "    acc_train_full,\n",
    "    label=\"train\",\n",
    ")\n",
    "plt.plot(\n",
    "    range(1, len(acc_val_full) + 1),\n",
    "    acc_val_full,\n",
    "    label=\"val\",\n",
    ")\n",
    "plt.plot(\n",
    "    range(1, len(acc_test_full) + 1),\n",
    "    acc_test_full,\n",
    "    label=\"test\",\n",
    ")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\n",
    "    f\"Full Process Accuracy vs Epochs with {optimal_first_hidden_size} Hidden Layer Neurons\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5a1393-12e7-4820-a4aa-a2d6f765cf3d",
   "metadata": {},
   "source": [
    "Part A, Q4 (10 marks)\n",
    "---\n",
    "In this section, we will understand the utility of such a neural network for a test audio. \n",
    "\n",
    "Do a model prediction on the sample test audio and obtain the predicted label using a threshold of 0.5. The model used is the optimized pretrained model using the selected optimal batch size and optimal number of neurons. \n",
    "Find the most important features on the model prediction for the test sample using SHAP. Plot the local feature importance with a force plot and explain your observations.  (Refer to the documentation and these three useful references:\n",
    "https://christophm.github.io/interpretable-ml-book/shap.html#examples-5, \n",
    "https://towardsdatascience.com/deep-learning-model-interpretation-using-shap-a21786e91d16,  \n",
    "https://medium.com/mlearning-ai/shap-force-plots-for-classification-d30be430e195) \n",
    "\n",
    "To reduce repeated code, you may need to import the network (MLP defined in QA1) from **common_utils.py**. You will not be repenalised for any error in QA1 here as the code in QA1 will not be remarked. The following code cell will not be marked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b701b7fa-7f8b-40ed-8bff-a0f315781c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enter your code here\n",
    "\n",
    "model_select = model_dict[256]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989a8de7-7394-4c9a-a7f4-c9dd9e818455",
   "metadata": {},
   "source": [
    "> Install and import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a769f3e-8748-4385-a2ce-e190fb2c32f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enter your code here\n",
    "\n",
    "!pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c74259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfffe05d-9e17-4329-8009-c4cdb94595ff",
   "metadata": {},
   "source": [
    "> Preprocess 'audio_test.wav' using the function 'extract_features' in common_utils.py. Please make sure the features are stored in a pandas dataframe, using variable name 'df', and fill the size of 'df' in 'size_row' and 'size_column'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc4e942-811e-4878-9842-5bfa0af86f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.display.Audio(\"./audio_test.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c8df59-5e09-41cf-ba25-22144ee676af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enter your code here\n",
    "from common_utils import extract_features\n",
    "\n",
    "df = extract_features(\"audio_test.wav\")\n",
    "size_row = 1\n",
    "size_column = 58\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fc2c3a-7eb5-474c-a6a4-5d288b470955",
   "metadata": {},
   "source": [
    "> Do a model prediction on the sample test audio and obtain the predicted label using a threshold of 0.5. The model used is the optimized pretrained model using the selected optimal batch size and optimal number of neurons. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda1f722-5bea-4127-b9ab-1a03782d77a4",
   "metadata": {},
   "source": [
    " 1.  Preprocess to obtain the test data, save the test data as numpy array, print the shape of the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6add5c3c-6bf4-48da-9600-b66de1eb74f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enter your code here\n",
    "X_train, _, _, _ = split_dataset(load_csv(\"./audio_gtzan.csv\"), [\"label\"], 0.3, 42)\n",
    "X_test = df.drop([\"filename\"], axis=1).to_numpy()\n",
    "X_train, X_test = preprocess_dataset(X_train, X_test)\n",
    "\n",
    "X_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4a9090-0f9c-4e19-b609-37e46bbc4a31",
   "metadata": {},
   "source": [
    "2. Do a model prediction on the sample test audio and obtain the predicted label using a threshold of 0.5. The model used is the optimized pretrained model using the selected optimal batch size and optimal number of neurons. Note: Please define the variable of your final predicted label as 'pred_label'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fdd28d-7662-4f75-8430-68e3868faf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enter your code here\n",
    "pred = model(torch.tensor(X_test).float()).detach().numpy()[0]\n",
    "pred_label = (pred > 0.5).astype(int)\n",
    "pred, pred_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca46f49-8dc9-423c-aadf-56c0a60a740b",
   "metadata": {},
   "source": [
    "> Find the most important features on the model prediction for your test sample using SHAP. Create an instance of the DeepSHAP which is called DeepExplainer using traianing dataset: https://shap-lrjball.readthedocs.io/en/latest/generated/shap.DeepExplainer.html.\n",
    "\n",
    "Plot the local feature importance with a force plot and explain your observations.  (Refer to the documentation and these three useful references:\n",
    "https://christophm.github.io/interpretable-ml-book/shap.html#examples-5, \n",
    "https://towardsdatascience.com/deep-learning-model-interpretation-using-shap-a21786e91d16,  \n",
    "https://medium.com/mlearning-ai/shap-force-plots-for-classification-d30be430e195) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03406410-0615-4c8d-ba89-5b430019516c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Fit the explainer on a subset of the data (you can try all but then gets slower)\n",
    "Return approximate SHAP values for the model applied to the data given by X.\n",
    "Plot the local feature importance with a force plot and explain your observations.\n",
    "'''\n",
    "# TODO: Enter your code here\n",
    "\n",
    "X_train, _, X_test, _ = split_dataset(load_csv(\"./audio_gtzan.csv\"), [\"label\"], 0.3, 42)\n",
    "X_train, X_test = preprocess_dataset(X_train, X_test)\n",
    "\n",
    "explainer = shap.DeepExplainer(model, torch.tensor(X_train).float())\n",
    "shap_values = explainer.shap_values(torch.tensor(X_train).float())\n",
    "shap.force_plot(explainer.expected_value, shap_values, X_test, matplotlib=True)\n",
    "plt.title(\"Local Feature Importance with a Force Plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696dc88a-5095-4554-9523-03a7614606e6",
   "metadata": {},
   "source": [
    "\\# TODO: \\<Enter your answer here\\>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
