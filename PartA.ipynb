{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cc09d0e-76ad-40ea-8c5f-016dc0c7ad25",
   "metadata": {},
   "source": [
    "CS4001/4042 Assignment 1\n",
    "---\n",
    "Part A, Q1 (15 marks)\n",
    "---\n",
    "\n",
    ">Design a feedforward deep neural network (DNN) which consists of **three** hidden layers of 128 neurons each with ReLU activation function, and an output layer with sigmoid activation function. Apply dropout of probability **0.2** to each of the hidden layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f452e6c-4a7f-4565-b42d-28890075e2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import time\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from scipy.io import wavfile as wav\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "from common_utils import set_seed\n",
    "\n",
    "# setting seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cf9d7a-b94d-4b39-87a2-839d21f52f87",
   "metadata": {},
   "source": [
    "Define the model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac16b16f-245a-4096-990a-521d04ee437e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enter your code here\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128, output_size=1, dropout_prob=0.2):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.sigmoid(self.fc4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445d9a47-d690-4992-8151-19e7ab3c68ab",
   "metadata": {},
   "source": [
    "> Divide the dataset into a 70:30 ratio for training and testing. Use **appropriate** scaling of input features. We solely assume that there are only two datasets here: training & test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5154111-ba80-4eff-8ed9-67d3451ca515",
   "metadata": {},
   "source": [
    "Split the dataset and do preprocessing. You can use the split_dataset and preprocess_dataset provided for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cee8112e-7e1c-46da-a21b-a70849d6b621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1400, 57) (1400,) (600, 57) (600,)\n"
     ]
    }
   ],
   "source": [
    "from common_utils import split_dataset, preprocess_dataset\n",
    "\n",
    "# TODO: Enter your code here\n",
    "def load_csv(file_path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(file_path)\n",
    "    filename_col = df[\"filename\"]\n",
    "    lable_col = filename_col.apply(lambda x: x.split(\".\")[0])\n",
    "    df[\"label\"] = lable_col\n",
    "    df.drop([\"filename\"], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "def load_dataframe_split(df: pd.DataFrame) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    df_train, y_train, df_test, y_test = split_dataset(df, [\"label\"], 0.3, 42)\n",
    "    df_train, df_test = preprocess_dataset(df_train, df_test)\n",
    "    return df_train, y_train, df_test, y_test\n",
    "\n",
    "df = load_csv(\"./audio_gtzan.csv\")\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_dataframe_split(df)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba7da82-1a87-4415-b1fd-c7ba7b706eb6",
   "metadata": {},
   "source": [
    "> Use the training dataset to train the model for 100 epochs. Use a mini-batch gradient descent with **‘Adam’** optimizer with learning rate of **0.001**, and **batch size = 128**. Implement early stopping with patience of **3**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19d12e7-562a-4a74-9646-4d90bdeb4367",
   "metadata": {},
   "source": [
    "1. Define a Pytorch Dataset and Dataloaders.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e86f216-50ba-4670-8cfd-bf28949e7dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AudioDataLoader(\n",
      "\tDataset length: 1400, \n",
      "\tDataset shape: torch.Size([57]), \n",
      "\tBatch size: 128, \n",
      "\tShuffle: True\n",
      ")\n",
      " AudioDataLoader(\n",
      "\tDataset length: 600, \n",
      "\tDataset shape: torch.Size([57]), \n",
      "\tBatch size: 128, \n",
      "\tShuffle: True\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Enter your code here\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "        if self.X.shape[0] != self.y.shape[0]:\n",
    "            raise ValueError(\"X and y must have the same length.\")\n",
    "        if self.y.ndim == 1:\n",
    "            self.y = self.y.reshape(-1, 1)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "class AudioDataLoader(DataLoader):\n",
    "    def __init__(self, dataset, batch_size, shuffle=True):\n",
    "        super(AudioDataLoader, self).__init__(dataset, batch_size, shuffle=shuffle)\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i in range(0, len(self.dataset), self.batch_size):\n",
    "            yield self.dataset[i : i + self.batch_size]\n",
    "            \n",
    "    def __str__(self):\n",
    "        dataset_info = f\"\\tDataset length: {len(self.dataset)}, \\n\\tDataset shape: {self.dataset[0][0].shape}, \\n\\tBatch size: {self.batch_size}, \\n\\tShuffle: {self.shuffle}\"\n",
    "        return f\"AudioDataLoader(\\n{dataset_info}\\n)\\n\"\n",
    "            \n",
    "train_dataset = AudioDataset(X_train, y_train)\n",
    "test_dataset = AudioDataset(X_test, y_test)\n",
    "\n",
    "train_loader = AudioDataLoader(train_dataset, 128)\n",
    "test_loader = AudioDataLoader(test_dataset, 128)\n",
    "\n",
    "print(train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c905954-db7b-46b8-b758-ac3182872854",
   "metadata": {},
   "source": [
    "2. Next, define the model, optimizer and loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e64a6b1-e29d-4b24-87f6-d67c79589427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enter your code here\n",
    "model = MLP(57)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38afed0-48b9-4512-a6d7-28181261b9fb",
   "metadata": {},
   "source": [
    "3. Train model for 100 epochs. Record down train and test accuracies. Implement early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a3b26a3-07aa-4608-b865-72b261b73079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 \t| train loss: 0.69534 \t| test loss: 0.68477\n",
      "epoch: 1 \t| train loss: 0.68065 \t| test loss: 0.66993\n",
      "epoch: 2 \t| train loss: 0.66535 \t| test loss: 0.65274\n",
      "epoch: 3 \t| train loss: 0.64601 \t| test loss: 0.63107\n",
      "epoch: 4 \t| train loss: 0.62185 \t| test loss: 0.60306\n",
      "epoch: 5 \t| train loss: 0.59159 \t| test loss: 0.56812\n",
      "epoch: 6 \t| train loss: 0.55488 \t| test loss: 0.52708\n",
      "epoch: 7 \t| train loss: 0.51146 \t| test loss: 0.48214\n",
      "epoch: 8 \t| train loss: 0.46406 \t| test loss: 0.43822\n",
      "epoch: 9 \t| train loss: 0.41859 \t| test loss: 0.40012\n",
      "epoch: 10 \t| train loss: 0.38057 \t| test loss: 0.36905\n",
      "epoch: 11 \t| train loss: 0.35069 \t| test loss: 0.34384\n",
      "epoch: 12 \t| train loss: 0.32686 \t| test loss: 0.32253\n",
      "epoch: 13 \t| train loss: 0.30112 \t| test loss: 0.30356\n",
      "epoch: 14 \t| train loss: 0.28311 \t| test loss: 0.28605\n",
      "epoch: 15 \t| train loss: 0.26028 \t| test loss: 0.26985\n",
      "epoch: 16 \t| train loss: 0.24221 \t| test loss: 0.25455\n",
      "epoch: 17 \t| train loss: 0.22158 \t| test loss: 0.24068\n",
      "epoch: 18 \t| train loss: 0.20682 \t| test loss: 0.22862\n",
      "epoch: 19 \t| train loss: 0.19169 \t| test loss: 0.21841\n",
      "epoch: 20 \t| train loss: 0.18053 \t| test loss: 0.20975\n",
      "epoch: 21 \t| train loss: 0.16730 \t| test loss: 0.20245\n",
      "epoch: 22 \t| train loss: 0.15814 \t| test loss: 0.19654\n",
      "epoch: 23 \t| train loss: 0.14631 \t| test loss: 0.19163\n",
      "epoch: 24 \t| train loss: 0.13811 \t| test loss: 0.18751\n",
      "epoch: 25 \t| train loss: 0.13475 \t| test loss: 0.18364\n",
      "epoch: 26 \t| train loss: 0.12851 \t| test loss: 0.17994\n",
      "epoch: 27 \t| train loss: 0.12613 \t| test loss: 0.17643\n",
      "epoch: 28 \t| train loss: 0.11882 \t| test loss: 0.17387\n",
      "epoch: 29 \t| train loss: 0.11774 \t| test loss: 0.17154\n",
      "epoch: 30 \t| train loss: 0.11424 \t| test loss: 0.16953\n",
      "epoch: 31 \t| train loss: 0.11091 \t| test loss: 0.16735\n",
      "epoch: 32 \t| train loss: 0.10075 \t| test loss: 0.16530\n",
      "epoch: 33 \t| train loss: 0.10093 \t| test loss: 0.16278\n",
      "epoch: 34 \t| train loss: 0.09796 \t| test loss: 0.16029\n",
      "epoch: 35 \t| train loss: 0.10085 \t| test loss: 0.15795\n",
      "epoch: 36 \t| train loss: 0.08938 \t| test loss: 0.15621\n",
      "epoch: 37 \t| train loss: 0.08922 \t| test loss: 0.15527\n",
      "epoch: 38 \t| train loss: 0.08784 \t| test loss: 0.15257\n",
      "epoch: 39 \t| train loss: 0.08435 \t| test loss: 0.15074\n",
      "epoch: 40 \t| train loss: 0.07608 \t| test loss: 0.14989\n",
      "epoch: 41 \t| train loss: 0.08270 \t| test loss: 0.14834\n",
      "epoch: 42 \t| train loss: 0.07826 \t| test loss: 0.14612\n",
      "epoch: 43 \t| train loss: 0.07198 \t| test loss: 0.14421\n",
      "epoch: 44 \t| train loss: 0.07229 \t| test loss: 0.14285\n",
      "epoch: 45 \t| train loss: 0.07134 \t| test loss: 0.14007\n",
      "epoch: 46 \t| train loss: 0.06381 \t| test loss: 0.13874\n",
      "epoch: 47 \t| train loss: 0.06421 \t| test loss: 0.13874\n",
      "epoch: 48 \t| train loss: 0.06849 \t| test loss: 0.13835\n",
      "epoch: 49 \t| train loss: 0.06543 \t| test loss: 0.13617\n",
      "epoch: 50 \t| train loss: 0.06217 \t| test loss: 0.13479\n",
      "epoch: 51 \t| train loss: 0.06414 \t| test loss: 0.13526\n",
      "epoch: 52 \t| train loss: 0.05884 \t| test loss: 0.13419\n",
      "epoch: 53 \t| train loss: 0.05697 \t| test loss: 0.13249\n",
      "epoch: 54 \t| train loss: 0.05762 \t| test loss: 0.13095\n",
      "epoch: 55 \t| train loss: 0.05943 \t| test loss: 0.12987\n",
      "epoch: 56 \t| train loss: 0.05456 \t| test loss: 0.13018\n",
      "epoch: 57 \t| train loss: 0.04711 \t| test loss: 0.12890\n",
      "epoch: 58 \t| train loss: 0.05104 \t| test loss: 0.12687\n",
      "epoch: 59 \t| train loss: 0.04805 \t| test loss: 0.12528\n",
      "epoch: 60 \t| train loss: 0.05381 \t| test loss: 0.12461\n",
      "epoch: 61 \t| train loss: 0.05191 \t| test loss: 0.12353\n",
      "epoch: 62 \t| train loss: 0.04621 \t| test loss: 0.12389\n",
      "epoch: 63 \t| train loss: 0.04829 \t| test loss: 0.12533\n",
      "epoch: 64 \t| train loss: 0.04899 \t| test loss: 0.12402\n",
      "early stopping at epoch 64\n"
     ]
    }
   ],
   "source": [
    "from common_utils import EarlyStopper\n",
    "\n",
    "# TODO: Enter your code here\n",
    "early_stopper = EarlyStopper(patience=3, min_delta=1e-5)\n",
    "n_epochs = 100\n",
    "acc_train = []\n",
    "acc_test = []\n",
    "loss_train = []\n",
    "loss_test = []\n",
    "f1_train = []\n",
    "f1_test = []\n",
    "precision_train = []\n",
    "precision_test = []\n",
    "recall_train = []\n",
    "recall_test = []\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    with torch.enable_grad():\n",
    "        y_pred_list = []\n",
    "        y_true_list = []\n",
    "        for batch in train_loader:\n",
    "            X, y = batch\n",
    "            y_pred = model(X)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            y_pred_list.append(y_pred)\n",
    "            y_true_list.append(y)\n",
    "        y_pred_list = torch.cat(y_pred_list)\n",
    "        y_true_list= torch.cat(y_true_list)\n",
    "        loss_train.append(loss_fn(y_pred_list, y_true_list))\n",
    "        y_pred_list = torch.round(y_pred_list)\n",
    "        acc_train.append((y_pred_list == y_true_list).sum().item() / len(y_true_list))\n",
    "        y_pred_list, y_true_list = y_pred_list.detach().numpy(), y_true_list.detach().numpy()\n",
    "        f1_train.append(f1_score(y_true_list, y_pred_list))\n",
    "        precision_train.append(precision_score(y_true_list, y_pred_list))\n",
    "        recall_train.append(recall_score(y_true_list, y_pred_list))\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred_list = []\n",
    "        y_true_list = []\n",
    "        for batch in test_loader:\n",
    "            X, y = batch\n",
    "            y_pred = model(X)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            y_pred_list.append(y_pred)\n",
    "            y_true_list.append(y)\n",
    "        y_pred_list = torch.cat(y_pred_list)\n",
    "        y_true_list= torch.cat(y_true_list)\n",
    "        loss_test.append(loss_fn(y_pred_list, y_true_list))\n",
    "        y_pred_list = torch.round(y_pred_list)\n",
    "        acc_test.append((y_pred_list == y_true_list).sum().item() / len(y_true_list))\n",
    "        y_pred_list, y_true_list = y_pred_list.detach().numpy(), y_true_list.detach().numpy()\n",
    "        f1_test.append(f1_score(y_true_list, y_pred_list))\n",
    "        precision_test.append(precision_score(y_true_list, y_pred_list))\n",
    "        recall_test.append(recall_score(y_true_list, y_pred_list))\n",
    "\n",
    "    log_info = f\"epoch: {epoch} \\t| train loss: {loss_train[-1]:.5f} \\t| test loss: {loss_test[-1]:.5f}\"\n",
    "    print(log_info)\n",
    "\n",
    "    if early_stopper.early_stop(loss_test[-1]):\n",
    "        print(f\"early stopping at epoch {epoch}\")\n",
    "        break\n",
    "    \n",
    "loss_train = torch.tensor(loss_train)\n",
    "loss_test = torch.tensor(loss_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cde204-f469-45c3-a78e-36640b9829d1",
   "metadata": {},
   "source": [
    "> Plot train and test accuracies and losses on training and test data against training epochs and comment on the line plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13f33ffc-6ce7-487a-8610-d92eaeba514d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA04AAAHACAYAAACVhTgAAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAmmNJREFUeJzs3Xd4VGXax/HvzKSTRkgjIRB6h9BFRFFBbKxgw7JS1rKrYou+q1hgLStWFgvKiiCoq2LXXRXBKAKC9N4hQEJIIQnpfWbeP04yEEmAhCSTSX6f6zrXTM6cM3PPiMm5536e+zHZ7XY7IiIiIiIiUi2zswMQERERERFp7JQ4iYiIiIiInIESJxERERERkTNQ4iQiIiIiInIGSpxERERERETOQImTiIiIiIjIGShxEhEREREROQMlTiIiIiIiImfg5uwAGprNZuPo0aP4+flhMpmcHY6ISLNit9vJzc0lIiICs1nf3VXQ3yYREeeoyd+lZpc4HT16lKioKGeHISLSrCUmJtKmTRtnh9Fo6G+TiIhznc3fpWaXOPn5+QHGh+Pv7+/kaEREmpecnByioqIcv4vFoL9NIiLOUZO/S80ucaoYAuHv768/TiIiTqLhaJXpb5OIiHOdzd8lDTAXERERERE5A6cmTsuXL2fMmDFERERgMpn4+uuvz3jOsmXL6N+/P56ennTq1IkFCxbUe5wiIiIiItK8OTVxys/Pp2/fvsyePfusjj948CBXXXUVF198MZs3b+bBBx/kjjvu4Mcff6znSEVEREREpDlz6hynK664giuuuOKsj58zZw7t27fn1VdfBaB79+6sXLmSf/3rX4wePbrO4rLb7ZSVlWG1WuvsOZsTi8WCm5ub5jCIiIiI1AFdm54bd3d3LBbLOT+PSzWHWL16NSNHjqy0b/To0Tz44IN19holJSUkJydTUFBQZ8/ZHPn4+NC6dWs8PDycHYqIiIiIy9K16bkzmUy0adMGX1/fc3oel0qcUlJSCAsLq7QvLCyMnJwcCgsL8fb2PuWc4uJiiouLHT/n5ORU+/w2m42DBw9isViIiIjAw8NDVZMastvtlJSUcOzYMQ4ePEjnzp21yKWIiIhILeja9NzZ7XaOHTvGkSNH6Ny58zlVnlwqcaqNGTNm8PTTT5/VsSUlJdhsNqKiovDx8annyJoub29v3N3dOXz4MCUlJXh5eTk7JBERERGXo2vTuhESEsKhQ4coLS09p8TJpUoB4eHhpKamVtqXmpqKv79/ldUmgKlTp5Kdne3YEhMTz/g6qpCcO32GIuLKZs+eTXR0NF5eXgwZMoS1a9dWe+yIESMwmUynbFdddVUDRiwiTZmuq85NXVXpXKriNHToUL7//vtK+5YuXcrQoUOrPcfT0xNPT8/6Dk1ERJqIRYsWERsby5w5cxgyZAizZs1i9OjR7Nmzh9DQ0FOO//LLLykpKXH8nJGRQd++fbnhhhsaMmwREalnTk1f8/Ly2Lx5M5s3bwaMduObN28mISEBMKpFEyZMcBz/t7/9jfj4eP7+97+ze/du3nrrLT799FMeeughZ4QvIiJN0MyZM7nzzjuZPHkyPXr0YM6cOfj4+DB//vwqjw8KCiI8PNyxLV26FB8fHyVOIiJNjFMTp/Xr19OvXz/69esHQGxsLP369WPatGkAJCcnO5IogPbt2/Pdd9+xdOlS+vbty6uvvsq7775bp63IBaKjo5k1a5azwxARaXAlJSVs2LChUgdXs9nMyJEjWb169Vk9x7x587jpppto0aJFfYUpItKsNJZrU6cO1RsxYgR2u73axxcsWFDlOZs2barHqFzTiBEjiImJqZN/VOvWrdMffBFpltLT07FarVV2cN29e/cZz1+7di3bt29n3rx5pz2uJh1fRURcUVO8NtVMs2aiYuG0sxESEqLOLSIitTBv3jx69+7N4MGDT3vcjBkzCAgIcGxRUVHn/Np5xWf3O15EpDFwxWtTJU5nYLfbKSgpc8p2umrcySZNmsSvv/7Ka6+95ujmtGDBAkwmEz/88AMDBgzA09OTlStXcuDAAa655hrCwsLw9fVl0KBB/PTTT5We74/lUJPJxLvvvsu4cePw8fGhc+fOfPvtt3X5MYvIOcjIK2b2L/u56Z3VrD6QUePzi8us7DyaQ1GpVqQPDg7GYrFU2cE1PDz8tOfm5+fzySefcPvtt5/xdWrT8bU6aTlF3PX+ev787hqstrP7uyEirskVrkuh6V6bulRXPWcoLLXSY9qPTnntnc+MxsfjzP+JXnvtNfbu3UuvXr145plnANixYwcAjz32GK+88godOnSgZcuWJCYmcuWVV/LPf/4TT09P3n//fcaMGcOePXto27Ztta/x9NNP89JLL/Hyyy/zxhtvcOutt3L48GGCgoLq5s2KSJV+25/OR2sSCPHzpF/bQGKiAmkb5IPJZGLbkWwWrDrEf7cepaTMBsCu5A38774LiAo68zdzqTlF/Of3w3y0NoH0vBIiA715cGRnru3fBou5eS6w6OHhwYABA4iLi2Ps2LGAsQBlXFwcU6ZMOe25n332GcXFxfz5z38+4+vUZcdXO7D6QAa5xWV8+PthJp4fXSfPKyKNjytcl0LTvTZV4tQEBAQE4OHhgY+Pj+Mb0Yqx+M888wyjRo1yHBsUFETfvn0dPz/77LN89dVXfPvtt6e9KJg0aRI333wzAM8//zyvv/46a9eu5fLLL6+PtyTS7Fltdl77aS9v/LKfii/5Fqwyblv6uBPm78XulFzH8X3aBFBSZmN3Si53/2cDn//tfLzcq17kb8Ph4yxYdYgftiVTVl6hsJhNJGUV8n+fb2Xuinj+b3Q3RnYPbZYr1MfGxjJx4kQGDhzI4MGDmTVrFvn5+UyePBmACRMmEBkZyYwZMyqdN2/ePMaOHUurVq0aNN4wfy/+fkU3nvp6Oy8t3s2oHmFEBFa9tqGISENoqtemSpzOwNvdws5nnNO1z7uai56aGDhwYKWf8/Ly+Mc//sF3331HcnIyZWVlFBYWVupeWJU+ffo47rdo0QJ/f3/S0tLOOT4RV1dmteFmqdtRz6k5Rdz/8SbWHMwE4Np+kfh7u7MpMYtdR3M4XlDK8YJS3MwmrurTmonnR9MvKpDk7CKufmMl25NymPbNdl66vm+l580vLuOJr7bx9eajjn2Do4OYNCyaC7uE8J/fD/PWsgPsTc3jzvfX079tIOd3DCY8wItwfy/jNsCLIB8PzE24IjV+/HiOHTvGtGnTSElJISYmhsWLFzsaRiQkJJyyGOWePXtYuXIlS5YscUbI3Dq4LV9vSmLD4eNM/3YHcycMPPNJIuJyXP26FFz72lSJ0xmYTKazLks2Rn/sQPLII4+wdOlSXnnlFTp16oS3tzfXX399pcUbq+Lu7l7pZ5PJhM1mq/N4RerKpoTjrDqQwQ0D2hDq71Xnz5+ZX8Kz/9vJt1uOEurnSUxUoGPr3SbgrH5vVDVefMW+dB5atJmM/BJaeFh4/treXBMT6Xi8uMzKruRcDmfkM7RDq0rvLSLQmzdu7sdt89bw6foj9G/bkpsGG8McdiXncO9HG4k/lo/FbOLafpFMGhZNz4gAx/l/vagjNw1uy5xfD/DebwfZmJDFxoSsU2K8tn8kM2+MqcGn5XqmTJlS7Tedy5YtO2Vf165dazT+v66ZzSZmXNubq15fwdKdqSzensLlvU4/J0tEXI+rX5eCa1+buvYnLw4eHh5YrWee2P3bb78xadIkxo0bBxhZ/qFDh+o5OpEzO55fwtJdqVzWI4xAH49aPUdxmZXvtyWz4LdDbDmSDcCHvx9m3sRB9Ijwr/Y8q83O3tRctiRmsbl8O3K8kIu6hjDp/GgGtmvpGLJmt9v5dstRnv7vTjLzjV/qydlFJGen8MP2FADMJugS5ueYkxQT1ZL2wS2IT89zvMamhCz2peVVO5m/W7gfs2/tT8cQ30r7Pd0sjgStKsM6BfPwZV15+cc9TPt2Bz0i/Nl5NIfp3+6guMxGuL8Xb9zSj0HRVY8BD/B259HLuzHp/Gi+2pRE0vFCkrOLSM0pIiWniPS8YkL96j4RlXNUkEmXkv387aKOvPHzfqZ/u53zO7XC38v9zOeKiNSDpnhtqsSpiYiOjmbNmjUcOnQIX1/fajPuzp078+WXXzJmzBhMJhNPPfWUKkfidMfzS7jx36vZl5bHa4HevH5zPwa0a3nW5+cUlfLuioN8tOYw6XlGMuPhZibIx4Pk7CJumLOKN2/pz8XdQiudl5lfwlu/7OfjtQnkl5z6y/27rcl8tzWZnhH+TDw/mgHtWvLs/3aybM8xALqG+fHMNT2x2SlPuI6zJTGblJwidqfksjsll4/XGt3STCY4m4KEyQQ3D27LtKt7VDtH6UzuvqgjmxKy+GlXKjf+ezVFpcb/4yO6hjDzxhiCWpw5MQ3z9+JvF3U8ZX+p1UapVb8zGpXUHbDwTwDc+7c1/G9rMgfT83l58R6eHdvLycGJSHPVFK9NlTg1EY888ggTJ06kR48eFBYW8t5771V53MyZM/nLX/7C+eefT3BwMI8++qgWXhSnyisuY9J7a9mXlgdAUlYh4/+9mkcv78Ydw9ufsTlBQkYBkxes5cCxfADC/b24bWg7bhoUhZvFzN0fbmDVgQxuX7iO6WN6MvH8aApKypi34iDvLI8nt3ztmxYeFvq0CSSmvEoU7OvJZ+sT+WpTEjuO5vD3z7c6XtPDYub+Sztx14Ud8XAz5roM7XiiIUBydiFbErPYlJjFlsQsth7JpqDEio+HhT5tAugbFUi/qEB6RgTg41E5OfJwM+N3jlUCs9nEqzf25Zo3V3IoowCL2cQjl3Xlrxd2OOe5Se4WM+51PKdLzlGrztAiGI7txmvZ0/xz3HRumbuGD9ccZmy/yBp9CSEiUlea4rWpye7MQdlOkJOTQ0BAANnZ2fj7Vx66U1RUxMGDB2nfvj1eXhqKci70WcrZKCq1Mvm9dayOz6CljzvzJw3i3ZUH+W5rMgCXdgvllRv60rKaCsmGw8e56/31ZOSX0DrAiyeu6s7onuGVLuxLymw8+fU2Pl1/BIAreoWz7tBx0vOKAejR2p//u7wrF3YOqbIF9/H8EhatT+SD1YdJyipkcHQQz1/bm06hvqccWx2rzU5qThFh/l4N2ub7YHo+81bGM65fJAPaNY6lA073O7g5O+fP5fBqeK+8k9TkH/i/tS34bMMRuoT58t39w5XsirgoXU/VjdN9jjX5/auKk4jUG5vNTnp+MTmFZbQN8nFUZ8AY8jXlo02sjs/A19ONhX8ZTJ82gbx5cyBDO7Timf/tJG53Gle9voK/XNCe0T3DK61N9N8tR3n4sy2UlNnoFenPvImDCKuiCYSHm5kXr+tD+2BfXly82zEPqW2QDw9f1oUxfSJOW4Vp2cKDv13UkTsuaM+hjHw6BPvWuGpjMZuc0h66fXALnhvbu8FfV5yg3VDoPxE2LoT/PsjjE34mbncae1PzmL/yIH+tYtiliIjUjBInEakzGw4fZ+GqQxw5XkBqTjGpOUWOdYI83Mz0ivAnJqolMW0DiduVyk+7UvF0M/PuxIH0aRMIGF1x/nxeO/q1DWTKR5s4mJ7Pc9/t4rnvdtEr0p/Le4ZTUmbj9Z/3AzCyexiv3xxz2i5DJpOJu0d0pH2wD/N/O8TVfVpz06C2lRK5M3GzmOkU6lf7D0ekvo38B+z5HtL30HLz20y94ib+7/OtvBa3jz/FRNA6QGs7iYicCyVOInLObDY776yI5+Uf95zSJc5kAi83C4Wl1hPtrX8zHnMzm3jr1v6c1+HUBUN7RgTw3/su4PP1iSzekcLag5lsT8phe9KJcc93XNCeqVd2P+vhb5f3as3lvVrX+n2KNGo+QTB6Bnx5B/z6MtfdfS2L2rVk/eHjPPu/nbx16wBnRygi4tKUOIlIJUezCvluazLeHhZaB3gR5u9F6wAvglp4VNmoITO/hIc/3cwv5Z3mru7Tmqt6tyYswDgvxNcTi9nEoYwCNiceZ3OC0Y47KauQaWN6cmn3sGpj8fV0Y9Kw9kwa1p6MvGJjfZodKWxPyuHBkZ3583nt6u1zEHFJva+Hzf+B+F8wfx/Ls9e8z9Vv/sb321L4de8xLuoS4uwIRURclhInEXE4udnCH3m6mekR4e9YQ6hfVEvScou47+NNJGcX4eFm5uk/9eSmQVFVJljtg1vQPrgF4/q1qVVsrXw9uWlwW8eCriJSBZMJrp4Jbw2F+GV0T/+RiUN7Mf+3g0z/ZjuLH7yw1m3uRUSaOyVOIgJUbrbQJcyXtkEtSM0pIjnbWPS0uMzGpgRj4dY/6hDcgjdv6X/aRWZFpIEEdYAL/w9+fhYWT+Whu9byv61HOZRRwDvL47n/0s7OjlBExCUpcRJp5ux2O28tO8DLP+4Bqm62UFJmI/F4AVuPZDmG2u1MzqHUaueamAj+Oa43vp76dSLSaJx/P2xdBOl78dv4Nk9e/Rfu/3gTs3/Zz7h+kZU6VIqIyNnRlY6Ii8jML+GTdQlsSsjiwZGd6RkRcM7PWVJm4/GvtvH5BmONo9svaM/jVTRb8HAz0zHEl44hvo6hdsVlVnKLygj29TznOESkjrl5wKXTYdGtsHo2Y+67nU86tmLVgQz+8e0O5k0a5OwIRURcjhInkUZux9FsFq46xDebj1JcZgNgTXwGC/8ymH5tW9b6ebMKSvjbhxv4PT4TswmevqYXt9Wg2YKnmwVPX82VEGm0ul0FbQbDkbWYfn2RZ655jiteW07c7jTWHcpkUHTjWBRZRMRVaCnxJmLEiBE8+OCDdfZ8kyZNYuzYsXX2fFIzdrudZXvSuHHOaq56fSWfrj9CcflCr33aBJBTVMaf313DmviM0z5HdQ5n5HPt26v4PT4TX0835k8aVKOkSURcgMkEo5427m98n07mZK4fYFSMZ/+y34mBiUhz0BSvTVVxEmlkNiUc54UfdrPmYCYAFrOJK3qFM3lYNP3btqSgxMqd769n1YEMJr63lrkTBjK8s9FiuLjMyndbk1m46hBbjmRzYZcQJp3fjhFdQjGXD79bfyiTO99fz/GCUiICvJg/eRDdwtXUQaRJanc+dLkC9v4AcU/zt5H/ZtG6RJbtOcb2pGx6RZ77kF8RkeZCFacmYNKkSfz666+89tprmEwmTCYThw4dYvv27VxxxRX4+voSFhbGbbfdRnp6uuO8zz//nN69e+Pt7U2rVq0YOXIk+fn5/OMf/2DhwoV88803judbtmyZ896gCzuckc8Hvx8mu6D0jMfuT8vjrx+sZ9xbq1hzMBMPi5nbL2jPb49ewpu39GdAuyBMJhMtyitEI7qGUFRq4/YF6/liwxFmLt3LsBd+JvbTLWw5kg3A8r3H+MuC9Vzy6jLmrTzIp+sTuWXuGo4XlNKnTQBf3ztMSZNUZrNB8lZY/RZ8cSd8NunUbeMHTg1RamjkdDCZYdd/aVewkz/1jQBUdRKR+tNUr01N9tON52mCcnJyCAgIIDs7G3//yheMRUVFHDx4kPbt2+Pl5WXstNuhtMAJkQLuPsZQizPIzs7miiuuoFevXjzzzDPGqe7udO/enTvuuIMJEyZQWFjIo48+SllZGT///DPJycm0bduWl156iXHjxpGbm8uKFSuYMGECALfffjs5OTm89957AAQFBeHh4XHWoVf5WTYjZVYbc1ccZNZPeykusxHs68nTf+rJlb3DT1njKDm7kNd+2sen6xOx2cFsgmv7t+GhUV2IDPSu9jWKy6zc//EmftyRWml/uL8Xfz6vLSO6hvLN5iQWrUskp6is0jGje4bxr/GVO+dJE2OzQup2OLoJrGdO3CnJh8Q1cHgVFGWd/thBd8JVr9QqrNP9Dm7O6v1z+eZe2PQhtD2fPVcsYvRrKzCZYOlDF9Ip1K/uX09E6sQp11MucF0Kje/a9HTXpTX5/aurpjMpLYDnI5zz2o8fBY8WZzwsICAADw8PfHx8CA8PB+C5556jX79+PP/8847j5s+fT1RUFHv37iUvL4+ysjKuvfZa2rUz5rb07t3bcay3tzfFxcWO55Oztz0pm0e/2MqOozkA+Hm5kZ5XzL0fbWRUjzCevaYX4QFeZBWU8PayAyxYdcjR9GFUjzD+b3RXuoSd+ULG083Cm7f05++fb+WrTUkMim7JpPPbc1nPMNwtRjG5V2QAD43qwlebkli46hB7U/O468IOPHZ5N8fQPXFBNiscP3RqQlSSD4m/w6GVcPg3KMqu3fN7+ELbodD2PPCs4o9IWI/aPa84z4ipsO1zSFhF19zVXNYjjCU7U3nrlwPMHB/j7OhE5Gy5wHUpNN1rUyVOTdSWLVv45Zdf8PX1PeWxAwcOcNlll3HppZfSu3dvRo8ezWWXXcb1119Py5a179LWXNntdo4XlJKSXcQ3m5N4d+VBrDY7Ad7uPHV1D67u05q3lh3g7WX7Wbozld8PZHBNvwi+3XzUUQkaHB3Eo1d0ZUC7mnW5creY+df4GKaP6UGgT9Xfuvh4uHHrkHbcMrgt2YWl1R4njZjNCinbjITo0EqjKlR8FkmRhx+0GQieZ1FRMLtB674QPdy4tejPQ5MS0AaG/BV+ew1++gdTxnzHkp2pfLPlKA+O7ELbVlrXSUTqV1O4NtVfxjNx9zEybGe9di3l5eUxZswYXnzxxVMea926NRaLhaVLl7Jq1SqWLFnCG2+8wRNPPMGaNWto3779uUTt0kqtNrYnZZOcXURydhGpOcZtZn4xfxzUWlJmIzW3iNScYkrKK0YVxvSNYNrVPQjxM9Y4ih3Vhat6t+bRL7ayOTGLD39PAKBbuB9/v7wrF3cNPWUIX02cTTJkMpmUNLma/HRY9TpsWHBq9cjNGzz+8DvC7AatYyD6AogeBuFKgOQkFzwE6xdA2k76FK5neOdgVuxLZ87yAzw/rvcZTxeRRsBFr0uhaVyb6i/qmZhMZ12WdCYPDw+sVqvj5/79+/PFF18QHR2Nm1vV/5lNJhPDhg1j2LBhTJs2jXbt2vHVV18RGxt7yvM1dTabne+2JfPqkj0cyqjd2OFgXw+ignyYcnEnLu0edsrjXcP9+OLu8/lg9SF+3JHKDQPbcE1M5CmLzYqQd8xImNa9e2Isu6e/MXwu+gJjC++jpEhqxrsl9L8NVr8Ja+Yw5eK5rNiXzufrj3D/JZ0JD2h+81FFXI6LXJdC07w21V/dJiI6Opo1a9Zw6NAhfH19uffee5k7dy4333wzf//73wkKCmL//v188sknvPvuu6xfv564uDguu+wyQkNDWbNmDceOHaN79+6O5/vxxx/Zs2cPrVq1IiAgAHd3dye/y/qxYt8xXly8m+1Jxpwkfy83Oof5ER7gRbi/sQX7eWD+Q0XIzWwmzN+TMH8vQv098XQ782KwFrOJScPaM2lY4/jmpMnITYGDKyCkC4T1ArMLLMybdwz2fA/Wksr7M+ONClNFwhTRDy56FDqNUqIk527QHbB6NhyIY8gVmQyKbsm6Q8eZuyKep67W3DURqTtN8dpUf4WbiEceeYSJEyfSo0cPCgsLOXjwIL/99huPPvool112GcXFxbRr147LL78cs9mMv78/y5cvZ9asWeTk5NCuXTteffVVrrjiCgDuvPNOli1bxsCBA8nLy+OXX35hxIgRzn2TdWx/Wi7Tv93Bb/uNRWRbeFi468KO3D68Pb6e+l/DJeSmwMpZsOE9KCsy9nkGGGvXRF9gNDeo72/mzO4Q1P7sk7W8Y7DqNVg37/SdkSL6GxP6O4866y5GImcU1B66XG6s67T2He65+P+Y/N46PlqTwP2XdibAu2l+QSYiDa8pXpuqHflJmnsL7brU2D/LFfuOcc+HG8ktLsPdYuLP57VjysWdaOXr6ezQ5GxUlTAFd4XcZCjOafh4zmYYXV6aMTF/3TwoKzT2hfeBoA6Vj3PzhF7XN9mESe3Iq9agn8uBX+CDseDhi/2hHYyes4W9qXk8N7YXfz6vXf2+tojUSGO/nnIVakcuUksfr03gya+3Y7XZGRwdxKs39iUqSB2lGjVrKSRvgUMrjK5yB1eAtdh4LGoIjHgMOlxc3n1u64nuc8lbwFZ2+uc+V6UFRrK270djA6Nxg/sf1uAqzgVbefvwyAFGNanTyCaZHEkj12GE8UVD+h5MWz7mxoGjee67XXy2PlGJk4jIaShxkmbDZrPz4uLd/Ht5PADj+kXywnW9z2puktQjaykc3WwkRSlbjeTnZMU5cGQ9lORV3n9ywlSRfFjcILK/sQ27v0HCr7ZVeEVV6WRKmKQxMJlgyF3w3cOw9h3GTp7ECz/sZsuRbHan5NAtXJVAEZGqKHGSJi+vuIyU7EJeXbKXH7anAPDQyC7cf2mnc2oBLn9gLYWjm8qTH9uZjy/OhsOrIeF3KM0/8/HeLaHdsBPD4cJ6NY7kw2yBiBhjO3+KkUhlHjy10uXmAS3bN46YRfrcBD89A5nxBCevYGT3MBbvSOHTdUeYNkZNIkREqqLESZqcFfuOMXfFQZKOF5CaU0xe8YkLWA+LmZdv6MM1MZFOjLCRKysxOrvZz6LlZ1E2JKw2Ki0Ja84uAapKRVIUNeTUtYksHkajhNAeYDbX7vkbktkCwZ2cHYXI6Xn6nmhNvvbf3DjoLRbvSOGrTUd47IpueLi5wP9rIiINTImTNCnfbU3mgU82UWar3PPEz8uNDsEtePLqHgyKDnJSdI1UWYlRKaqYP5S45vTd3k7HOwjaDDo1+alKRUIUfYHrJEUiTUlFa/L9P3HhqCxC/TxJyy3mp12pXNm7tbOjExFpdJQ4VaGZNRqsF874DL/ceIRHPtuCzQ5X9WnNrYPbEla+FlMLtRc/4WwSJQ8/cD+L7j0WD2M+UfRwIwEK6a4ESMRVnNSa3G3Du1w/4HbeWnaAT9cnKnESaWR0bXpu6urz09XkSSoW0SooKMDb2/sMR8vpFBQYF+INtTDZR2sSeOLrbdjtcOPANsy4tg8Ws+aSAOWJ0sbyROm3qhMl76DyuUMVCVA3JUAizcGQvxprOm3+iBsn/523lh1g+d5jJGcX0jpAfwdFnE3XpnWjpMRYbN5iObeGYEqcTmKxWAgMDCQtLQ0AHx8fNQ+oIbvdTkFBAWlpaQQGBp7zP9CzMX/lQZ75304AJgxtxz/G9MTcnJOmshJI2gCHV56Ye/THDm8+rcobLQyH6GGqFIk0Vx1GQMtoOH6I6MyVDI4OZ+2hTL7YcIQpl3R2dnQizZ6uTc+dzWbj2LFj+Pj44OZ2bqmPEqc/CA8PB3D8A5XaCQwMdHyW9WneyoM8W5403XVhB6Ze0a1p/0IpKzYaN/yxZXdRltGh7tAKSFxbdaIUfQG0uwDaDzfWcFGiJCImE/S4xlicecfX3Djon6w9lMmn649wz4hOzftLKJFGQtem585sNtO2bdtzvkZU4vQHJpOJ1q1bExoaSmlpqbPDcUnu7u4NUmn6cUcKz31nJE33X9KJh0Z1aXpJU1kxJG0sXyOomqSoKhUVpfYXnhh619Q+GxGpGz3GGonTviVcedUbTPewkJBZwNpDmZzXoZWzoxNp9nRteu48PDww18EXxkqcqmGxWBrk4l9qZ3tSNg9+shm7Hf58XtumkzSVFRvD7ByJ0rpTEyXPgFMbN5zSpEGJkoicpYh+ENgWshLwOfQzY/p24JN1iXy6LlGJk0gjomtT51PiJC4nObuQ2xeuo7DUyvDOwfxjTE/XSZpsVtj+Bez+7tQFUguzIGk9lBVV3t8i5MSir9HDIbiLkiIRqTsmk1F1WvU67PyaG4f8i0/WJfL99mSeHdtLXUlFRMrpt6G4lPziMm5fsJ7UnGI6h/oy+9b+uFlcYK5ORcL060uQse/0x56cKLW7AEK6KlESkfrVc6yROO1dQr9rPGjXyofDGQUs33uMK9SaXEQEUOIkLsRqs/PAJ5vYmZxDsK8H8ycNwt+rYdqdn5XSIsg8AHZb5f0p22HFK5Cx3/jZuyUMuhP8/3AxYvGENgNVURKRhhfR3zFcz7Q/jpHdOzFv5UGW7kpV4iQiUk6Jk7iM137ay0+70vBwM/Pv2wYSFeTj3IBKi+DIOjj8W/lCsmvBWlz98d5BcP4UGHwXePo1XJwiImdS0V1v1Ruw82tG9X+ZeSsP8svuNMqsNteo7IuI1DMlTuISDqbn8/avBwB46bo+DGjXsmFeODMelr8Ke74/tQV4aQHY/tDdxisA3P6wQJ2HD/S7DQbfqYRJRBqvHmONxGnPYgZe/QaBPu4cLyhlw+HjDFGTCBERJU7iGp77305KrXYu6hLCNTER9f+CGQdgxauw5ROwW6s/zjescuOGVp00zE5EXFPkAAiIguxE3A7+zCVdo/hyUxI/7UpV4iQighIncQG/7E4jbncabmYT08b0qLsOehkHYH/cqd3tUrbB1kUnEqZOo2DYA+D3h3H+bh7GRYYSJRFpCiqG661+E3Z+w6gez/LlpiSW7kzl8Su7u073UhGReqLESRq1kjIbz/7PWOR28rBoOob4nvuTZhyA5a9UTo6q0vkyuOgxaDPg3F9TRMQV9BhrJE57FjP88ll4WMwcyijgwLE8OoVqqLGING+a7SmN2oJVB4lPzyfY15P7L+18bk+WcQC++hu8ORC2fGQkTdHDofcNlbcBk+COn+HWz5Q0iTRTs2fPJjo6Gi8vL4YMGcLatWtPe3xWVhb33nsvrVu3xtPTky5duvD99983ULR1qM1A8G8DJbn4Jv7K+Z2MIXpLdqY6OTAREedTxUkarbScIl77yVjz6NHLu+J3utbjpYVGV7tDK40tbQfY7ZWPKc4Fyvd1Hg0jHjXG9IuInGTRokXExsYyZ84chgwZwqxZsxg9ejR79uwhNDT0lONLSkoYNWoUoaGhfP7550RGRnL48GECAwMbPvhzVTFc7/fZsONrRnZ/kmV7jvHTzlTuGdHJ2dGJiDiVEidptF5cvIf8Eit9owK5rn+byg+WFBitwCsSpaT1YC0585N2uRwu+rsSJhGp1syZM7nzzjuZPHkyAHPmzOG7775j/vz5PPbYY6ccP3/+fDIzM1m1ahXu7sYXPNHR0Q0Zct2qSJz2/MDIEa/wJLApMYtjucWE+Hk6OzoREadR4iSN0saN6/Db8j63W2xMbh+Nec1m44GCTDi8qupEyS/iRIe7NgPBzavy4x4twC+8QeIXEddUUlLChg0bmDp1qmOf2Wxm5MiRrF69uspzvv32W4YOHcq9997LN998Q0hICLfccguPPvooFouloUKvO20GQYtQyE8jPHcbfdoEsPVINj/vTmX8oLbOjk5ExGmUOEmjYbXZWbVmNablLzO04Bf6u5cPq6tuaoFfBLQfDu2GGbct26vDnYick/T0dKxWK2FhYZX2h4WFsXv37irPiY+P5+eff+bWW2/l+++/Z//+/dxzzz2UlpYyffr0Ks8pLi6muPjEgtk5OTl19ybOldkMHUbAtk8hfhmjuo9n65Fslu5U4iQizZsSJ3G6whIr3/70C4HrX2OkdQUWkx1MsNunPx2iO+Bx8or1bp4QNdioKilREpFGwGazERoayjvvvIPFYmHAgAEkJSXx8ssvV5s4zZgxg6effrqBI62BkxKnkVfdz6tL97JiXzqFJVa8PVywiiYiUgeUOIlz2e38POchrs9435Ew7Wt5IYFXPEm3LkOcHZ2INDPBwcFYLBZSUyt3kUtNTSU8vOqhvq1bt8bd3b3SsLzu3buTkpJCSUkJHh4ep5wzdepUYmNjHT/n5OQQFRVVR++iDnS4yLhN2kC3QBttWnpz5HghK/enM6pH2OnPFRFpotSOXJzHbuf4t09wVeZCLCY7iaEXU/yXX+j8wH8JUdIkIk7g4eHBgAEDiIuLc+yz2WzExcUxdOjQKs8ZNmwY+/fvx2azOfbt3buX1q1bV5k0AXh6euLv719pa1QC2kCrzmC3YTr8GyO7G8nS0p0pTg5MRMR5lDiJc9jtEPc0LTfNBuDjVlOIuudrPNv2d3JgItLcxcbGMnfuXBYuXMiuXbu4++67yc/Pd3TZmzBhQqXmEXfffTeZmZk88MAD7N27l++++47nn3+ee++911lvoW50GGHcxi/jsvIqU9yuNKw2e/XniIg0YRqqJw3Pboef/gG/zQJgeulErrnmEaeGJCJSYfz48Rw7doxp06aRkpJCTEwMixcvdjSMSEhIwGw+8b1jVFQUP/74Iw899BB9+vQhMjKSBx54gEcffdRZb6FudBgB6+ZC/DIGjX4RP083MvJL2JWcQ6/IAGdHJyLS4JQ4ScOqImna0+5m+rdt6dSwRERONmXKFKZMmVLlY8uWLTtl39ChQ/n999/rOaoGFn0BmMyQvhf3vGQGtQ/i591p/B6focRJRJolDdWT+lecB/vj4Ken4d1LHUnTc7bJLLSO5m8XdXRufCIicirvQIjoZ9w/+CtDO7QCYPWBDOfFJCLiRKo4Sd04uBx+nwO2ssr7CzIgeXPl/SYzyzo8zLs7+tG9tT8XdQlp0FBFROQsdRgBSRsgfhlDh1wFwNqDmZRZbbhZ9N2riDQvSpzk3JWVwFd3Q86R6o8JiILo4RB9AUVtLiB2zl6ghLtHdMSktZhERBqnDiNgxasQv4zu1/jh7+VGTlEZO5Nz6NMm0NnRiYg0KCVOcu42fWAkTb7hcMmTlR9z94Y2g6BlO8euRasOkZlfQlSQN1f2qnpdFBERaQTaDAY3b8hLxZKxh8HtW/HTrlRWH8hQ4iQizY7T6+yzZ88mOjoaLy8vhgwZwtq1a6s9trS0lGeeeYaOHTvi5eVF3759Wbx4cQNGK6coK4YVM437w2Oh/22Vt97XV0qaSq023lkeD8BdF3bUUA8RkcbM3Qvala9fFb+MoR3L5znFa56TiDQ/Tr1qXbRoEbGxsUyfPp2NGzfSt29fRo8eTVpaWpXHP/nkk/z73//mjTfeYOfOnfztb39j3LhxbNq0qYEjF4dNH56oNvWfeMbDv9qYRFJWIcG+HtwwoE0DBCgiIufkpPWcKhpErDuYSanVVv05IiJNkFMTp5kzZ3LnnXcyefJkevTowZw5c/Dx8WH+/PlVHv/BBx/w+OOPc+WVV9KhQwfuvvturrzySl599dUGjlyAU6tN7l7VHmqz2Zn9y34e+3IrAJOHtcfL3dIQUYqIyLmoSJwOraRbiBeBPu7kl1jZnpTt1LBERBqa0xKnkpISNmzYwMiRI08EYzYzcuRIVq9eXeU5xcXFeHlVvjj39vZm5cqV9RqrVKNibpNf69NWm9Lzipn43lpe/nEPNjtc2y+SO4a3b8BARUSk1sJ6g3cQlORhTt7IkPZBgIbriUjz47TEKT09HavV6liJvUJYWBgpKSlVnjN69GhmzpzJvn37sNlsLF26lC+//JLk5ORqX6e4uJicnJxKm9SBk6tNF1Rfbfo9PoMrX1vBin3peLmbeen6Prx6Y1883VRtEhFxCWYzdLjIuH/ScD2t5yQizY1Lzcx/7bXX6Ny5M926dcPDw4MpU6YwefJkzObq38aMGTMICAhwbFFRUQ0YcRO26QPISQK/COg/wbE7u7CUFfuO8UbcPm5fsI5b5v5OWm4xnUJ9+XbKBdw4MErtx0VEXM1J85zOK28Qsf7QcUrKNM9JRJoPp7UjDw4OxmKxkJqaWml/amoq4eFVt6gOCQnh66+/pqioiIyMDCIiInjsscfo0KFDta8zdepUYmNjHT/n5OQoeTpXVcxt+mV3Gs99t5MDx/JPOfza/pE8N7YXPh7qfi8i4pIqEqcj6+gSaCKohQeZ+SVsS8piQLsgp4YmItJQnHYl6+HhwYABA4iLi2Ps2LEA2Gw24uLimDJlymnP9fLyIjIyktLSUr744gtuvPHGao/19PTE09OzLkOXje+fqDb1u420nCLu/3gTucVlALRr5UPfNoHERAUyKDqI3m0CnBywiIick5bRxkLm2YmYkzdxXocgvt+WwuoDGUqcRKTZcGoJIDY2lokTJzJw4EAGDx7MrFmzyM/PZ/LkyQBMmDCByMhIZsyYAcCaNWtISkoiJiaGpKQk/vGPf2Cz2fj73//uzLfRvBTlwK8vGvfLq01Pf7aR3OIy+rYJ4L3Jgwlq4eHcGEVEpO5FDYbsREhcw9AONxiJU3wGUy7p7OzIREQahFMTp/Hjx3Ps2DGmTZtGSkoKMTExLF682NEwIiEhodL8paKiIp588kni4+Px9fXlyiuv5IMPPiAwMNBJ76AZWvEq5B+DVp2g/0R+2ZPGd1uTMZvgn+N6K2kSEWmq2gyG7V9A4lrOG/k3wJjnVFxmVcMfEWkWnD7pZMqUKdUOzVu2bFmlny+66CJ27tzZAFFJlTLj4fe3jPuX/ZNCm4Wnvt4OwF+GtadXpIbkiYg0WVGDjdvEtXQK8SHY15P0vGK2JGYzuL2G64lI0+dSXfXEyZZOA2sJdLwEuozmtbh9HDleSESAFw+N6uLs6EREpD6F9wY3byjKwpSxn/M6lK/npLbkItJMKHGSs3NwBez6L5gsMPp5dqfm8u6KeACevqYXLTydXrwUEZH6ZHGHyAHG/cQ1nFexnlN8uhODEhFpOEqc5MxsVlg81bg/8C/Ygrvx+JfbKLPZGd0zjFE9wk5/voiINA2O4XprGFq+ntPGhCyKSq1ODEpEpGEocZIz2/QBpG4DrwAYMZVF6xPZmJBFCw8L//hTT2dHJyIiDSVqiHGbuJYOwS0I9vWgpMzGzuQc58YlItIAlDjJ6RVlQ9yzxv0RU7F6B/Hmz/sBiL2sK60DvJ0YnIiINKg2g4zb9L2YCo8TExUIwOaELKeFJCLSUJQ4yemtnAUF6dCqMwy6g7hdqSRlFdLSx51bh7R1dnQiItKQWrQy/h4AHFlH3zaBAGxOzHJaSCIiDUWJk1TPWgob3zfuj5wOFncWrj4EwPhBbfFy17odIiLNjmO43hpi2gYCSpxEpHlQ4iTVO/CzUW1qEQJdrmBfai6/7c/AbII/n6dqk4hIs3TSek59yitOCZkFZOaXOC8mEZEGoMRJqrflY+O29w1gcXNUm0b1CKNNSx/nxSUiIs5TUXFK2kCAh4kOIS0A2KKqk4g0cUqcpGpF2bD7e+N+n/HkFJXy5cYkACYOjXZeXCIi4lzBXYwuq6UFkLrd0SBikxInEWnilDhJ1XZ+A9ZiCOkGrfvy+fojFJRY6RLm61i7Q0REmiGzGdqcGK7Xr6KznhInEWnilDhJ1bYsMm77jMdmh/fLh+lNGBqNyWRyXlwiIuJ8Jy2E27c8cdqSmIXdbndeTCIi9UyJk5zq+GE4vBIwQZ8b+XXfMQ5lFODn5ca4fpHOjk5ERJztpAYR3cL98XAzk11YyqGMAufGJSJSj5Q4yam2fWrcth8OAW1YuOoQADcMiKKFp5vz4hIRkcYhcgCYzJCdgEdBCr0i/AHYnHjcyYGJiNQfJU5Smd1+0jC9mziYns+yPccwmWDC0HbOjU1ERBoHTz8I62ncT1xLTFRLADYnZDkvJhGReqbESSpL2ggZ+8DNG3v3Mbzx8z4ARnQJITq4hZODExGRRsOxEO5a+kYFALD5SLYTAxIRqV9KnKSyrZ8Yt92u4l8rUhwtyO+6sKMTgxIRkUbHkTitoV95xWnX0RyKy6xODEpEpP4ocZITrKWw/QsAFrtdzOtxRrXpmWt6qgW5iIhUVtEgInkLUX4Q1MKDEquNnUdznBuXiEg9UeIkJ+z/CQoyKPRsxb2/GxN9Hx7VhQla8FZERP4osB20CAFbKabUHfRtUz5cT+s5iUgTpcRJTtj4PgAf5Q/GioU7h7dnyiWdnByUiIg0SiYTRPQ37h/d6GgQsUWJk4g0UUqcxJC8FfZ8j81u4iPrJYwfGMXjV3bXYrciIlK9yPLEKWkjMW0DAVWcRKTpUuIklFlt7Pv0SQD+axtKt14Def7a3kqaRETk9CL6GbdHNzqG6h3KKOB4fokTgxIRqR9KnJq5lOwiHn/rP3Q+/is2u4mDPe7hX+NjsJiVNImIyBlUDNVL30eguYj25ctWbD6S5byYRETqiRKnZmzZnjSufH0Fl6YtACA56koevHkMHm76ZyEiImfBNwQCogA7JG8hJioQ0DwnEWmadIXcTL297ACT3ltH64K9jLasx46JyGv+4eywRETE1Zw0XK8icdI8JxFpipQ4NUMLfjvIi4t3A/ByyA8AmHpfDyFdnBmWiIi4opMaRPQ9qeJkt9udF5OISD1Q4tTMfLnxCP/4704Anhtio0fOCjCZ4cK/OzkyERFxSSe1JO/e2g8Pi5njBaUkZBY4Ny4RkTqmxKkZWbIjhf/7fCsAk86P5taij4wHeqnaJCIitdS6r3GblYBn8XG6hvsBsONojhODEhGpe0qcmolVB9KZ8vEmrDY71/Vvw7QBpZj2fG9Umy5StUlE5GSzZ88mOjoaLy8vhgwZwtq1a6s9dsGCBZhMpkqbl5dXA0brZN6B0Kp8sfSjm+kV6Q/A9qRs58UkIlIPlDg1A5sSjnPnwvWUlNkY3TOMF6/rjXn5i8aDvW+A4M7ODVBEpBFZtGgRsbGxTJ8+nY0bN9K3b19Gjx5NWlpatef4+/uTnJzs2A4fPtyAETcCJw3X6xlhrOe0XRUnEWlilDg1cT/uSOGWuWvIL7EyrFMrXrupH26pW6Gi2nTh/zk7RBGRRmXmzJnceeedTJ48mR49ejBnzhx8fHyYP39+teeYTCbCw8MdW1hYWANG3Aic1CCiV6SROO1IylaDCBFpUpQ4NVF2u525y+P524cbKCy1clGXEN65bSBe7hb4VdUmEZGqlJSUsGHDBkaOHOnYZzabGTlyJKtXr672vLy8PNq1a0dUVBTXXHMNO3bsaIhwG4+TKk7dwnyxmE1k5JeQklPk3LhEROqQEqcmqMxq44mvt/PP73dht8Ofz2vLvIkDaeHpBkc3q9okIlKN9PR0rFbrKRWjsLAwUlJSqjyna9euzJ8/n2+++YYPP/wQm83G+eefz5EjR6p9neLiYnJyciptLi28N5gskJeKV2EqnUN9Adie5OLvS0TkJEqcmpjk7EImL1jHR2sSMJngyau68+w1vXCzlP+nXvaCcatqk4hInRg6dCgTJkwgJiaGiy66iC+//JKQkBD+/e9/V3vOjBkzCAgIcGxRUVENGHE98PCB0O7G/aMnhuttU4MIEWlC3JwdgNSezWZn3aFMNiVmsTkhiy1HskjONoZFeLtbeO2mGC7rGX7ihKObYO8PqjaJiFQjODgYi8VCampqpf2pqamEh4dXc1Zl7u7u9OvXj/3791d7zNSpU4mNjXX8nJOT4/rJU0Q/SN0ORzfRK6IXn28w5jmJiDQVqji5KKvNzl0frGf8O7/zwg+7WbwjheTsIswm6NsmgE//OrRy0gSwTHObREROx8PDgwEDBhAXF+fYZ7PZiIuLY+jQoWf1HFarlW3bttG6detqj/H09MTf37/S5vKqaBCx/agSJxFpOlRxclGvxe3jp11peLqZuaRbKH2jAomJCqR3ZIAxl+mPKlWbtG6TiEh1YmNjmThxIgMHDmTw4MHMmjWL/Px8Jk+eDMCECROIjIxkxowZADzzzDOcd955dOrUiaysLF5++WUOHz7MHXfc4cy30fAcDSI20T3cD5MJUnOKScstItSvGa1rJSJNlhInF/Tz7lRej9sHwAvX9WZcvzZnPskxt+lGCO5Uj9GJiLi28ePHc+zYMaZNm0ZKSgoxMTEsXrzY0TAiISEBs/nEgI3jx49z5513kpKSQsuWLRkwYACrVq2iR48eznoLzhHWEyyeUJRFi/wEOgS34MCxfHYczSG0qxInEXF9JnszW2QhJyeHgIAAsrOzXXJoREJGAVe/sYKcojJuO68dz47tdeaTkjbC3IuNatO965Q4iYjTuPrv4PrSZD6XuZdC0nq4bh4P7OjIN5uP8shlXZhyiYaHi0jjVJPfv5rj5EKKSq387cMN5BSV0a9tIE9dfRbfZhZkwn/vN+6r2iQiIvUpop9xm7SRXhHl85zUklxEmgglTi7Cbrfz5Nfb2ZmcQ6sWHrx1a3883M7wn68gEz4YCynbwCcYLp7aILGKiEgzFXliIVw1iBCRpkaJk4v4dH0in284gtkEb9zcj9YB3qc/oSAT3r8GkrcYSdPE/0LL6AaJVUREmqmKBhHJW+gR7gPAkeOFZBWUODEoEZG6ocTJBeQWlfLCD7sBePiyrpzfKfj0J1QkTSlbTyRNYc1skrKIiDS84M7g4QulBQTkxdOulZE87Tiq4Xoi4vqUOLmAeSsPcryglA4hLfjrhR1Of/DJSVOLEJj0PyVNIiLSMMyWk+Y5bXDMc9qmhXBFpAlQ4uRk+9Nyee+3gxSVWqt8/Hh+Ce+uOAhA7KguuFlO85+srAQ+vulE0jTxvxDavT7CFhERqVrkAOM2aQM9I40OVduVOIlIE6B1nJxs6pfbWHfoONuOZPPqjX0xmUyVHp+z/AB5xWV0b+3Plb2qX4UegB/+DolrwDNASZOIiDjHSYlTr0uMipOG6olIU6CKkxPlFJWyMSELgC83JfHhmoRKj6flFLFw1SEAHrmsC2aziWptWAAb3gNMcP08JU0iIuIcFYlT6k56hhjfzx5Mzye3qNSJQYmInDslTk70+4EMrDY77hYjIXrmvzvYlHDc8fjsX/ZTVGqjX9tALukWWv0TJa6F7x4x7l/yJHQeVZ9hi4iIVM8/AnzDwW6lVe4eIgK8ANipqpOIuDglTk60cn86ADcNasvlPcMptdq55z8bycgr5sjxAj5aa1Sg/u+yrqcM4XPISYZFt4GtFLr/CYY/3FDhi4iInMpkqjxcz7GekxInEXFtmuPkRBWJ0/DOwQzt2Iq9abnEH8vnvo830TrAm1KrnfM7tjrRftxmBbv9xBNYS+DTCZCXAqE9YOzbxh8sERERZ4rsD3u+K0+cLmPJzlR2qEGEiLg4JU5OcjSrkPhj+ZhNcF7HVvh5uTPnzwMYO/s3Vh3IcBz3yOiuUJQN/3sItn8J2E99Mq8AuOk/4OnbcG9ARESkOidXnHqVd9Y7qsRJRFybhuo5SUW1qW9UIP5e7gB0CfPjxev6OI65tFso/S0H4d8XwvYvqDppCoTr34OgM6zvJCIi0lAq1nI6fojeLcsA2J+WR0FJmRODEhE5N6o4OclvFcP0KobhlRvTN4L4Y/l8uTGRF9qsgnnPGvOXAtvCtXMhpGvlJ3L3ATfPhgpbRETkzLwDoVVnyNhHSM5OQv08ScstZldyDgPaBTk7OhGRWlHFyQlsNrsjcRr2h8QJ4IFhwfzabj4hK6cZSVO3q+GvK6DteeDdsvKmpElERBqjNgON2yPr6dPGaBCxJVHD9UTEdSlxcoI9qbmk55Xg7W6hX9uWJx4ozIJlL8JrfWHXf8HsDpe/COM/NL69ExERcRUnzXPqHRkIwDY1iBARF6ahek5QUW0a0iEIDzezkTCtmQOr34Li8j8qId1h7OwTf3hERERcSWR/4zZpA30GGA0ith7Jcl48IiLnSImTE6zYZyROl7TzhGUv/CFh6gYXPQo9xoJZBUEREXFRYb3A4gGFmfT1ywIgPj2f3KJS/MqbIomIuBIlTg2suMzKroOJPOj2P279/ScoKV8QUAmTiIg0JW6eEN4bkjYQdHwbkYFBJGUVsj0ph6EdWzk7OhGRGlPi1JCKczn23Uv8ZH4Xf1MBlGAMyRvxKHS/RgmTiIg0LZEDIGkDJG2kd+R1JGUVsi0pS4mTiLgkXak3pC/vos3W1/E3FZDsEQ03LIC7V0HPcUqaRESk6Tm5QUR5Z72tR9QgQkRck67WG0rqTtjzPVbM3FcyhZWjvlXCJCIiTVtF4pS8hZiIFoA664mI69JVe0P5/S0AfrQO5L+287mgS6iTAxIREalnQR3BMwDKCunjkQzA4YwCsgpKnByYiEjNKXFqCHlpsPVTAN4tu5KOIS1oHeDt5KBERETqmdkMkf0A8MvYQrtWPoCqTiLimpQ4NYR188BaTIJPTzbau3BBp2BnRyQiItIwIgcat0nr6R2peU4i4rqcnjjNnj2b6OhovLy8GDJkCGvXrj3t8bNmzaJr1654e3sTFRXFQw89RFFRUQNFWwulhbDuXQA+db8GgIHRQc6MSEREpOE4GkRspG+bQAC2KXESERfk1MRp0aJFxMbGMn36dDZu3Ejfvn0ZPXo0aWlpVR7/0Ucf8dhjjzF9+nR27drFvHnzWLRoEY8//ngDR14DWz+FgnQIaMt3JcYfj9YBXk4OSkREpIFE9jdu03bRN8wCwNYjWc6LR0SklpyaOM2cOZM777yTyZMn06NHD+bMmYOPjw/z58+v8vhVq1YxbNgwbrnlFqKjo7nsssu4+eabz1ilchq7HVbPNu4O+SspeWUAhPh5OjMqERGRhuMXDoFtATu9bHswmeBodhHHcoudHZmISI04LXEqKSlhw4YNjBw58kQwZjMjR45k9erVVZ5z/vnns2HDBkeiFB8fz/fff8+VV17ZIDHX2P44SN8DHn7k97qFwlIrAMG+SpxERKQZaXcBAD5Hf6dDsNGWfLsaRIiIi3Fa4pSeno7VaiUsLKzS/rCwMFJSUqo855ZbbuGZZ57hggsuwN3dnY4dOzJixIjTDtUrLi4mJyen0tZgVr9p3A6YSHqJkSz5eFho4enWcDGIiIg4W7vzjdvDqxzznNQgQkRcjdObQ9TEsmXLeP7553nrrbfYuHEjX375Jd999x3PPvtstefMmDGDgIAAxxYVFdUwwabugPhfwGSGIX/lWJ4xJEHD9EREpNmpSJySNhDT2vg7qHlOIuJqnJY4BQcHY7FYSE1NrbQ/NTWV8PDwKs956qmnuO2227jjjjvo3bs348aN4/nnn2fGjBnYbLYqz5k6dSrZ2dmOLTExsc7fS5XWzDFue1wDgW0dY7lDNExPRESam6AO4NcarCUM8TgIwNakbOx2u5MDExE5e05LnDw8PBgwYABxcXGOfTabjbi4OIYOHVrlOQUFBZjNlUO2WIwOPdX98vX09MTf37/S1iAS1xm3fW8GIL284qT5TSIi0uyYTI6qU4f8LZhNcCy3mNQcNYgQEdfh1KF6sbGxzJ07l4ULF7Jr1y7uvvtu8vPzmTx5MgATJkxg6tSpjuPHjBnD22+/zSeffMLBgwdZunQpTz31FGPGjHEkUI2CzQbHjW/UCO4McKLipKF6IiLSHJUnTu5HVtElzA/QcD0RcS017lIQHR3NX/7yFyZNmkTbtm3P6cXHjx/PsWPHmDZtGikpKcTExLB48WJHw4iEhIRKFaYnn3wSk8nEk08+SVJSEiEhIYwZM4Z//vOf5xRHnctJgrIiMLtBgPEZKXESEZFmrbyzHonr6NfFh90puWw9ks1lPaseni8i0tjUuOL04IMP8uWXX9KhQwdGjRrFJ598QnFx7UvtU6ZM4fDhwxQXF7NmzRqGDBnieGzZsmUsWLDA8bObmxvTp09n//79FBYWkpCQwOzZswkMDKz169eLzHjjtmU0WIzcVImTiIg0ayFdwacVlBVyod9RwJjnJCLiKmqVOG3evJm1a9fSvXt37rvvPlq3bs2UKVPYuHFjfcToejIPGLdBHRy7NMdJRESaNZMJ2hpzmGOs2wHYdiRLDSJExGXUeo5T//79ef311zl69CjTp0/n3XffZdCgQcTExDB//vzm/YswoyJx6ujYpYqTiIg0e9HGcL3Q4xtwt5g4XlDK4YwCJwclInJ2ap04lZaW8umnn/KnP/2Jhx9+mIEDB/Luu+9y3XXX8fjjj3PrrbfWZZyuJbO8MUR5xclut5OeVwIocRIRkWasvEGEJXEN/SKNBhFrD2Y6MyIRkbNW4+YQGzdu5L333uPjjz/GbDYzYcIE/vWvf9GtWzfHMePGjWPQoEF1GqhLqRiq18pInHIKyyixGutMtWrh4ayoREREnCusF3gGQHE2V4WmszbBgzUHM7lxUAMtTi8icg5qnDgNGjSIUaNG8fbbbzN27Fjc3d1POaZ9+/bcdNNNdRKgy7HZTqo4GUP1juUVAeDv5YaXeyNqmy4iItKQzBZoex7s+5FhHnuBXqw5mOHsqEREzkqNE6f4+HjatWt32mNatGjBe++9V+ugXFpOEliLy1uRG9+gpWl+k4iIiKHd+bDvR6JzN2Ex9+bI8UKSsgqJDPR2dmQiIqdV4zlOaWlprFmz5pT9a9asYf369XUSlEurGKanVuQiIiKnKm8Q4Xbkd3q39gVgrapOIuICapw43XvvvSQmJp6yPykpiXvvvbdOgnJpFWs4ndRRr6IxhFqRi4i4htmzZxMdHY2XlxdDhgxh7dq1Z3XeJ598gslkYuzYsfUboCtr3RfcfaDwOFeGG+s4qUGEiLiCGidOO3fupH///qfs79evHzt37qyToFxaxqlrOKniJCLiOhYtWkRsbCzTp09n48aN9O3bl9GjR5OWlnba8w4dOsQjjzzC8OHDGyhSF2Vxh6jBAFzosReANfFKnESk8atx4uTp6Ulqauop+5OTk3Fzq/GUqaanouLUSms4iYi4opkzZ3LnnXcyefJkevTowZw5c/Dx8WH+/PnVnmO1Wrn11lt5+umn6dChQ7XHSbl2xnC9DgVbMJkgPj2ftNwiJwclInJ6NU6cLrvsMqZOnUp2drZjX1ZWFo8//jijRo2q0+BckmOo3ok/nOl5RuKkoXoiIo1bSUkJGzZsYOTIkY59ZrOZkSNHsnr16mrPe+aZZwgNDeX2228/q9cpLi4mJyen0taslK/n5JH0O93CtJ6TiLiGGidOr7zyComJibRr146LL76Yiy++mPbt25OSksKrr75aHzG6jkqtyDVUT0TE1aSnp2O1WgkLC6u0PywsjJSUlCrPWblyJfPmzWPu3Lln/TozZswgICDAsUVFNbN1jCIHgJsX5KUypnUWoMRJRBq/GidOkZGRbN26lZdeeokePXowYMAAXnvtNbZt29b8fvH/Uc6R8lbk7o5W5ADHyitOIao4iYg0Kbm5udx2223MnTuX4ODgsz6vYuRGxVZV06Umzd0L2l8IwKWWTYASJxFp/Go1KalFixbcdddddR2L66sYpndSK3KrzU5GnipOIiKuIDg4GIvFcspc3tTUVMLDw085/sCBAxw6dIgxY8Y49tlsNgDc3NzYs2cPHTt2POU8T09PPD2b+d+ELqNh3xI6HF8JDGR3Si7H80to2cLD2ZGJiFSp1t0cdu7cSUJCAiUlJZX2/+lPfzrnoFxWFR31jheUYLODyQRB+mMgItKoeXh4MGDAAOLi4hwtxW02G3FxcUyZMuWU47t168a2bdsq7XvyySfJzc3ltdde00iM0+k8GngY96Pr6R9sY2O6mXWHMrms56kJqohIY1DjxCk+Pp5x48axbds2TCYTdrsdAJPJBBidhZqt03TUC/LxwN1S45GRIiLSwGJjY5k4cSIDBw5k8ODBzJo1i/z8fCZPngzAhAkTiIyMZMaMGXh5edGrV69K5wcGBgKcsl/+IDAKwnpD6jZuarmbjek9WHNQiZOINF41vpJ/4IEHaN++PWlpafj4+LBjxw6WL1/OwIEDWbZsWT2E6EKq6KinxhAiIg0jMTGRI0eOOH5eu3YtDz74IO+8806Nnmf8+PG88sorTJs2jZiYGDZv3szixYsdDSMSEhJITk6u09ibrS6jARhmXQdonpOING41rjitXr2an3/+meDgYMxmM2azmQsuuIAZM2Zw//33s2nTpvqI0zVUMVRPrchFRBrGLbfcwl133cVtt91GSkoKo0aNomfPnvznP/8hJSWFadOmnfVzTZkypcqhecAZvyRcsGBBDaJu5rpeASteoXX6Kty5lR1Hs8kpKsXfy93ZkYmInKLGFSer1Yqfn7HmQnBwMEePHgWgXbt27Nmzp26jcyU2Kxwvb0WuxW9FRBrc9u3bGTx4MACffvopvXr1YtWqVfznP/9RMtNYRfSHFiGYS3K5OvAgNjtsOHzc2VGJiFSpxolTr1692LJlCwBDhgzhpZde4rfffuOZZ55p3qul5ySBtcRoRe7fxrFbiZOISMMoLS11dKr76aefHM2KunXrpqF1jZXZXN4kAsb5bAdgTbyG64lI41TjxOnJJ590tFp95plnOHjwIMOHD+f777/n9ddfr/MAXUbFML2TWpHDiTWcgn3VUU9EpD717NmTOXPmsGLFCpYuXcrll18OwNGjR2nVqpWTo5Nqlc9z6l/0O2Bn7cEM58YjIlKNGs9xGj16tON+p06d2L17N5mZmbRs2dLRWa9ZqqKjHpyY46SKk4hI/XrxxRcZN24cL7/8MhMnTqRv374AfPvtt44hfNIIdbwYLB74FiTS0XSUrUfM5BeX0cKz1iumiIjUixpVnEpLS3Fzc2P79u2V9gcFBTXvpAmq7KgHJw3V8/Vq6IhERJqVESNGkJ6eTnp6OvPnz3fsv+uuu5gzZ44TI5PT8vSD6AsAuM53O2U2Oyv2pTs5KBGRU9UocXJ3d6dt27bNe62m6lTRUQ80x0lEpKEUFhZSXFxMy5YtATh8+DCzZs1iz549hIaGOjk6Oa0uVwBwlacxh3rJzhRnRiMiUqUaz3F64oknePzxx8nM1OTNSqoYqldqtXG8oBTQHCcRkfp2zTXX8P777wOQlZXFkCFDePXVVxk7dixvv/22k6OT0+pyGQBt87cRQB4/706jzGpzclAiIpXVOHF68803Wb58OREREXTt2pX+/ftX2pqlk1uRn1RxysgrAcBiNtHSR4mTiEh92rhxI8OHDwfg888/JywsjMOHD/P+++837+ZFrqBlNIR0x2S3cqX3drIKSll3SG3JRaRxqfHMy7Fjx9ZDGC4u+4jRitziAQFRjt0Vw/SCfT0wm5v5HDARkXpWUFDgWGdwyZIlXHvttZjNZs477zwOHz7s5OjkjLpeDsd2cYP/Tj4uPI+lO1MZ2lHdEEWk8ahx4jR9+vT6iMO1VQzTaxkNZotjd7qjFbnmN4mI1LdOnTrx9ddfM27cOH788UceeughANLS0vD393dydHJGXS6Hlf+id8Fa3JjAkp0pPHV1dzWfEpFGo8ZD9aQKmWoMISLibNOmTeORRx4hOjqawYMHM3ToUMCoPvXr18/J0ckZtRkELUJwL83hUvftHDleyO6UXGdHJSLiUOPEyWw2Y7FYqt2apcyK+U2V13CqWPw2RBUnEZF6d/3115OQkMD69ev58ccfHfsvvfRS/vWvfzkxMjkrZgv0uh6AO/zXALBkR6ozIxIRqaTGQ/W++uqrSj+XlpayadMmFi5cyNNPP11ngbkUR0e9qitOwao4iYg0iPDwcMLDwzly5AgAbdq00eK3riTmZljzNv0LV+PPrSzZmcIDIzs7OyoREaAWidM111xzyr7rr7+enj17smjRIm6//fY6CcylFGQYt75hlXar4iQi0nBsNhvPPfccr776Knl5eQD4+fnx8MMP88QTT2A2a3R6oxfeB0J7YEnbydWWNXx09FKSsgqJDPR2dmQiInU3x+m8884jLi6urp7OtZQWGrdulX+xa46TiEjDeeKJJ3jzzTd54YUX2LRpE5s2beL555/njTfe4KmnnnJ2eHI2TCboexMAE3xWA7B0hxbDFZHGoU4Sp8LCQl5//XUiIyPr4ulcT0Xi5F45cUpX4iQi0mAWLlzIu+++y913302fPn3o06cP99xzD3PnzmXBggXODk/OVu8bwWSmW+lO2ppSWbpL85xEpHGo8VC9li1bVmoNarfbyc3NxcfHhw8//LBOg3MZZUXGrbtXpd3H1I5cRKTBZGZm0q1bt1P2d+vWjczMTCdEJLXi3xo6XAwH4rjOsoLX48PJLiglwMfd2ZGJSDNX48TpX//6V6XEyWw2ExISwpAhQ2jZsmWdBucyqhiqV1RqJbeoDFDFSUSkIfTt25c333yT119/vdL+N998kz59+jgpKqmVvjfDgTjGu69kVuG1/LInjbH9mumoFhFpNGqcOE2aNKkewnBxVVScKuY3ebiZ8feq8ccsIiI19NJLL3HVVVfx008/OdZwWr16NYmJiXz//fdOjk5qpNtV4OFHeEkaA017WbIzQomTiDhdjec4vffee3z22Wen7P/ss89YuHBhnQTlUux2KC0w7rv7OHaf3FFPq56LiNS/iy66iL179zJu3DiysrLIysri2muvZceOHXzwwQfODk9qwsMHehpdfK+1rODXPccoKrU6OSgRae5qnDjNmDGD4ODgU/aHhoby/PPP10lQLsVaCnabcd/tRMUpXWs4iYg0uIiICP75z3/yxRdf8MUXX/Dcc89x/Phx5s2b5+zQpKb63gzAGLc1lJUU8qO664mIk9U4cUpISKB9+/an7G/Xrh0JCQl1EpRLKSs8cf+krnpaw0lEROQctD0fAtriSwGXmdezaF2isyMSkWauxolTaGgoW7duPWX/li1baNWqVZ0E5VJKy+c3YQKLh2O31nASERE5B2Yz9B0PGMP1Vh3I4HBGvpODEpHmrMaJ080338z999/PL7/8gtVqxWq18vPPP/PAAw9w00031UeMjdvJ85tOmsuU7qg4eVR1loiIiJxJ+XC9iyzbCCeDT9er6iQizlPjdm/PPvsshw4d4tJLL8XNzTjdZrMxYcKE5jnHqbo1nFRxEhFpENdee+1pH8/KymqYQKTuteoI7S7AfHglt7ktZeH6CB4a2QU3S42/9xUROWc1Tpw8PDxYtGgRzz33HJs3b8bb25vevXvTrl27+oiv8atiDSeAjLwSAFppjpOISL0KCAg44+MTJkxooGikzp13Nxxeya1uP/NG7jh+3XuMS7uHOTsqEWmGar3AUOfOnencuXNdxuKaqqk45RSVAhDgrZXORUTq03vvvefsEKQ+db0CAtsRmHWYsZbf+GRdWyVOIuIUNa51X3fddbz44oun7H/ppZe44YYb6iQol+KY41S54pRTWAaAv5cSJxERkVozW2DwXQBMtizm592ppOUUneEkEZG6V+PEafny5Vx55ZWn7L/iiitYvnx5nQTlUiq66v1hqF52oSpOIiIidaL/beDhS1fzEc5jG19sTHJ2RCLSDNU4ccrLy8PD49ROce7u7uTk5NRJUC6liqF6JWU2CstXOPf3rvVoSBEREQHwCoCYWwD4i2Uxi9YlYLfbnRyUiDQ3NU6cevfuzaJFi07Z/8knn9CjR486CcqlVNEcomJ+E4CfhuqJiIicuyF/A+BSyybIPMCag5lODkhEmpsal0Oeeuoprr32Wg4cOMAll1wCQFxcHB999BGff/55nQfY6FUkTifNccopH6bn5+mGxWyq6iwRERGpiVYdofNo2Pcjkyw/8um6gZzXoZWzoxKRZqTGFacxY8bw9ddfs3//fu655x4efvhhkpKS+Pnnn+nUqVN9xNi4lVWROBWVN4bQ/CYREZG6c55RdbrB8ivLt+2vNMJDRKS+1WoFuauuuorffvuN/Px84uPjufHGG3nkkUfo27dvXcfX+DmaQ5yY41TRGEKJk4iISB3qcDH2kG60MBVzjf1nftiW7OyIRKQZqfXS28uXL2fixIlERETw6quvcskll/D777/XZWyuoaqKU0Xi5KXGECIiInXGZMJUPtdpkmUJ32xMcHJAItKc1OjKPiUlhQULFjBv3jxycnK48cYbKS4u5uuvv26ejSHgpOYQp1ac1IpcRESkjvUZj/WnZ4gqOkbw4e9JyhpAZKD3mc8TETlHZ11xGjNmDF27dmXr1q3MmjWLo0eP8sYbb9RnbK7B0RzCx7GrYsy1huqJiIjUMQ8fLEPvAeA+t6/5dtMRJwckIs3FWSdOP/zwA7fffjtPP/00V111FRaLpT7jch1VrOOUU1jeHEKtyEVEROrekLsocfOjszmJ9HWfak0nEWkQZ504rVy5ktzcXAYMGMCQIUN48803SU9Pr8/YXIOG6omIiDQsrwBsg/8KwPV5n7DzaJZz4xGRZuGsE6fzzjuPuXPnkpyczF//+lc++eQTIiIisNlsLF26lNzc3PqMs/Gqah0nx1A9NYcQERGpD14X3EuRyZvu5gR2/rLI2eGISDNQ4656LVq04C9/+QsrV65k27ZtPPzww7zwwguEhobypz/9qT5ibNwcQ/VO7aqnipOIiEg98QniaNcJAPQ68A5Wq83JAYlIU1frduQAXbt25aWXXuLIkSN8/PHHdRWTa3EM1auqHbkSJxERkfrS5opHKMST7vYD7FrxhbPDEZEm7pwSpwoWi4WxY8fy7bff1sXTuZaqmkMUlTeHUMVJRESk3ngEhLIhZBwAvmtmgppEiEg9qpPEqVmrouKk5hAiIq5t9uzZREdH4+XlxZAhQ1i7dm21x3755ZcMHDiQwMBAWrRoQUxMDB988EEDRtu8+V0SS5HdnejCnRTtjXN2OCLShClxOld/aA5ht9tPDNVTcwgREZezaNEiYmNjmT59Ohs3bqRv376MHj2atLS0Ko8PCgriiSeeYPXq1WzdupXJkyczefJkfvzxxwaOvHnq060L/3W/DIC8JTOcHI2INGVKnM5VWeXEqaDESpnNGCqgipOIiOuZOXMmd955J5MnT6ZHjx7MmTMHHx8f5s+fX+XxI0aMYNy4cXTv3p2OHTvywAMP0KdPH1auXNnAkTdPJpOJ433vptjuRnDGejjwi7NDEpEmqlEkTjUZEjFixAhMJtMp21VXXdWAEZ+ktHyOU/k6ThWtyN3MJrzdtUiwiIgrKSkpYcOGDYwcOdKxz2w2M3LkSFavXn3G8+12O3FxcezZs4cLL7yw2uOKi4vJycmptEntjTyvH/+xGv/NSn6crrlOIlIvnJ441XRIxJdffklycrJj2759OxaLhRtuuKGBI8f4xfyHilNO4YnGECaTqeFjEhGRWktPT8dqtRIWFlZpf1hYGCkpKdWel52dja+vLx4eHlx11VW88cYbjBo1qtrjZ8yYQUBAgGOLioqqs/fQHHUI8WVd1GTy7Z54pG2BXc2wWZWI1DunJ041HRIRFBREeHi4Y1u6dCk+Pj7OSZwqOuqBI3FSYwgRkebHz8+PzZs3s27dOv75z38SGxvLsmXLqj1+6tSpZGdnO7bExMSGC7aJuuPywbxrNUaflC55GqxlTo5IRJoap3YvqBgSMXXqVMe+mgyJAJg3bx433XQTLVq0qPLx4uJiiouLHT/X6XCIisYQ4Oiqd2INJzWGEBFxNcHBwVgsFlJTUyvtT01NJTw8vNrzzGYznTp1AiAmJoZdu3YxY8YMRowYUeXxnp6eeHp61lncAgPaBfFu+wlkJiwhKOsAbPkY+t/m7LBEpAlxasWptkMiKqxdu5bt27dzxx13VHtMvQ6HqKg4md3AYiRKFXOctIaTiIjr8fDwYMCAAcTFnWhrbbPZiIuLY+jQoWf9PDabrdKXdtIw7hndn9llYwEojfvniXnIIiJ1wOlD9c7FvHnz6N27N4MHD672mHodDnGaNZyUOImIuKbY2Fjmzp3LwoUL2bVrF3fffTf5+flMnjwZgAkTJlQaKTFjxgyWLl1KfHw8u3bt4tVXX+WDDz7gz3/+s7PeQrPVu00AqV3+TJK9Fe75ybDuXWeHJCJNiFPHk9V2SARAfn4+n3zyCc8888xpj6vX4RB/WMMJTmoO4aXESUTEFY0fP55jx44xbdo0UlJSiImJYfHixY7REQkJCZjNJ753zM/P55577uHIkSN4e3vTrVs3PvzwQ8aPH++st9Cs3Te6F6+9cR0vub9D2a+v4NZ/Anj5OzssEWkCnFpxOpchEZ999hnFxcXO/UavYqieu5djl5pDiIi4vilTpnD48GGKi4tZs2YNQ4YMcTy2bNkyFixY4Pj5ueeeY9++fRQWFpKZmcmqVauUNDlR13A/SnvdyH5bBG7Fx2H1m84OSUSaCKcP1avpkIgK8+bNY+zYsbRq1aqhQz6hiqF6J+Y4qTmEiIiIM9w3sjszrTcCYP3tDchNPcMZIiJn5vSr+5oOiQDYs2cPK1euZMmSJc4I+YQqKk45qjiJiIg4VYcQX3xjxrFp2//oV7Yffn4Grpnt7LBExMU5PXECY0jElClTqnysqnUwunbtir0xrApeWmDcuvs4djmaQ2iOk4iIiNPcd2kXYrdM4DPzNOyb/oNp0J0QEePssETEhTl9qJ5Lq2hz6nZSxamovDmEKk4iIiJOExXkQ0SvC/naej4m7LB4KjSGL11FxGUpcToXZVV11dNQPRERkcbg1iHteLH0ZgrtHpCwCnZ+7eyQRMSFKXE6F1VVnBxD9RrFKEgREZFma1B0S3xD2zGnbIyxY8k0LYorIrWmxOlcOOY4GRUnq81ObrExVE8VJxEREecymUzcOqQt/7ZezTFTMGQnqD25iNSaEqdz4eiqZyROueWtyAH81BxCRETE6a4d0AaTuw/PFpevrbViJuQkOzcoEXFJSpzOhWMdJ2OoXk6hUW3ydrfg4aaPVkRExNn8vdz5U98IvrWdzyHvHlCaDz8/6+ywRMQF6er+XPyh4pStxhAiIiKNzq3ntQVMPJJ7i7Fj838gcZ1TYxIR16PE6VxUzHGqqDiVD9Xz91ZjCBERkcaiT5tAekcGsL6sA3vC/2Ts/C4WbFbnBiYiLkWJ07mo6MxTvgCuWpGLiIg0TrcOaQvAYznXYfcKgJStsG6ek6MSEVeixOlcOIbqGRWnbEcrciVOIiIijcmYvhH4ebqxKdOdA70fNnb+/BzkpTk3MBFxGUqczoWjOYQxx+nEUD0lTiIiIo1JC083xvWPBGBm5lCI6AfF2bDkKSdHJiKuQonTuahInP5QcdJQPRERkcbnlvLhej/uSid1+POACbZ+AodWOjcwEXEJSpzORVlF4lQxx8loR+7vpeYQIiIijU23cH+GdWqF1WbnXzt9YcAk44HvHgFr6WnPFRFR4nQuKppDnNJVTxUnERGRxih2VBcAPttwhIR+j4BPKzi2C35/28mRiUhjp8TpXDgqTpXXcVLiJCIi0jgNaBfEiK4hWG12Zv2WDqOeMR5Y9gIcP+zc4ESkUVPidC4czSHKK07qqiciItLoVVSdvtqcxP6IMdD2fCjNh2/vA7vdydGJSGOlxOlc/HEdpyJjjpOaQ4iIiDRefdoEclmPMOx2+FfcAbjmTaND7sFfYf18Z4cnIo2UEqdzUVZ1Vz1/bzWHEBERacweKq86fbc1mV0lITDyH8YDS56C44ecFpeINF5KnGrLZgVriXG/Yh0ntSMXERFxCd1b+3N1n9YAzFy6FwbfBe2GGUP2vpkCNpuTIxSRxkaJU21VzG8CcPeiqNRKcZnxS1bNIURERBq/B0d2wWyCpTtT2Xo0xxiy5+4Dh1bA+nnODk9EGhklTrVVVnTivpu3oxW5yQS+HhqqJyIi0th1CvVlbL9IAF5dsheCOsDIp40Hl06DzINOjE5EGhslTrVVUXGyeILZfNLit+6YzSYnBiYiIiJn64FLO+NmNvHr3mNsPZIFg+6A6OFQWlA+ZM/q7BBFpJFQ4lRbFRUnNYYQERFxWe1atWBM3wgA5q44CGYz/OkNcG8Bh1fC8pedHKGINBZKnGqrtMC4rWgMUaTGECIiIq7ojuHtAfh+WzJJWYUQ1B6unmk8uOwF2PeTE6MTkcZCiVNtOdZwqtxRT4vfioiIuJaeEQGc37EVVpud91aWz2vqexMMmAzY4cs7ICvRqTGKiPMpcaotxxpOSpxERERc3Z3DOwDwybpExygSLn8BWsdA4XH4bCKUFTsvQBFxOiVOtVVRcXIz5jjlFBnNITRUT0RExPVc1CWEzqG+5BWXsWhteXXJ3QtufB+8AiFpA/z4hFNjFBHnUuJUWxVznMorTmoOISIi4rrMZpNjrtP83w5Sai1fALdlO7j2HeP+urmw9TMnRSgizqbEqbbKqp7jpIqTiIiIa7omJpJgXw+Ss4v4flvyiQe6jIbhjxj3/3s/HNngnABFxKmUONVWxTpObn9sR67ESURExBV5uVuYMDQagLkr4rHb7ScevPhx6HipMeLkoxsgfZ9zghQRp1HiVFt/rDgVqTmEiIiIq/vzee3wcjezPSmH3+MzTzxgtsCNCyGiHxRkwAfXQk5y9U8kIk2OEqfa+kPFKadQzSFERERcXVALD67r3waAd1fEV37Q0w9u/RyCOkJ2Anx4rdFxT0SaBSVOtVVauR25mkOIiIg0Dbdf0B6TCeJ2p7Ex4Q+JUYtguO0r8A2HtJ3w8c0nrglEpElT4lRb1QzVU8VJRETEtXUI8XVUnR7/ctuJDnsVWraDP38BngGQsBo+mwxlJU6IVEQakhKn2nIM1fPGbrdrAVwREZEm5PEru9PSx53dKbm8u+LgqQeE94KbPwaLJ+z9AT65GUoKGj5QEWkwSpxqy1Fx8iKvuAxbeeMdddUTERFxfUEtPHjyqh4AvBa3l4SMKpKi6GFG8uTuA/t/gg/GQWFWwwYqIg1GiVNtVSyA6+ZNTpHRGMLDzYyXu8WJQYmIiEhdubZ/JOd3bEVRqY0nvt5WuT15hU6Xwm1fg1cAJP4OC6+GvLQGj1VE6p8Sp9oqPTHHKbtAw/RERESaGpPJxD/H9cbDzcyKfel8u+Vo1Qe2HQKTvocWoZCyDeZfDlkJDRusiNQ7JU61VXaiq55jDSd11BMREWlS2ge34L6LOwHwzH93klVQTROI8F7wl8UQ0BYyD8C80XBkQwNGKiL1TYlTbVVUnNy8HI0h1FFPRESk6fnrRR3pHOpLRn4JM77fXf2BrTrC7T9CSDfIPQrvXQ4bFjZcoCJSr5Q41VbFHCd37xNrOGmonohIkzB79myio6Px8vJiyJAhrF27ttpj586dy/Dhw2nZsiUtW7Zk5MiRpz1eXI+Hm5nnr+0NwKL1iaw9mFn9wf4RcPtS6HY1WEvgv/fDt/dDWXEDRSsi9UWJU22dtI5TRXMIVZxERFzfokWLiI2NZfr06WzcuJG+ffsyevRo0tKqnvC/bNkybr75Zn755RdWr15NVFQUl112GUlJSQ0cudSnQdFB3Dw4CoCnvt5+6tpOJ/Pyhxs/gEunASbYuNCY95R9pGGCFZF6ocSpthxD9U6qOGmOk4iIy5s5cyZ33nknkydPpkePHsyZMwcfHx/mz59f5fH/+c9/uOeee4iJiaFbt268++672Gw24uLiGjhyqW+PXt6NoBYe7EnNZf7KKtZ2OpnZDMMfNhbK9W4JRzfCnOGw5ROoqjufiDR6Spxqy9EcwkuL34qINBElJSVs2LCBkSNHOvaZzWZGjhzJ6tWrz+o5CgoKKC0tJSgoqNpjiouLycnJqbRJ4xfo48HUK7oBMOunfSRlFZ75pE6Xwl3LoHVfKMyEr/4KH14Lxw/Va6wiUveUONVWafkvSzcvR1c9DdUTEXFt6enpWK1WwsLCKu0PCwsjJSXlrJ7j0UcfJSIiolLy9UczZswgICDAsUVFRZ1T3NJwrh/QhsHRQRSWWvnHtzvO7qSW0XBHHFzyFFg84cDP8NZQWPUGWMvqNV4RqTtKnGqrInFy9zlRcVLiJCLSrL3wwgt88sknfPXVV3h5eVV73NSpU8nOznZsiYmJDRilnAuTycRz43rhZjaxdGcqP+1MPbsTLe5w4SNw9yqIHm40mVryJMwdAfvjNHxPxAUocaoNaynYrcZ9dy9yCo1vizRUT0TEtQUHB2OxWEhNrXwxnJqaSnh4+GnPfeWVV3jhhRdYsmQJffr0Oe2xnp6e+Pv7V9rEdXQJ8+P24e0BmP7tDgpKalA1Cu4EE/8Lf3oDvAKMBXM/vBYWjoHEdfUUsYjUBSVOtVF60phmN28N1RMRaSI8PDwYMGBApcYOFY0ehg4dWu15L730Es8++yyLFy9m4MCBDRGqONkDl3YmMtCbpKxC3vh5f81ONpmg/wS4byOcdw9YPODQCpg3Ej6+BVJ31k/QInJOlDjVhiNxMoGbp2Oonp+XuuqJiLi62NhY5s6dy8KFC9m1axd33303+fn5TJ48GYAJEyYwdepUx/EvvvgiTz31FPPnzyc6OpqUlBRSUlLIy8tz1luQBuDj4cb0MT0AmLs8/vRrO1WnRTBcPsNIoPr9GUxm2PMdvD0U/nMDHPhFQ/hEGhElTrXh6KjnDSaTYx0nzXESEXF948eP55VXXmHatGnExMSwefNmFi9e7GgYkZCQQHJysuP4t99+m5KSEq6//npat27t2F555RVnvQVpIJf1DGdM3wjKbHb+9uEGEjMLavdEgVFwzWy4Zw30GAuYYN8S+GAsvD0MNv1HC+iKNAImu715fZWRk5NDQEAA2dnZtR9TnrYb3hoC3kFY/y+ejo9/D8CGJ0fSytezDqMVEWla6uR3cBOkz8V1FZZYueHfq9ielEPXMD++uOd8fD3PcQRKxgFY82/Y9CGU5hv7WoRA/4kwcDIEtDn3wEUEqNnvX1WcauOkilNe0YkJoX5qDiEiItKseHtYmDthIKF+nuxJzeWBjzdhtZ3jd9KtOsKVL0HsDhj5NPhFQP4xWPEKzOoDn9wK8cs0jE+kgSlxqo0q1nDydrfg4aaPU0REpLlpHeDNOxMG4uFmJm53Gi8t3l03T+zdEi54EB7cCjcsMNqY262w+3/w/jXw5iBY9SbkZ9TN64nIaelKvzZOWsMp27GGkxpDiIiINFcxUYG8fL3Rhv7fy+P5bH0drs1lcYee42DS/+Ce32HQHeDhCxn7YMkTMLMbfH47HFyhKpRIPVLiVBtlRcat+4mKk9ZwEhERad6uiYlkysWdAHjiq+3sOJpd9y8S2h2uehUe3g1Xz4LWMWAtge2fw8Kr4Y0BsPwVyE6q+9cWaeaUONXGyUP1yhe/VStyERERiR3VhZHdQymx2nho0WaKSq3180KefkajiL/+CnctgwGTjSpU5gH4+VmY1Qs+uBa2fQ6lRfUTg0gzo8SpNkpPNIfIrag4qRW5iIhIs2c2m3jxuj4E+3qyNzWPlxbvqf8XjegHY2bBw3vgmreg3TCw2+BAHHxxO7zcET66CdbONTr2iUitqExSG46het4n1nDSUD0REREBWvl68vL1fZi8YB3zfzvIxd1CGN45pP5f2NMX+t1qbJnxsPlj2PIxZCfC3h+MDaBlNHS8xGg2EX0B+IbWf2wiTYAqTrXhGKrnTY6aQ4iIiMgfXNwtlD+f1xaARz7bQlZBScMGENQBLnkCHtgKf10Ol043EiWzOxw/BOvnw+eT4ZXO8OZg+F+sMazv+GE1mBCphq72a0PNIUREROQMnriyB6sOZBB/LJ/Hv9rG7Fv6YzKZGjYIsxla9zW24bFQnAeHVsDB5cZtynZI32Ns6+cZ57QIgTaDIHIARA2GyIHg4dOwcYs0QkqcaqO0wLh18yYnt3yonuY4iYiIyEm8PSzMGh/DtW+t4vttKXy5MYnrBrRxblCevtD1CmMDKMiEw6uMJCpxDaRsMxbb3fO9sQGY3YzEq+3Q8u08aBHsvPcg4iRKnGqj9NSKk7rqiYiIyB/1aRPIgyM788qSvUz/dgcdQ32JiQp0dlgn+ARB96uNDYzpCMlbIWk9HFkHiWshJwmSNhjb6jeN41q2N6pRbQZBm4EQ1stYb0qkCdPVfm2UnVgA1zHHSUP1REREpAp3j+jEb/szWB2fwW3vrmHh7YPp37als8Oqmrs3tB1ibGDMd8pOhMOrIaF8O7Ybjh80tq2Lys9rAe2HQ4eLjcYTwZ2hoYclitQzJU61UVFxcvMit0hD9URERKR6FrOJdycOZPKCdaw9mMmEeWtZ+JdBDGgX5OzQzsxkgsC2xtZ3vLGvMKu8IlVelTqyDoqyYe9iYwPwj4SOFxuJVPuLwLcBugqK1DOnd9WbPXs20dHReHl5MWTIENauXXva47Oysrj33ntp3bo1np6edOnShe+//76Boi1XMcfJ3fuk5hDKQUVERKRqLTzdWDB5EEM7tCKvuIwJ89ay7lCms8OqHe9A6DQSRjwGf/4C/n4I/roCRj4NHUaAxdMY3rfpQ2MdqVc6wdsXwJInYf9PRoMKERfk1Kv9RYsWERsby5w5cxgyZAizZs1i9OjR7Nmzh9DQU9cUKCkpYdSoUYSGhvL5558TGRnJ4cOHCQwMbNjAy05UnE60I1fFSURERKrn4+HG/EmDuOP9dfy2P4OJ89fy3qRBDOnQytmhnRuzGVr3MbYLHoSSAkhYBQd+gfhfIXXbiW3VG0aziYj+xtC+6OHGXCmPFs5+FyJnZLLbndesf8iQIQwaNIg33zQmGtpsNqKiorjvvvt47LHHTjl+zpw5vPzyy+zevRt399olKjk5OQQEBJCdnY2/v3/tAl9wNRxage3aeXT82Bu7HdY9MZIQP8/aPZ+ISDNRJ7+DmyB9Ls1LUamVO99fz4p96Xi7W/jynvPp3roJ/3fPSzPanx/4xbjNTqj8uMkMrTpDeO8TW0g38A0Di0b0SP2qye9fp/1rLCkpYcOGDUydOtWxz2w2M3LkSFavXl3lOd9++y1Dhw7l3nvv5ZtvviEkJIRbbrmFRx99FIvFUuU5xcXFFBcXO37Oyck59+DLK05FuDvWiFNXPRERETkbXu4W5k4YyO0LjcrT3z7cwLdTLiCgqY5e8Q2F3tcbGxiL7B5aAYdWwsEVkHPkxFpS2z8/cZ7JDC1CwS8c/FpDUHvoeClEXwDuXs55L9KsOe1qPz09HavVSlhYWKX9YWFh7N69u8pz4uPj+fnnn7n11lv5/vvv2b9/P/fccw+lpaVMnz69ynNmzJjB008/XbfBlxpd9fJt7oAdDzczXu5VJ24iIiIif+TlbuHNm/tz9RsrOZxRQOyizcydMBCzuRl0omvZztj6/dno2peXaqwflbLVWJA3ZRtkxoPdCnkpxpa82Tj397fA3cdoONF5FHS6FALbqYOfNAiXKpPYbDZCQ0N55513sFgsDBgwgKSkJF5++eVqE6epU6cSGxvr+DknJ4eoqKhzC6RS4lSiVuQiIiJSYy1bePDv2wZw7duriNudxuxf9nPfpZ2dHVbDMpnKK0rhRiJUwWaFggzITYbcFMg5Ckc3wb6lkHsU9v5gbADeQcYCva37QkQMhPYwhvl5BSihkjrltMQpODgYi8VCampqpf2pqamEh4dXeU7r1q1xd3evNCyve/fupKSkUFJSgoeHxynneHp64ulZx3OPyofq5VrLEydvl8o/RUREpJHoFRnAc2N78ffPtzLzp730bhPAiK6nNshqdswWY4ifb6iREAEw2ahQpW6HfUtg7xJjUd7CTIj/xdhOZvE0zm8RAv4Rxryp0O7GbatOGu4nNea0K34PDw8GDBhAXFwcY8eOBYyKUlxcHFOmTKnynGHDhvHRRx9hs9kwm41O6nv37qV169ZVJk31przilFtmVJpUcRIREZHaunFgFJsTs/hoTQIPfLKZ/913AVFBPs4Oq3EymU40kBj+MJQVQ9pOOLrZGM53dDNkHoTibLAWG4v3ZifC0Y2w+38nPY8ZgrtA1BBoNwzaDTXWqhI5DaeWSmJjY5k4cSIDBw5k8ODBzJo1i/z8fCZPngzAhAkTiIyMZMaMGQDcfffdvPnmmzzwwAPcd9997Nu3j+eff57777+/YQMvrzhllxmVL7UiFxERkXMxfUwPdhzNYUtiFn/9YAMf3TmEQJ8G/FLYVbl5QkQ/YztZaRHkp0HeMWMOVdZhSNsFx/bAsV3Ggr3HdhvbxoXGOf5toO15ENnfeL7WfdUmXSpxauI0fvx4jh07xrRp00hJSSEmJobFixc7GkYkJCQ4KksAUVFR/Pjjjzz00EP06dOHyMhIHnjgAR599NGGC9pudyyAm11anjipo56IiIicA083C2/f2p8xb6xkZ3IO1729igWTB6vyVFvuXkYFqaoqkt1uzJs6ugkO/wYJq41KVc4Ro6tfRWc/k9kY1hfeB4I7GcP7gjpCq45KqJopp67j5AznvFZGaRH800js3j5vGS8uO8rNg9sy49redRypiEjTo/WKqqbPRSrsScll0ntrSc4uIsTPk/cmDaJXZICzw2r6SvLhyDpjS9pkJFW5R6s/PrCd0cyiyxVqj+7iXGIdJ5dVVui4m1lSMVRPH6OIiIicu67hfnx5z/lMfm8du1NyufHfq3nr1v5qGFHfPFpAhxHGViEn2Zg3lbodMuIhY7+xFWYaQ//WvWts7i2g48XQ8RJj3lRQB2PdqZNGTUnToCv+mio15jdhspBdYhTr1BxCRERE6krrAG8+/dtQ/vbBBlYdyOD2hev5x596ctOgKNwtuhhvMP6tja3rFZX3F2RC4trylug/Gi3Td/+vcvMJN29jwV6/1saQv5OZLcZaVO4+4OED7t5G5782g4126m513A1a6owSp5oqn9+Euzc5hWWAmkOIiIhI3fL3cmfB5MH8/fMtfL35KE99vZ1/Ld3LmD6tGdsvkpioQExao8g5fIKg6+XGZrdD8hbYu9gY5pcZD8cPGyOU0nYaW024eUHkAKNJRZtBRgUrsC1YdK3ZGChxqqnyjnq4e5NTVAqoOYSIiIjUPQ83M/8aH0OXcD/mrzxEel4xC1cfZuHqw3QIbsH4QVFMHtYeDzdVoZzGZDKqRBExJ/ZZSyErwUii8tJOPcdWZixtU5pv3JYUGEP/En6HgnSjYcXh304cb3aDltFGcwrfMGM+VkkeFOdBSa6RvAW0Af9I4zagjXF8SDfw9K3f99/M6Iq/piqG6rmdlDip4iQiIiL1wGQycc+ITtw1vAMr96fz1aYkftyRQnx6PjN+2M1Xm5J49ca+9IxQA4lGw+JudN5r1bFm59ntkHEAElYZSVTyFuPnssIT86uqk7K1ip0mY7hgWC9j3auwnkYy1TLaGC4oNabEqaYqmkO4e5FTUD5UT3OcREREpB65WcyM6BrKiK6h5BWX8d3Wo7y4eA+7U3K55s3fuO+SztxzcUfNgXJlJpPR9jy4E/SfYOyz2Yw5VBWJU0GG0cjCw9eoJnn4gd0GOUnGln0EspOMY/NSjKpXZjzs+vbE61g8jSGAod2gVWcIjDpRqfKPPDHHymYzRlqVFRnztLwDG/wjaWyUONVUaXni5OaloXoiIiLS4Hw93Rg/qC2Xdg/jqa+388P2FP71016W7Ezhxev6qH15U2I2Q0CksXW4qGbn5qdDyjajK2DqDuM2fZ+RCKVuM7aqePiBtRisJZX3ewUYHQNbtjdugzsb87GCOjabDoK64q+p8sTJ7u5DbpGaQ4iIiIhzBPt68tat/fnv1mSmfbOdHUdzuPqNlUQGenN+x1YMLd9aB3g7O1RxhhbB5W3SLz6xz2Y15lMd2wPHdhuVqeyKStURY2RVSW7Vz1eUbaxvdXRT5f1eARA50GhmETkAwnoYlasm2LxEiVNNlTeHsFk8sdrUjlxEREScx2Qy8ae+EZzXIYinv93JjztSSMoq5LMNR/hswxEAekcGMPPGvnQO83NytOJ0ZotRLQrqcGqbdbsdCo8bm5un0VLdzdNol15WDMcPwfGD5cP/DhodA49uNhKqA3HGVsHTH0K7G1urzuDTCrxbGh0JvVue+Plsk6vC45C+H9L3GvdbtjtR/fLwqatP54yUONVUecWp1GyM/3S3mPBybx7lSREREWmcQv28mH1rf/KLy1h/+DirDqTz+4EMtiVlsy0pm7Gzf+OVG/pyRe/Wzg5VGiuTyUhsfIJOfczDx6gkhfWovN9aagwDPLIOjqw3qlEZ+6E4BxLXGFt13H1OzK0KaGMkU2XFxtI/pYXGbX4GZOyD/GPVP49fayOJuujvlRcwrgdKnGqqPHEqMRmJk7+Xu9ZREBERkUahhacbF3UJ4aIuIQCk5RbxwMebWR2fwd3/2chfL+rA/13WFTc1kZC6YHE/0Y598J3GvrJiI3lK22Vsxw+eqGQVHoeC41CcbSRG6XuN7Wz4RRjzqnyCjLWyMg8Y1a7cZGOzltXXu3RQ4lRT5V31iisSJ81vEhERkUYq1M+LD24fzEs/7uGd5fH8+9d4th3J5o2b+9HK19PZ4UlT5OZptD4P61n9MaVFJ3UBLN8KjxvDAt19ym+9jflTrToaa1h5VjHUtCDTGDaYGV95La16osSppsrXcSq2ewDgp456IiIi0oi5Wcw8fmV3+rQJ4O+fb2XVgQyGv/QLbYN8iAj0JiLQi4hAb7qG+TG8c4gW1JX65+5Vu7Wu/qhiaGGbAXUT1xnoqr+myitOhRiVJjWGEBEREVdwdZ8IuoT58bcPNxB/LJ/dKbnsTqncQa2ljzvXxERyw8A2WlRX5A+UONVU+RynwvKKk7+3PkIRERFxDV3C/Fjy4IUcTM/naHYRR7MKOZpVSNLxQlbuTyctt5gFqw6xYNUhurf257r+kfwpJoJQPy9nhy7idLrqr6nyxCnfZnx0qjiJiIiIK3GzmOkc5ndKe/Iyq40V+9P5fMMRlu5IZVdyDs99l8Pz3+/igs4hjOsXwWU9wmnhqctHaZ70L7+mytdxyrNVVJyUOImIiIjrc7OYubhrKBd3DSWroIRvtxzlq01JbErIYvneYyzfewxv9+2M7RfB9DE98XK3ODtkkQalxKmmyitOeWUVFSd9hCIiItK0BPp4MGFoNBOGRnMwPZ9vNifx9aYkDmUU8PHaRDLySnjr1v5qay7Niv6111R54pRjLU+cVHESERGRJqx9cAseHNmFXx4ZwfxJA/FwM7NkZypPfLUdu93u7PBEGowSp5oqH6qXU2okTmpHLiLS9MyePZvo6Gi8vLwYMmQIa9eurfbYHTt2cN111xEdHY3JZGLWrFkNF6hIAzKZTFzSLYzXb+qH2QSL1ify8o97nB2WSINR4lRTnUdB35s5aDNW5FZzCBGRpmXRokXExsYyffp0Nm7cSN++fRk9ejRpaWlVHl9QUECHDh144YUXCA8Pb+BoRRre5b3CeX5cbwDeWnaAd1fEOzkikYahcklNXfAQAJteWQbka6ieiEgTM3PmTO68804mT54MwJw5c/juu++YP38+jz322CnHDxo0iEGDBgFU+bhIU3TT4LZk5Jfw8o97eO67XZhNJrqF+1Fqs1NmtVFqtdPSx53+7Vrifpp5UImZBZRabXQI8W3A6EVqR4lTLeUUlgKqOImINCUlJSVs2LCBqVOnOvaZzWZGjhzJ6tWr6+x1iouLKS4udvyck5NTZ88t0lDuGdGR9Lxi3vvtEM/8b2eVxwT6uDOyexije4YzvHMwnm5mdhzNYcmOFJbsTGV3Si5mE7w7cSCXdAtr4HcgUjNKnGrBbreTU1SeOGkBXBGRJiM9PR2r1UpYWOULuLCwMHbv3l1nrzNjxgyefvrpOns+EWcwmUw8dVUPPN0sLNmRgsVsws1ixt1iws1s4lBGAZn5JXy+4QifbziCj4cFfy93UnKKKj2PzQ4PfLyZr6cMo6MqT9KI6aq/FopKjRI0qOIkIiI1N3XqVGJjYx0/5+TkEBUV5cSIRGrHbDbx2BXdeOyKbqc8Vma1sf7wcRZvT+HHHSkkZxdRUGLF293ChV2CuayHUYW696ONrDt0nDvfX8/X9w7TtZU0WkqcaqGi2mQxm/Dx0OJvIiJNRXBwMBaLhdTU1Er7U1NT67Txg6enJ56ennX2fCKNkZvFzHkdWnFeh1ZMH9ODbUnZZBeWMig6qNLiuW/dOoA/vbmS+GP5PPTJZuZOGIjZbHJi5CJVU1e9WqiY3+Tn5YbJpP+xRUSaCg8PDwYMGEBcXJxjn81mIy4ujqFDhzoxMhHXZjKZ6NMmkOGdQyolTQAhfp78+7YBeLqZidudxsyle50UpcjpKXGqhZyiMkDD9EREmqLY2Fjmzp3LwoUL2bVrF3fffTf5+fmOLnsTJkyo1DyipKSEzZs3s3nzZkpKSkhKSmLz5s3s37/fWW9BxOX0aRPIi9f1AeDNX/bz9aYkjueXkFNUSmGJlZIymxbbFafTUL1aUGMIEZGma/z48Rw7doxp06aRkpJCTEwMixcvdjSMSEhIwGw+8b3j0aNH6devn+PnV155hVdeeYWLLrqIZcuWNXT4Ii5rbL9IdhzNZu6Kgzy4aPMpj3u6mWkf3MKxRQe3oEdrf3pG+GsEkDQIXfnXglqRi4g0bVOmTGHKlClVPvbHZCg6OlrfhIvUkUcv70ZGXglfbU7ij/9bFZfZ2J2Sy+6U3Er7O4X6cv2ANlzbL5JQf68GjFaaGyVOtaCheiIiIiJ1z81iZub4GGaOj8Fqs1Nms1FmtVNms3M8v4SDGfkcSs/nYHo+8cfyWX84k/1pebzww25eWrybi7qEcMuQdozqoTWhpO4pcaoFR8VJQ/VERERE6oXFbMJituBZfrkV4O1OdHAL6HrimNyiUr7bmsznG46w/vBxftlzjF/2HOOOC9rz+JXdq+3Ot/5QJvvS8hjTNwJfT13PydnRv5RaqJjj5KeKk4iIiIjT+Hm5c9Pgttw0uC3xx/L48PcE5v92kHdXHiQlp4hXb+yLp9uJLn4lZTZeXbKHd1bEY7fDCz/sZvKwaCadH02gj4cT34m4AnXVq4VcDdUTERERaVQ6hPgybUwPZo2Pwd1i4n9bk5k4fy3Z5SOF9qflMe6t3/j3ciNpCvP3JLuwlFk/7WPYCz8z4/tdpOUWOfldSGOmxKkWNFRPREREpHEa2y+S9yYNxtfTjd/jM7lxzmrmLo/n6jdWsONoDi193HnntgGseuxSZt/Sn+6t/ckvsfLv5fFc9NIyFvx2EJtNDV/kVEqcakHNIUREREQarws6B7Por+cR4ufJntRc/vn9LopKbVzQKZjFD17IZT3DsZhNXNWnNd/ffwHzJw0kJiqQwlIr//jvTm59dw2JmQXOfhvSyChxqoUTFSclTiIiIiKNUc+IAL68+3w6hfriYTHzxJXdef8vgwn7Q8tyk8nEJd3C+PLu83nmmp54u1tYHZ/B5bOW89GaBC03IA4aa1YLjgVwvfTxiYiIiDRWUUE+LH5gOAWl1jOOFDKbTUwYGs2FnUP4v8+3sO7QcR7/ahsf/H4YX08LRaU2ikqtFJfZMJsg1N+LMH8vwvw8CQ/wolu4P8M6tdJivE2YrvxrIafQGKqnrnoiIiIijZubxYy/5ewHWUUHt+CTu4by3m8HeenHPexKzqnyuEMZpw7lGz8wimfG9qzUyU+aDiVOteCoOKk5hIiIiEiTYzGbuGN4B0b3DGdjwnE8LGa83C14upnxdLdQ9v/t3XtUlPW6B/DvcJlhhstwUxjumIgCQgZK42W1C45IbVOzsn2wyL3Tg4JZ1lnVaSe29yrctdMux4Vame1tR8xWplZoRkk7Qw3wAoImioAIjCiX4U7M7/zhdvaeAAc0eWfg+1nrXTLv7/eOzzOzhodnve/83h4DdPpO1DV3QKfvRHVDO7KLa7A9vwpndHpsWBiN0b+4JLAvZTo93v++HJVX2jAjZBTujdAgwEM1BBnSjeBf/oPU0d2Drp8NAPgdJyIiIqLhzN9dBX/3gTUyuT9dwvL/K0RhZSPu/9+D2PhoNKL8Xfucm3/+CjbknsPXpXXGfQfLLmNN9ilE+LogMUKD+yZqrt7wlywGG6dBunYPJ5kMcJLz5SMiIiIi4K5xo7ArbToW/y0fZboWPLQxD8/OHAd3RwXau35GW1cP2rp68H1ZPQoqGgBc/XtyZpgXYoM9kHOqDnlnL6O4uhnF1c14fd9pRPqpcX+UD2ZH+ZgsamEwCJy/3IrjFxrRYwB+G6mBgz0vD7zVZGKELRXS3NwMtVqNpqYmuLi4DPr4s5daEPdGLlwc7HBidcItiJCIaPi62d/BwxVfF6LhQ9/Rjae3H8PXpbp+58htbTA/2hdPzBiD20Y5GfdfbunEVyV1+LKoBj+cvYyef95PSiYD7gz2QKSfGiU1zThe1Wi8PQ4A+Loq8XziePw2UsPFKQZpML9/ecpkkLgUORERERH1x9nBHpsejcGmf5zDN6U6KOxtoJLbwlFuB6XcFj6uSjwU44fRzr2/A+XhpMDvpgTgd1MCUN/SiS+LarD72EXkVzQg79xl5J27bJyrsLNBhK8aNY3tqG5sx/JtR/HhD+fx0m/D+r1EkG4OG6dB4s1viYiIiOh6bGxkSLnrNqTcddsNP4enkwKPaYPwmDYIVVfasOfERVRdaUe4jwtu93dFqLcz7G1t0N7Vg3f/cQ6ZB84iv6IBc9YfxH2RGkwf64mJvmqM83KG3K7vVQX1Hd0oudiM4ovNOFndhOKLTfi5R+DxaUF4ZHJAv8eNVGycBunaGSdn3sOJiIiIiIaAv7sKy34zts8xpdwWT8aF4OEYf7y27xQ+LazGFydq8MWJGgBXLwucoHGGv7sK+o6f0djejaa2LjS2d6OxrbvP51y16yQ2fXcOT8WPw7xJvrC14eV/ABunQbu2OAQv1SMiIiIiS+GtdsDah2/HoqnB+LzoIoqrm1B0oQnNHT/j+IUmHL/Q1OdxPmoHRPiq/7m5oLqxA+/knMGFhnY8u+M4NuSexX8nhCIh3HuIM7I8bJwGyXgPJ16qR0REREQWZqKfGhP91AAAIQQqr7ShqLoJdc2dUCvt4aq0h1p19V9PJwXcHOW9nuPBO/zwYd55ZB44izJdC/7r7wX43RR/rL7/5m7uW9/Sib9kn0JBRQPGjnZClL8rJvqqEemnhquqdxyWho3TIP1rcQi+dERERERkuWQyGQI9HBHoMbj7QSnltki56zb8Z2wAMg+cxYbcs9h2pAqna/XIXBhtsjT6NcXVTTh/uRV3jvGAp5PCZMxgEMj6sQprskuN6wWcq2/FVyX/uo/VOC8n/HfCePxHmNcNZDo0+Nf/IPGMExERERGNBC4O9nhu1njEBrvjyW1HUVjZiNnvfI/MhdGIDnRD188GfFlUgw/zzuNoZSMAwEYGTA5yx6wIbySEe6O5oxv/82kRCv85Hu7jgrS7x6K6sR3HLzSh6EIjzl9uw091LVj8t3zET/DC6vvD4OfW/42H27t6cKWtCw2tXWhs68aVti7cOca9z5UKf01snAapuZ3fcSIiIiKikeM3oaOxO206lvw9Hz/VteCRTXl4YJIfck7pUN/SCQCwt5UhyMMRZ3QtOFx+BYfLr+DlPSWwkQEGATjKbfHMzFA8pg2Ena3pan2NbV3Y+N05vPvdOXxdWofvyy5hRdw4/GF6MOqaO1BY2YDCigYUVjbijE6Pjm5Drxg3Px6De8azcbIo1844cVU9IiIiIhopgjwd8emyaXj24+PYe7IW2/OrAABeLgosjA3EI1MCMMpZgQsNbdh3sg57i2uQX9EAgwASI7yRPjsc3uq+GxtXlRzPzRqPByb54sXPinGk/Ar+svcU3sr5qc8mCbjaqLmq5HBXyeGqsofS/tb/bc6//gdJz/s4EREREdEI5KSwQ+bCO/DeP8pxuPwy5k3yw8xwL9j/2xkkPzcV/jA9GH+YHgydvgNtnT0I8hzYd6xCvJyxfcmd+LSwGq9+WYrLrV2ws5Eh3FeNOwJccUeAGyJ81fB0ksNJYQeZbGiXSZcJIcSQ/o8Sa25uhlqtRlNTE1xcXAZ9fMnFZlxq6USYxgWjnBXmDyAiIqOb/R08XPF1ISIy1dL5M87Xt2LsaCc42N/4Sn7mDOb3L884DVKYDwsaEREREdGt5KSwQ4SvWuowTNiYn0JERERERDSysXEiIiIiIiIyg40TERERERGRGWyciIiIiIiIzGDjREREREREZAYbJyIiIiIiIjPYOBEREREREZnBxomIiIiIiMgMNk5ERERERERmWETjtH79egQFBcHBwQGxsbE4cuRIv3O3bNkCmUxmsjk4OAxhtERERERENNJI3jht374dK1euRHp6OgoLCxEVFYWEhATodLp+j3FxcUFNTY1xq6ioGMKIiYiIiIhopJG8cVq7di0WL16MRYsWISwsDBs2bIBKpcLmzZv7PUYmk8Hb29u4eXl5DWHEREREREQ00kjaOHV1daGgoADx8fHGfTY2NoiPj0deXl6/x7W0tCAwMBD+/v6YM2cOTp482e/czs5ONDc3m2xERERERESDIWnjVF9fj56enl5njLy8vFBbW9vnMaGhodi8eTN27dqFrVu3wmAwYOrUqbhw4UKf8zMyMqBWq42bv7//r54HERERERENb3ZSBzBYWq0WWq3W+Hjq1KmYMGECNm7ciD//+c+95r/wwgtYuXKl8XFTUxMCAgJ45omISALXfvcKISSOxLJcez1Ym4iIhtZg6pKkjZOnpydsbW1RV1dnsr+urg7e3t4Deg57e3tMmjQJZWVlfY4rFAooFArj42svDs88ERFJR6/XQ61WSx2GxdDr9QBYm4iIpDKQuiRp4ySXyxEdHY2cnBzMnTsXAGAwGJCTk4O0tLQBPUdPTw+Kiopw7733Dmi+j48Pqqqq4OzsDJlMdt25zc3N8Pf3R1VVFVxcXAb0/JaIeVgW5mFZhksegHXkIoSAXq+Hj4+P1KFYFNYm5iE15mFZmMfQGUxdkvxSvZUrVyI5ORkxMTGYMmUK3nzzTbS2tmLRokUAgMceewy+vr7IyMgAAPzpT3/CnXfeibFjx6KxsRGvv/46Kioq8MQTTwzo/7OxsYGfn9+gYnRxcbHYN3swmIdlYR6WZbjkAVh+LjzT1BtrE/OwFMzDsjCPoTHQuiR547RgwQJcunQJq1atQm1tLW6//Xbs3bvXuGBEZWUlbGz+tYZFQ0MDFi9ejNraWri5uSE6Oho//PADwsLCpEqBiIiIiIiGOckbJwBIS0vr99K8AwcOmDxet24d1q1bNwRRERERERERXSX5DXAtmUKhQHp6usniEtaIeVgW5mFZhksewPDKhfo3XN5n5mFZmIdlYR6WSSa4JiwREREREdF18YwTERERERGRGWyciIiIiIiIzGDjREREREREZAYbJyIiIiIiIjPYOF3H+vXrERQUBAcHB8TGxuLIkSNSh3Rd3333HWbPng0fHx/IZDJ89tlnJuNCCKxatQoajQZKpRLx8fE4c+aMNMFeR0ZGBiZPngxnZ2eMHj0ac+fOxenTp03mdHR0IDU1FR4eHnBycsL8+fNRV1cnUcR9y8zMRGRkpPGmb1qtFtnZ2cZxa8jhl9asWQOZTIannnrKuM9a8li9ejVkMpnJNn78eOO4teQBANXV1Vi4cCE8PDygVCoxceJE5OfnG8et5bNON4a1aeixLlk21ibpjZS6xMapH9u3b8fKlSuRnp6OwsJCREVFISEhATqdTurQ+tXa2oqoqCisX7++z/HXXnsNb7/9NjZs2IDDhw/D0dERCQkJ6OjoGOJIry83Nxepqak4dOgQ9u/fj+7ubsycOROtra3GOU8//TT27NmDHTt2IDc3FxcvXsQDDzwgYdS9+fn5Yc2aNSgoKEB+fj7uuecezJkzBydPngRgHTn8ux9//BEbN25EZGSkyX5ryiM8PBw1NTXG7fvvvzeOWUseDQ0NmDZtGuzt7ZGdnY2SkhK88cYbcHNzM86xls86DR5rkzRYlywXa5P0RlRdEtSnKVOmiNTUVOPjnp4e4ePjIzIyMiSMauAAiJ07dxofGwwG4e3tLV5//XXjvsbGRqFQKMS2bdskiHDgdDqdACByc3OFEFfjtre3Fzt27DDOKS0tFQBEXl6eVGEOiJubm3jvvfesLge9Xi9CQkLE/v37xV133SVWrFghhLCu9yI9PV1ERUX1OWZNeTz33HNi+vTp/Y5b82edzGNtsgysS5aBtckyjKS6xDNOfejq6kJBQQHi4+ON+2xsbBAfH4+8vDwJI7tx5eXlqK2tNclJrVYjNjbW4nNqamoCALi7uwMACgoK0N3dbZLL+PHjERAQYLG59PT0ICsrC62trdBqtVaXQ2pqKu677z6TeAHrey/OnDkDHx8fjBkzBklJSaisrARgXXns3r0bMTExeOihhzB69GhMmjQJ7777rnHcmj/rdH2sTZaDdckysDZZhpFUl9g49aG+vh49PT3w8vIy2e/l5YXa2lqJoro51+K2tpwMBgOeeuopTJs2DREREQCu5iKXy+Hq6moy1xJzKSoqgpOTExQKBVJSUrBz506EhYVZVQ5ZWVkoLCxERkZGrzFryiM2NhZbtmzB3r17kZmZifLycsyYMQN6vd6q8jh37hwyMzMREhKCffv2YenSpXjyySfx4YcfArDezzqZx9pkGViXLANrk+XkMZLqkp3UARBdT2pqKoqLi02u97UmoaGhOHbsGJqamvDJJ58gOTkZubm5Uoc1YFVVVVixYgX2798PBwcHqcO5KYmJicafIyMjERsbi8DAQHz88cdQKpUSRjY4BoMBMTExePXVVwEAkyZNQnFxMTZs2IDk5GSJoyMa/liXpMfaZFlGUl3iGac+eHp6wtbWtteqJXV1dfD29pYoqptzLW5ryiktLQ2ff/45vv32W/j5+Rn3e3t7o6urC42NjSbzLTEXuVyOsWPHIjo6GhkZGYiKisJbb71lNTkUFBRAp9PhjjvugJ2dHezs7JCbm4u3334bdnZ28PLysoo8+uLq6opx48ahrKzMat4PANBoNAgLCzPZN2HCBOOlHdb4WaeBYW2SHuuSZWBtsqw8RlJdYuPUB7lcjujoaOTk5Bj3GQwG5OTkQKvVShjZjQsODoa3t7dJTs3NzTh8+LDF5SSEQFpaGnbu3IlvvvkGwcHBJuPR0dGwt7c3yeX06dOorKy0uFx+yWAwoLOz02pyiIuLQ1FREY4dO2bcYmJikJSUZPzZGvLoS0tLC86ePQuNRmM17wcATJs2rdcyyD/99BMCAwMBWNdnnQaHtUk6rEuWlQNrk2XlMaLqktSrU1iqrKwsoVAoxJYtW0RJSYlYsmSJcHV1FbW1tVKH1i+9Xi+OHj0qjh49KgCItWvXiqNHj4qKigohhBBr1qwRrq6uYteuXeLEiRNizpw5Ijg4WLS3t0scuamlS5cKtVotDhw4IGpqaoxbW1ubcU5KSooICAgQ33zzjcjPzxdarVZotVoJo+7t+eefF7m5uaK8vFycOHFCPP/880Imk4mvvvpKCGEdOfTl31cuEsJ68njmmWfEgQMHRHl5uTh48KCIj48Xnp6eQqfTCSGsJ48jR44IOzs78corr4gzZ86Ijz76SKhUKrF161bjHGv5rNPgsTZJg3XJ8rE2SWck1SU2TtfxzjvviICAACGXy8WUKVPEoUOHpA7pur799lsBoNeWnJwshLi6HORLL70kvLy8hEKhEHFxceL06dPSBt2HvnIAID744APjnPb2drFs2TLh5uYmVCqVmDdvnqipqZEu6D78/ve/F4GBgUIul4tRo0aJuLg4Y3ESwjpy6Msvi5O15LFgwQKh0WiEXC4Xvr6+YsGCBaKsrMw4bi15CCHEnj17REREhFAoFGL8+PFi06ZNJuPW8lmnG8PaNPRYlywfa5O0RkpdkgkhxNCd3yIiIiIiIrI+/I4TERERERGRGWyciIiIiIiIzGDjREREREREZAYbJyIiIiIiIjPYOBEREREREZnBxomIiIiIiMgMNk5ERERERERmsHEiGqZkMhk+++wzqcMgIiIyYm0ia8bGiegWePzxxyGTyXpts2bNkjo0IiIaoVibiG6OndQBEA1Xs2bNwgcffGCyT6FQSBQNERERaxPRzeAZJ6JbRKFQwNvb22Rzc3MDcPVShczMTCQmJkKpVGLMmDH45JNPTI4vKirCPffcA6VSCQ8PDyxZsgQtLS0mczZv3ozw8HAoFApoNBqkpaWZjNfX12PevHlQqVQICQnB7t27jWMNDQ1ISkrCqFGjoFQqERIS0quYEhHR8MLaRHTj2DgRSeSll17C/Pnzcfz4cSQlJeGRRx5BaWkpAKC1tRUJCQlwc3PDjz/+iB07duDrr782KT6ZmZlITU3FkiVLUFRUhN27d2Ps2LEm/8fLL7+Mhx9+GCdOnMC9996LpKQkXLlyxfj/l5SUIDs7G6WlpcjMzISnp+fQvQBERGRxWJuIrkMQ0a8uOTlZ2NraCkdHR5PtlVdeEUIIAUCkpKSYHBMbGyuWLl0qhBBi06ZNws3NTbS0tBjHv/jiC2FjYyNqa2uFEEL4+PiIF198sd8YAIg//vGPxsctLS0CgMjOzhZCCDF79myxaNGiXydhIiKyeKxNRDeH33EiukXuvvtuZGZmmuxzd3c3/qzVak3GtFotjh07BgAoLS1FVFQUHB0djePTpk2DwWDA6dOnIZPJcPHiRcTFxV03hsjISOPPjo6OcHFxgU6nAwAsXboU8+fPR2FhIWbOnIm5c+di6tSpN5QrERFZB9YmohvHxonoFnF0dOx1ecKvRalUDmievb29yWOZTAaDwQAASExMREVFBb788kvs378fcXFxSE1NxV//+tdfPV4iIrIMrE1EN47fcSKSyKFDh3o9njBhAgBgwoQJOH78OFpbW43jBw8ehI2NDUJDQ+Hs7IygoCDk5OTcVAyjRo1CcnIytm7dijfffBObNm26qecjIiLrxtpE1D+ecSK6RTo7O1FbW2uyz87Ozvgl1x07diAmJgbTp0/HRx99hCNHjuD9998HACQlJSE9PR3JyclYvXo1Ll26hOXLl+PRRx+Fl5cXAGD16tVISUnB6NGjkZiYCL1ej4MHD2L58uUDim/VqlWIjo5GeHg4Ojs78fnnnxuLIxERDU+sTUQ3jo0T0S2yd+9eaDQak32hoaE4deoUgKurCmVlZWHZsmXQaDTYtm0bwsLCAAAqlQr79u3DihUrMHnyZKhUKsyfPx9r1641PldycjI6Ojqwbt06PPvss/D09MSDDz444PjkcjleeOEFnD9/HkqlEjNmzEBWVtavkDkREVkq1iaiGycTQgipgyAaaWQyGXbu3Im5c+dKHQoREREA1iYic/gdJyIiIiIiIjPYOBEREREREZnBS/WIiIiIiIjM4BknIiIiIiIiM9g4ERERERERmcHGiYiIiIiIyAw2TkRERERERGawcSIiIiIiIjKDjRMREREREZEZbJyIiIiIiIjMYONERERERERkBhsnIiIiIiIiM/4fq9zZ4R9LN5wAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Enter your code here\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, len(acc_train)+1), acc_train, label='train')\n",
    "plt.plot(range(1, len(acc_test)+1), acc_test, label='test')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, len(loss_train)+1), loss_train, label='train')\n",
    "plt.plot(range(1, len(loss_test)+1), loss_test, label='test')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fe4c0b-c63d-4354-9141-31ac94607f77",
   "metadata": {},
   "source": [
    "\\# TODO: \\<Enter your answer here\\>\n",
    "\n",
    "The train and test accuracy both increase with epochs, with train accuracy being higher than test accuracy. And the train and test loss both decrease with epochs, with train loss being lower than test loss. The model is doing well in fitting the training data, but the test accuracy is not as high as the train accuracy. Hence, the model may be overfitting the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71e5ee7-d56a-4d1c-829e-f841d0d12518",
   "metadata": {},
   "source": [
    "Part A, Q2 (10 marks)\n",
    "---\n",
    "\n",
    "In this question, we will determine the optimal batch size for mini-batch gradient descent. Find the optimal batch size for mini-batch gradient descent by training the neural network and evaluating the performances for different batch sizes. Note: Use 5-fold cross-validation on training partition to perform hyperparameter selection. You will have to reconsider the scaling of the dataset during the 5-fold cross validation.\n",
    "\n",
    "To reduce repeated code, you may need to place the network (MLP defined in QA1) in a separate file called **common_utils.py**. Import it here for Q2. You will not be repenalised for any error in QA1 here as the code in QA1 will not be remarked. The following code cell will not be marked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5c76ec9-cd41-4cec-870f-1c559af5eaff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{32: MLP(\n",
       "   (fc1): Linear(in_features=57, out_features=128, bias=True)\n",
       "   (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
       "   (fc3): Linear(in_features=128, out_features=128, bias=True)\n",
       "   (fc4): Linear(in_features=128, out_features=1, bias=True)\n",
       "   (dropout): Dropout(p=0.2, inplace=False)\n",
       " ),\n",
       " 64: MLP(\n",
       "   (fc1): Linear(in_features=57, out_features=128, bias=True)\n",
       "   (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
       "   (fc3): Linear(in_features=128, out_features=128, bias=True)\n",
       "   (fc4): Linear(in_features=128, out_features=1, bias=True)\n",
       "   (dropout): Dropout(p=0.2, inplace=False)\n",
       " ),\n",
       " 128: MLP(\n",
       "   (fc1): Linear(in_features=57, out_features=128, bias=True)\n",
       "   (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
       "   (fc3): Linear(in_features=128, out_features=128, bias=True)\n",
       "   (fc4): Linear(in_features=128, out_features=1, bias=True)\n",
       "   (dropout): Dropout(p=0.2, inplace=False)\n",
       " ),\n",
       " 256: MLP(\n",
       "   (fc1): Linear(in_features=57, out_features=128, bias=True)\n",
       "   (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
       "   (fc3): Linear(in_features=128, out_features=128, bias=True)\n",
       "   (fc4): Linear(in_features=128, out_features=1, bias=True)\n",
       "   (dropout): Dropout(p=0.2, inplace=False)\n",
       " )}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Enter your code here\n",
    "\n",
    "batch_sizes = [32, 64, 128, 256]\n",
    "\n",
    "model_dict = {batch_size: MLP(57) for batch_size in batch_sizes}\n",
    "\n",
    "model_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c51b77-2743-4cb2-9169-ebca88ec57c8",
   "metadata": {},
   "source": [
    "> Plot mean cross-validation accuracies on the final epoch for different batch sizes as a scatter plot. Limit search space to batch sizes {32, 64, 128, 256}. Next, create a table of time taken to train the network on the last epoch against different batch sizes. Finally, select the optimal batch size and state a reason for your selection. This might take a while to run, so plan your time carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e29cb83-8e4b-43b4-b3f3-31162354cd30",
   "metadata": {},
   "source": [
    "1. Define different folds for different batch sizes to get a dictionary of training and validation datasets. Preprocess your datasets accordingly. Please use the following name conventions:\n",
    "    - X_train_scaled_dict[batch_size] is a list of the preprocessed training matrix for the different folds. \n",
    "    - X_val_scaled_dict[batch_size] is a list of the processed validation matrix for the different folds. \n",
    "    - y_train_dict[batch_size] and y_val_dict[batch_size] is a list of labels for the different folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61d677f5-01ad-43b4-bfeb-ef1f575d7730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 57) (400, 57) (1600,) (400,)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Enter your code here\n",
    "\n",
    "def load_dataframe(df: pd.DataFrame) -> tuple[np.ndarray, np.ndarray]:\n",
    "    X = df.drop([\"label\"], axis=1).values\n",
    "    y = df[\"label\"].values\n",
    "    return X, y\n",
    "\n",
    "X, y = load_dataframe(df)\n",
    "\n",
    "X_train_scaled_dict = {batch_size: None for batch_size in batch_sizes}\n",
    "X_val_scaled_dict = {batch_size: None for batch_size in batch_sizes}\n",
    "y_train_dict = {batch_size: None for batch_size in batch_sizes}\n",
    "y_val_dict = {batch_size: None for batch_size in batch_sizes}\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    X_train_folds = []\n",
    "    X_val_folds = []\n",
    "    y_train_folds = []\n",
    "    y_val_folds = []\n",
    "\n",
    "    for train_index, val_index in kf.split(X):\n",
    "        X_train_fold, X_val_fold = X[train_index], X[val_index]\n",
    "        y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
    "\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        X_train_fold = scaler.fit_transform(X_train_fold)\n",
    "        X_val_fold = scaler.transform(X_val_fold)\n",
    "\n",
    "        X_train_folds.append(X_train_fold)\n",
    "        X_val_folds.append(X_val_fold)\n",
    "        y_train_folds.append(y_train_fold)\n",
    "        y_val_folds.append(y_val_fold)\n",
    "\n",
    "    X_train_scaled_dict[batch_size] = X_train_folds\n",
    "    X_val_scaled_dict[batch_size] = X_val_folds\n",
    "    y_train_dict[batch_size] = y_train_folds\n",
    "    y_val_dict[batch_size] = y_val_folds\n",
    "    \n",
    "print(X_train_scaled_dict[32][0].shape, X_val_scaled_dict[32][0].shape, y_train_dict[32][0].shape, y_val_dict[32][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52ed444-b01b-44fd-bf8f-19df1ed70d6d",
   "metadata": {},
   "source": [
    "2. Perform hyperparameter tuning for the different batch sizes with 5-fold cross validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d615b70-ef9f-4fb3-9802-0af0fce8ec01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 32 \t| epoch: 0 \t| train loss: 0.71162 \t| test loss: 0.68234\t | time taken: 0.03811788558959961\n",
      "batch_size: 32 \t| epoch: 1 \t| train loss: 0.68303 \t| test loss: 0.65804\t | time taken: 0.024483203887939453\n",
      "batch_size: 32 \t| epoch: 2 \t| train loss: 0.65656 \t| test loss: 0.61935\t | time taken: 0.025115013122558594\n",
      "batch_size: 32 \t| epoch: 3 \t| train loss: 0.61434 \t| test loss: 0.55967\t | time taken: 0.023954153060913086\n",
      "batch_size: 32 \t| epoch: 4 \t| train loss: 0.55237 \t| test loss: 0.48843\t | time taken: 0.024173974990844727\n",
      "batch_size: 32 \t| epoch: 5 \t| train loss: 0.47712 \t| test loss: 0.41872\t | time taken: 0.02350902557373047\n",
      "batch_size: 32 \t| epoch: 6 \t| train loss: 0.41364 \t| test loss: 0.36534\t | time taken: 0.023663997650146484\n",
      "batch_size: 32 \t| epoch: 7 \t| train loss: 0.37190 \t| test loss: 0.32763\t | time taken: 0.02439713478088379\n",
      "batch_size: 32 \t| epoch: 8 \t| train loss: 0.33607 \t| test loss: 0.29811\t | time taken: 0.02391672134399414\n",
      "batch_size: 32 \t| epoch: 9 \t| train loss: 0.30978 \t| test loss: 0.27302\t | time taken: 0.025795936584472656\n",
      "batch_size: 32 \t| epoch: 10 \t| train loss: 0.27777 \t| test loss: 0.25056\t | time taken: 0.024638891220092773\n",
      "batch_size: 32 \t| epoch: 11 \t| train loss: 0.25857 \t| test loss: 0.23091\t | time taken: 0.02404928207397461\n",
      "batch_size: 32 \t| epoch: 12 \t| train loss: 0.24134 \t| test loss: 0.21472\t | time taken: 0.02370476722717285\n",
      "batch_size: 32 \t| epoch: 13 \t| train loss: 0.22209 \t| test loss: 0.20082\t | time taken: 0.023591041564941406\n",
      "batch_size: 32 \t| epoch: 14 \t| train loss: 0.19866 \t| test loss: 0.18937\t | time taken: 0.023785114288330078\n",
      "batch_size: 32 \t| epoch: 15 \t| train loss: 0.19082 \t| test loss: 0.17995\t | time taken: 0.02341318130493164\n",
      "batch_size: 32 \t| epoch: 16 \t| train loss: 0.17975 \t| test loss: 0.17190\t | time taken: 0.024052858352661133\n",
      "batch_size: 32 \t| epoch: 17 \t| train loss: 0.17180 \t| test loss: 0.16483\t | time taken: 0.026970863342285156\n",
      "batch_size: 32 \t| epoch: 18 \t| train loss: 0.16123 \t| test loss: 0.15894\t | time taken: 0.025104045867919922\n",
      "batch_size: 32 \t| epoch: 19 \t| train loss: 0.15543 \t| test loss: 0.15364\t | time taken: 0.02402520179748535\n",
      "batch_size: 32 \t| epoch: 20 \t| train loss: 0.14839 \t| test loss: 0.14891\t | time taken: 0.023395061492919922\n",
      "batch_size: 32 \t| epoch: 21 \t| train loss: 0.14135 \t| test loss: 0.14424\t | time taken: 0.02488398551940918\n",
      "batch_size: 32 \t| epoch: 22 \t| train loss: 0.13181 \t| test loss: 0.13988\t | time taken: 0.02349376678466797\n",
      "batch_size: 32 \t| epoch: 23 \t| train loss: 0.13505 \t| test loss: 0.13649\t | time taken: 0.0236051082611084\n",
      "batch_size: 32 \t| epoch: 24 \t| train loss: 0.12207 \t| test loss: 0.13262\t | time taken: 0.023286104202270508\n",
      "batch_size: 32 \t| epoch: 25 \t| train loss: 0.11734 \t| test loss: 0.12857\t | time taken: 0.026134967803955078\n",
      "batch_size: 32 \t| epoch: 26 \t| train loss: 0.11258 \t| test loss: 0.12486\t | time taken: 0.02545785903930664\n",
      "batch_size: 32 \t| epoch: 27 \t| train loss: 0.11226 \t| test loss: 0.12135\t | time taken: 0.0230867862701416\n",
      "batch_size: 32 \t| epoch: 28 \t| train loss: 0.10328 \t| test loss: 0.11742\t | time taken: 0.02330303192138672\n",
      "batch_size: 32 \t| epoch: 29 \t| train loss: 0.09991 \t| test loss: 0.11457\t | time taken: 0.02393198013305664\n",
      "batch_size: 32 \t| epoch: 30 \t| train loss: 0.09653 \t| test loss: 0.11164\t | time taken: 0.029283761978149414\n",
      "batch_size: 32 \t| epoch: 31 \t| train loss: 0.09164 \t| test loss: 0.10851\t | time taken: 0.025101900100708008\n",
      "batch_size: 32 \t| epoch: 32 \t| train loss: 0.08869 \t| test loss: 0.10543\t | time taken: 0.025393962860107422\n",
      "batch_size: 32 \t| epoch: 33 \t| train loss: 0.08474 \t| test loss: 0.10237\t | time taken: 0.0275421142578125\n",
      "batch_size: 32 \t| epoch: 34 \t| train loss: 0.08319 \t| test loss: 0.10026\t | time taken: 0.02416515350341797\n",
      "batch_size: 32 \t| epoch: 35 \t| train loss: 0.07328 \t| test loss: 0.09815\t | time taken: 0.02372598648071289\n",
      "batch_size: 32 \t| epoch: 36 \t| train loss: 0.07890 \t| test loss: 0.09701\t | time taken: 0.023983001708984375\n",
      "batch_size: 32 \t| epoch: 37 \t| train loss: 0.07578 \t| test loss: 0.09355\t | time taken: 0.0239260196685791\n",
      "batch_size: 32 \t| epoch: 38 \t| train loss: 0.06943 \t| test loss: 0.09020\t | time taken: 0.02317214012145996\n",
      "batch_size: 32 \t| epoch: 39 \t| train loss: 0.07006 \t| test loss: 0.08775\t | time taken: 0.02336907386779785\n",
      "batch_size: 32 \t| epoch: 40 \t| train loss: 0.06593 \t| test loss: 0.08599\t | time taken: 0.024339914321899414\n",
      "batch_size: 32 \t| epoch: 41 \t| train loss: 0.06194 \t| test loss: 0.08465\t | time taken: 0.024747133255004883\n",
      "batch_size: 32 \t| epoch: 42 \t| train loss: 0.06172 \t| test loss: 0.08389\t | time taken: 0.02447676658630371\n",
      "batch_size: 32 \t| epoch: 43 \t| train loss: 0.05428 \t| test loss: 0.08165\t | time taken: 0.023382186889648438\n",
      "batch_size: 32 \t| epoch: 44 \t| train loss: 0.05633 \t| test loss: 0.07931\t | time taken: 0.024492979049682617\n",
      "batch_size: 32 \t| epoch: 45 \t| train loss: 0.06177 \t| test loss: 0.07839\t | time taken: 0.02346491813659668\n",
      "batch_size: 32 \t| epoch: 46 \t| train loss: 0.05484 \t| test loss: 0.07741\t | time taken: 0.023190021514892578\n",
      "batch_size: 32 \t| epoch: 47 \t| train loss: 0.05560 \t| test loss: 0.07633\t | time taken: 0.028172016143798828\n",
      "batch_size: 32 \t| epoch: 48 \t| train loss: 0.05037 \t| test loss: 0.07461\t | time taken: 0.0243070125579834\n",
      "batch_size: 32 \t| epoch: 49 \t| train loss: 0.04729 \t| test loss: 0.07222\t | time taken: 0.06942510604858398\n",
      "batch_size: 32 \t| epoch: 50 \t| train loss: 0.04241 \t| test loss: 0.07071\t | time taken: 0.023726701736450195\n",
      "batch_size: 32 \t| epoch: 51 \t| train loss: 0.04328 \t| test loss: 0.06979\t | time taken: 0.02373194694519043\n",
      "batch_size: 32 \t| epoch: 52 \t| train loss: 0.04790 \t| test loss: 0.06789\t | time taken: 0.023287057876586914\n",
      "batch_size: 32 \t| epoch: 53 \t| train loss: 0.04211 \t| test loss: 0.06678\t | time taken: 0.023488998413085938\n",
      "batch_size: 32 \t| epoch: 54 \t| train loss: 0.04078 \t| test loss: 0.06615\t | time taken: 0.023941993713378906\n",
      "batch_size: 32 \t| epoch: 55 \t| train loss: 0.04700 \t| test loss: 0.06637\t | time taken: 0.025659799575805664\n",
      "batch_size: 32 \t| epoch: 56 \t| train loss: 0.03873 \t| test loss: 0.06540\t | time taken: 0.0233919620513916\n",
      "batch_size: 32 \t| epoch: 57 \t| train loss: 0.03187 \t| test loss: 0.06424\t | time taken: 0.024465084075927734\n",
      "batch_size: 32 \t| epoch: 58 \t| train loss: 0.03499 \t| test loss: 0.06207\t | time taken: 0.023478031158447266\n",
      "batch_size: 32 \t| epoch: 59 \t| train loss: 0.03504 \t| test loss: 0.06059\t | time taken: 0.023483991622924805\n",
      "batch_size: 32 \t| epoch: 60 \t| train loss: 0.03494 \t| test loss: 0.06034\t | time taken: 0.023166894912719727\n",
      "batch_size: 32 \t| epoch: 61 \t| train loss: 0.02737 \t| test loss: 0.05956\t | time taken: 0.023306846618652344\n",
      "batch_size: 32 \t| epoch: 62 \t| train loss: 0.03237 \t| test loss: 0.05784\t | time taken: 0.023868083953857422\n",
      "batch_size: 32 \t| epoch: 63 \t| train loss: 0.03583 \t| test loss: 0.05729\t | time taken: 0.02378225326538086\n",
      "batch_size: 32 \t| epoch: 64 \t| train loss: 0.02962 \t| test loss: 0.05636\t | time taken: 0.026927709579467773\n",
      "batch_size: 32 \t| epoch: 65 \t| train loss: 0.02818 \t| test loss: 0.05690\t | time taken: 0.023448944091796875\n",
      "batch_size: 32 \t| epoch: 66 \t| train loss: 0.02789 \t| test loss: 0.05644\t | time taken: 0.023634910583496094\n",
      "batch_size: 32 \t| epoch: 67 \t| train loss: 0.02730 \t| test loss: 0.05483\t | time taken: 0.023341894149780273\n",
      "batch_size: 32 \t| epoch: 68 \t| train loss: 0.02754 \t| test loss: 0.05393\t | time taken: 0.02311992645263672\n",
      "batch_size: 32 \t| epoch: 69 \t| train loss: 0.02788 \t| test loss: 0.05401\t | time taken: 0.023247957229614258\n",
      "batch_size: 32 \t| epoch: 70 \t| train loss: 0.02543 \t| test loss: 0.05368\t | time taken: 0.02339315414428711\n",
      "batch_size: 32 \t| epoch: 71 \t| train loss: 0.02517 \t| test loss: 0.05201\t | time taken: 0.023763179779052734\n",
      "batch_size: 32 \t| epoch: 72 \t| train loss: 0.01833 \t| test loss: 0.05056\t | time taken: 0.02472686767578125\n",
      "batch_size: 32 \t| epoch: 73 \t| train loss: 0.02051 \t| test loss: 0.05006\t | time taken: 0.023183822631835938\n",
      "batch_size: 32 \t| epoch: 74 \t| train loss: 0.02009 \t| test loss: 0.04946\t | time taken: 0.023732900619506836\n",
      "batch_size: 32 \t| epoch: 75 \t| train loss: 0.02726 \t| test loss: 0.05129\t | time taken: 0.023327112197875977\n",
      "batch_size: 32 \t| epoch: 76 \t| train loss: 0.01788 \t| test loss: 0.04920\t | time taken: 0.0229949951171875\n",
      "batch_size: 32 \t| epoch: 77 \t| train loss: 0.02029 \t| test loss: 0.04857\t | time taken: 0.022989988327026367\n",
      "batch_size: 32 \t| epoch: 78 \t| train loss: 0.01758 \t| test loss: 0.04891\t | time taken: 0.023405075073242188\n",
      "batch_size: 32 \t| epoch: 79 \t| train loss: 0.01585 \t| test loss: 0.04818\t | time taken: 0.023581981658935547\n",
      "batch_size: 32 \t| epoch: 80 \t| train loss: 0.01313 \t| test loss: 0.04709\t | time taken: 0.024279117584228516\n",
      "batch_size: 32 \t| epoch: 81 \t| train loss: 0.01644 \t| test loss: 0.04616\t | time taken: 0.023273944854736328\n",
      "batch_size: 32 \t| epoch: 82 \t| train loss: 0.01428 \t| test loss: 0.04589\t | time taken: 0.023643016815185547\n",
      "batch_size: 32 \t| epoch: 83 \t| train loss: 0.01135 \t| test loss: 0.04567\t | time taken: 0.02334904670715332\n",
      "batch_size: 32 \t| epoch: 84 \t| train loss: 0.02587 \t| test loss: 0.04754\t | time taken: 0.023314952850341797\n",
      "batch_size: 32 \t| epoch: 85 \t| train loss: 0.01894 \t| test loss: 0.04696\t | time taken: 0.02280879020690918\n",
      "batch_size: 32 \t| epoch: 86 \t| train loss: 0.01498 \t| test loss: 0.04812\t | time taken: 0.023395776748657227\n",
      "early stopping at epoch 86\n",
      "batch_size: 32 \t| epoch: 0 \t| train loss: 0.03688 \t| test loss: 0.04363\t | time taken: 0.025519132614135742\n",
      "batch_size: 32 \t| epoch: 1 \t| train loss: 0.02843 \t| test loss: 0.03711\t | time taken: 0.023324966430664062\n",
      "batch_size: 32 \t| epoch: 2 \t| train loss: 0.02823 \t| test loss: 0.03590\t | time taken: 0.023347139358520508\n",
      "batch_size: 32 \t| epoch: 3 \t| train loss: 0.01854 \t| test loss: 0.03133\t | time taken: 0.023094892501831055\n",
      "batch_size: 32 \t| epoch: 4 \t| train loss: 0.02356 \t| test loss: 0.02971\t | time taken: 0.023235797882080078\n",
      "batch_size: 32 \t| epoch: 5 \t| train loss: 0.02216 \t| test loss: 0.02740\t | time taken: 0.02295994758605957\n",
      "batch_size: 32 \t| epoch: 6 \t| train loss: 0.02460 \t| test loss: 0.02593\t | time taken: 0.024268150329589844\n",
      "batch_size: 32 \t| epoch: 7 \t| train loss: 0.01882 \t| test loss: 0.02398\t | time taken: 0.022908926010131836\n",
      "batch_size: 32 \t| epoch: 8 \t| train loss: 0.01989 \t| test loss: 0.02262\t | time taken: 0.023441314697265625\n",
      "batch_size: 32 \t| epoch: 9 \t| train loss: 0.02093 \t| test loss: 0.02057\t | time taken: 0.024052143096923828\n",
      "batch_size: 32 \t| epoch: 10 \t| train loss: 0.02001 \t| test loss: 0.02150\t | time taken: 0.023908138275146484\n",
      "batch_size: 32 \t| epoch: 11 \t| train loss: 0.01668 \t| test loss: 0.01964\t | time taken: 0.02280592918395996\n",
      "batch_size: 32 \t| epoch: 12 \t| train loss: 0.01679 \t| test loss: 0.01908\t | time taken: 0.023560762405395508\n",
      "batch_size: 32 \t| epoch: 13 \t| train loss: 0.01948 \t| test loss: 0.01901\t | time taken: 0.02462005615234375\n",
      "batch_size: 32 \t| epoch: 14 \t| train loss: 0.01826 \t| test loss: 0.01765\t | time taken: 0.023827075958251953\n",
      "batch_size: 32 \t| epoch: 15 \t| train loss: 0.02073 \t| test loss: 0.01648\t | time taken: 0.02284383773803711\n",
      "batch_size: 32 \t| epoch: 16 \t| train loss: 0.01382 \t| test loss: 0.01627\t | time taken: 0.02339482307434082\n",
      "batch_size: 32 \t| epoch: 17 \t| train loss: 0.01623 \t| test loss: 0.01639\t | time taken: 0.023189067840576172\n",
      "batch_size: 32 \t| epoch: 18 \t| train loss: 0.01232 \t| test loss: 0.01643\t | time taken: 0.024005889892578125\n",
      "batch_size: 32 \t| epoch: 19 \t| train loss: 0.01120 \t| test loss: 0.01512\t | time taken: 0.023521900177001953\n",
      "batch_size: 32 \t| epoch: 20 \t| train loss: 0.01274 \t| test loss: 0.01497\t | time taken: 0.023250102996826172\n",
      "batch_size: 32 \t| epoch: 21 \t| train loss: 0.01348 \t| test loss: 0.01406\t | time taken: 0.023270845413208008\n",
      "batch_size: 32 \t| epoch: 22 \t| train loss: 0.01669 \t| test loss: 0.01489\t | time taken: 0.0236358642578125\n",
      "batch_size: 32 \t| epoch: 23 \t| train loss: 0.01306 \t| test loss: 0.01279\t | time taken: 0.02325129508972168\n",
      "batch_size: 32 \t| epoch: 24 \t| train loss: 0.00958 \t| test loss: 0.01294\t | time taken: 0.023426055908203125\n",
      "batch_size: 32 \t| epoch: 25 \t| train loss: 0.01029 \t| test loss: 0.01298\t | time taken: 0.023061037063598633\n",
      "batch_size: 32 \t| epoch: 26 \t| train loss: 0.00676 \t| test loss: 0.01199\t | time taken: 0.024824857711791992\n",
      "batch_size: 32 \t| epoch: 27 \t| train loss: 0.00687 \t| test loss: 0.01132\t | time taken: 0.023211956024169922\n",
      "batch_size: 32 \t| epoch: 28 \t| train loss: 0.01065 \t| test loss: 0.01008\t | time taken: 0.023385286331176758\n",
      "batch_size: 32 \t| epoch: 29 \t| train loss: 0.01058 \t| test loss: 0.01079\t | time taken: 0.02305316925048828\n",
      "batch_size: 32 \t| epoch: 30 \t| train loss: 0.00954 \t| test loss: 0.01074\t | time taken: 0.0231170654296875\n",
      "batch_size: 32 \t| epoch: 31 \t| train loss: 0.01109 \t| test loss: 0.01084\t | time taken: 0.02343893051147461\n",
      "early stopping at epoch 31\n",
      "batch_size: 32 \t| epoch: 0 \t| train loss: 0.01465 \t| test loss: 0.00780\t | time taken: 0.023565053939819336\n",
      "batch_size: 32 \t| epoch: 1 \t| train loss: 0.01653 \t| test loss: 0.00585\t | time taken: 0.022902727127075195\n",
      "batch_size: 32 \t| epoch: 2 \t| train loss: 0.01978 \t| test loss: 0.00546\t | time taken: 0.02396082878112793\n",
      "batch_size: 32 \t| epoch: 3 \t| train loss: 0.01103 \t| test loss: 0.00485\t | time taken: 0.02430415153503418\n",
      "batch_size: 32 \t| epoch: 4 \t| train loss: 0.01231 \t| test loss: 0.00451\t | time taken: 0.024595975875854492\n",
      "batch_size: 32 \t| epoch: 5 \t| train loss: 0.01193 \t| test loss: 0.00461\t | time taken: 0.023228168487548828\n",
      "batch_size: 32 \t| epoch: 6 \t| train loss: 0.01395 \t| test loss: 0.00429\t | time taken: 0.02348780632019043\n",
      "batch_size: 32 \t| epoch: 7 \t| train loss: 0.01103 \t| test loss: 0.00400\t | time taken: 0.024413108825683594\n",
      "batch_size: 32 \t| epoch: 8 \t| train loss: 0.01439 \t| test loss: 0.00439\t | time taken: 0.02307605743408203\n",
      "batch_size: 32 \t| epoch: 9 \t| train loss: 0.01241 \t| test loss: 0.00438\t | time taken: 0.023222923278808594\n",
      "batch_size: 32 \t| epoch: 10 \t| train loss: 0.00849 \t| test loss: 0.00368\t | time taken: 0.023895740509033203\n",
      "batch_size: 32 \t| epoch: 11 \t| train loss: 0.00907 \t| test loss: 0.00352\t | time taken: 0.02427816390991211\n",
      "batch_size: 32 \t| epoch: 12 \t| train loss: 0.00713 \t| test loss: 0.00260\t | time taken: 0.024454116821289062\n",
      "batch_size: 32 \t| epoch: 13 \t| train loss: 0.00726 \t| test loss: 0.00239\t | time taken: 0.023422956466674805\n",
      "batch_size: 32 \t| epoch: 14 \t| train loss: 0.00537 \t| test loss: 0.00212\t | time taken: 0.023476123809814453\n",
      "batch_size: 32 \t| epoch: 15 \t| train loss: 0.00854 \t| test loss: 0.00202\t | time taken: 0.02314019203186035\n",
      "batch_size: 32 \t| epoch: 16 \t| train loss: 0.00653 \t| test loss: 0.00254\t | time taken: 0.02314281463623047\n",
      "batch_size: 32 \t| epoch: 17 \t| train loss: 0.00725 \t| test loss: 0.00264\t | time taken: 0.023308753967285156\n",
      "batch_size: 32 \t| epoch: 18 \t| train loss: 0.00763 \t| test loss: 0.00217\t | time taken: 0.02359318733215332\n",
      "early stopping at epoch 18\n",
      "batch_size: 32 \t| epoch: 0 \t| train loss: 0.00781 \t| test loss: 0.00190\t | time taken: 0.02478790283203125\n",
      "batch_size: 32 \t| epoch: 1 \t| train loss: 0.00988 \t| test loss: 0.00300\t | time taken: 0.022995948791503906\n",
      "batch_size: 32 \t| epoch: 2 \t| train loss: 0.00686 \t| test loss: 0.00141\t | time taken: 0.023441076278686523\n",
      "batch_size: 32 \t| epoch: 3 \t| train loss: 0.00488 \t| test loss: 0.00162\t | time taken: 0.022964000701904297\n",
      "batch_size: 32 \t| epoch: 4 \t| train loss: 0.00789 \t| test loss: 0.00176\t | time taken: 0.023076772689819336\n",
      "batch_size: 32 \t| epoch: 5 \t| train loss: 0.00397 \t| test loss: 0.00129\t | time taken: 0.02504277229309082\n",
      "batch_size: 32 \t| epoch: 6 \t| train loss: 0.00569 \t| test loss: 0.00127\t | time taken: 0.023530006408691406\n",
      "batch_size: 32 \t| epoch: 7 \t| train loss: 0.01054 \t| test loss: 0.00139\t | time taken: 0.023880958557128906\n",
      "batch_size: 32 \t| epoch: 8 \t| train loss: 0.00446 \t| test loss: 0.00096\t | time taken: 0.02447986602783203\n",
      "batch_size: 32 \t| epoch: 9 \t| train loss: 0.00715 \t| test loss: 0.00090\t | time taken: 0.023460865020751953\n",
      "batch_size: 32 \t| epoch: 10 \t| train loss: 0.00605 \t| test loss: 0.00087\t | time taken: 0.023134946823120117\n",
      "batch_size: 32 \t| epoch: 11 \t| train loss: 0.00579 \t| test loss: 0.00101\t | time taken: 0.02315497398376465\n",
      "batch_size: 32 \t| epoch: 12 \t| train loss: 0.00567 \t| test loss: 0.00107\t | time taken: 0.02303004264831543\n",
      "batch_size: 32 \t| epoch: 13 \t| train loss: 0.00789 \t| test loss: 0.00084\t | time taken: 0.02282094955444336\n",
      "batch_size: 32 \t| epoch: 14 \t| train loss: 0.00693 \t| test loss: 0.00110\t | time taken: 0.023675918579101562\n",
      "batch_size: 32 \t| epoch: 15 \t| train loss: 0.00360 \t| test loss: 0.00074\t | time taken: 0.02317190170288086\n",
      "batch_size: 32 \t| epoch: 16 \t| train loss: 0.00624 \t| test loss: 0.00080\t | time taken: 0.025019168853759766\n",
      "batch_size: 32 \t| epoch: 17 \t| train loss: 0.00417 \t| test loss: 0.00069\t | time taken: 0.023301124572753906\n",
      "batch_size: 32 \t| epoch: 18 \t| train loss: 0.00441 \t| test loss: 0.00070\t | time taken: 0.02475428581237793\n",
      "batch_size: 32 \t| epoch: 19 \t| train loss: 0.00325 \t| test loss: 0.00062\t | time taken: 0.023260831832885742\n",
      "batch_size: 32 \t| epoch: 20 \t| train loss: 0.00329 \t| test loss: 0.00062\t | time taken: 0.02325892448425293\n",
      "batch_size: 32 \t| epoch: 21 \t| train loss: 0.00379 \t| test loss: 0.00067\t | time taken: 0.023426055908203125\n",
      "batch_size: 32 \t| epoch: 22 \t| train loss: 0.00628 \t| test loss: 0.00080\t | time taken: 0.023334026336669922\n",
      "batch_size: 32 \t| epoch: 23 \t| train loss: 0.00366 \t| test loss: 0.00064\t | time taken: 0.02343273162841797\n",
      "early stopping at epoch 23\n",
      "batch_size: 32 \t| epoch: 0 \t| train loss: 0.00264 \t| test loss: 0.00054\t | time taken: 0.02384495735168457\n",
      "batch_size: 32 \t| epoch: 1 \t| train loss: 0.00501 \t| test loss: 0.00048\t | time taken: 0.024412155151367188\n",
      "batch_size: 32 \t| epoch: 2 \t| train loss: 0.01125 \t| test loss: 0.00045\t | time taken: 0.02350783348083496\n",
      "batch_size: 32 \t| epoch: 3 \t| train loss: 0.00656 \t| test loss: 0.00042\t | time taken: 0.023357868194580078\n",
      "batch_size: 32 \t| epoch: 4 \t| train loss: 0.00352 \t| test loss: 0.00053\t | time taken: 0.02417898178100586\n",
      "batch_size: 32 \t| epoch: 5 \t| train loss: 0.00351 \t| test loss: 0.00052\t | time taken: 0.02287006378173828\n",
      "batch_size: 32 \t| epoch: 6 \t| train loss: 0.00680 \t| test loss: 0.00046\t | time taken: 0.023148059844970703\n",
      "early stopping at epoch 6\n",
      "batch_size: 64 \t| epoch: 0 \t| train loss: 0.70008 \t| test loss: 0.67906\t | time taken: 0.01444697380065918\n",
      "batch_size: 64 \t| epoch: 1 \t| train loss: 0.68023 \t| test loss: 0.66434\t | time taken: 0.014449119567871094\n",
      "batch_size: 64 \t| epoch: 2 \t| train loss: 0.66460 \t| test loss: 0.64629\t | time taken: 0.014435052871704102\n",
      "batch_size: 64 \t| epoch: 3 \t| train loss: 0.64626 \t| test loss: 0.62347\t | time taken: 0.015128135681152344\n",
      "batch_size: 64 \t| epoch: 4 \t| train loss: 0.62363 \t| test loss: 0.59596\t | time taken: 0.014845132827758789\n",
      "batch_size: 64 \t| epoch: 5 \t| train loss: 0.59490 \t| test loss: 0.56526\t | time taken: 0.014595985412597656\n",
      "batch_size: 64 \t| epoch: 6 \t| train loss: 0.56336 \t| test loss: 0.53240\t | time taken: 0.015119075775146484\n",
      "batch_size: 64 \t| epoch: 7 \t| train loss: 0.52732 \t| test loss: 0.49819\t | time taken: 0.014375925064086914\n",
      "batch_size: 64 \t| epoch: 8 \t| train loss: 0.49177 \t| test loss: 0.46308\t | time taken: 0.014344930648803711\n",
      "batch_size: 64 \t| epoch: 9 \t| train loss: 0.45330 \t| test loss: 0.42748\t | time taken: 0.01422429084777832\n",
      "batch_size: 64 \t| epoch: 10 \t| train loss: 0.41574 \t| test loss: 0.39361\t | time taken: 0.01408696174621582\n",
      "batch_size: 64 \t| epoch: 11 \t| train loss: 0.38262 \t| test loss: 0.36353\t | time taken: 0.014333009719848633\n",
      "batch_size: 64 \t| epoch: 12 \t| train loss: 0.35488 \t| test loss: 0.33832\t | time taken: 0.014607906341552734\n",
      "batch_size: 64 \t| epoch: 13 \t| train loss: 0.33208 \t| test loss: 0.31786\t | time taken: 0.014127254486083984\n",
      "batch_size: 64 \t| epoch: 14 \t| train loss: 0.31369 \t| test loss: 0.30053\t | time taken: 0.014344930648803711\n",
      "batch_size: 64 \t| epoch: 15 \t| train loss: 0.29565 \t| test loss: 0.28559\t | time taken: 0.014176130294799805\n",
      "batch_size: 64 \t| epoch: 16 \t| train loss: 0.28039 \t| test loss: 0.27206\t | time taken: 0.014175176620483398\n",
      "batch_size: 64 \t| epoch: 17 \t| train loss: 0.26839 \t| test loss: 0.25938\t | time taken: 0.014935970306396484\n",
      "batch_size: 64 \t| epoch: 18 \t| train loss: 0.25649 \t| test loss: 0.24802\t | time taken: 0.014930009841918945\n",
      "batch_size: 64 \t| epoch: 19 \t| train loss: 0.24573 \t| test loss: 0.23774\t | time taken: 0.014549970626831055\n",
      "batch_size: 64 \t| epoch: 20 \t| train loss: 0.23105 \t| test loss: 0.22850\t | time taken: 0.01431894302368164\n",
      "batch_size: 64 \t| epoch: 21 \t| train loss: 0.21761 \t| test loss: 0.21984\t | time taken: 0.014500141143798828\n",
      "batch_size: 64 \t| epoch: 22 \t| train loss: 0.21246 \t| test loss: 0.21212\t | time taken: 0.014369010925292969\n",
      "batch_size: 64 \t| epoch: 23 \t| train loss: 0.20229 \t| test loss: 0.20529\t | time taken: 0.014301776885986328\n",
      "batch_size: 64 \t| epoch: 24 \t| train loss: 0.19096 \t| test loss: 0.19925\t | time taken: 0.014116287231445312\n",
      "batch_size: 64 \t| epoch: 25 \t| train loss: 0.18657 \t| test loss: 0.19403\t | time taken: 0.014642000198364258\n",
      "batch_size: 64 \t| epoch: 26 \t| train loss: 0.17752 \t| test loss: 0.18931\t | time taken: 0.014529943466186523\n",
      "batch_size: 64 \t| epoch: 27 \t| train loss: 0.17178 \t| test loss: 0.18513\t | time taken: 0.01415705680847168\n",
      "batch_size: 64 \t| epoch: 28 \t| train loss: 0.16899 \t| test loss: 0.18159\t | time taken: 0.014336109161376953\n",
      "batch_size: 64 \t| epoch: 29 \t| train loss: 0.16270 \t| test loss: 0.17794\t | time taken: 0.013740062713623047\n",
      "batch_size: 64 \t| epoch: 30 \t| train loss: 0.15669 \t| test loss: 0.17436\t | time taken: 0.014916181564331055\n",
      "batch_size: 64 \t| epoch: 31 \t| train loss: 0.14800 \t| test loss: 0.17136\t | time taken: 0.014490842819213867\n",
      "batch_size: 64 \t| epoch: 32 \t| train loss: 0.14327 \t| test loss: 0.16857\t | time taken: 0.015179872512817383\n",
      "batch_size: 64 \t| epoch: 33 \t| train loss: 0.13883 \t| test loss: 0.16555\t | time taken: 0.014609098434448242\n",
      "batch_size: 64 \t| epoch: 34 \t| train loss: 0.14376 \t| test loss: 0.16292\t | time taken: 0.014140844345092773\n",
      "batch_size: 64 \t| epoch: 35 \t| train loss: 0.12990 \t| test loss: 0.16028\t | time taken: 0.014485836029052734\n",
      "batch_size: 64 \t| epoch: 36 \t| train loss: 0.13387 \t| test loss: 0.15746\t | time taken: 0.014250040054321289\n",
      "batch_size: 64 \t| epoch: 37 \t| train loss: 0.13105 \t| test loss: 0.15493\t | time taken: 0.014467954635620117\n",
      "batch_size: 64 \t| epoch: 38 \t| train loss: 0.12484 \t| test loss: 0.15252\t | time taken: 0.01435995101928711\n",
      "batch_size: 64 \t| epoch: 39 \t| train loss: 0.12124 \t| test loss: 0.15016\t | time taken: 0.014656782150268555\n",
      "batch_size: 64 \t| epoch: 40 \t| train loss: 0.11720 \t| test loss: 0.14785\t | time taken: 0.014147043228149414\n",
      "batch_size: 64 \t| epoch: 41 \t| train loss: 0.11659 \t| test loss: 0.14563\t | time taken: 0.014272928237915039\n",
      "batch_size: 64 \t| epoch: 42 \t| train loss: 0.11142 \t| test loss: 0.14362\t | time taken: 0.014419078826904297\n",
      "batch_size: 64 \t| epoch: 43 \t| train loss: 0.10834 \t| test loss: 0.14149\t | time taken: 0.014839887619018555\n",
      "batch_size: 64 \t| epoch: 44 \t| train loss: 0.10508 \t| test loss: 0.13942\t | time taken: 0.0146331787109375\n",
      "batch_size: 64 \t| epoch: 45 \t| train loss: 0.10736 \t| test loss: 0.13714\t | time taken: 0.014935970306396484\n",
      "batch_size: 64 \t| epoch: 46 \t| train loss: 0.10387 \t| test loss: 0.13486\t | time taken: 0.014365196228027344\n",
      "batch_size: 64 \t| epoch: 47 \t| train loss: 0.09923 \t| test loss: 0.13277\t | time taken: 0.014094829559326172\n",
      "batch_size: 64 \t| epoch: 48 \t| train loss: 0.09617 \t| test loss: 0.13069\t | time taken: 0.014030933380126953\n",
      "batch_size: 64 \t| epoch: 49 \t| train loss: 0.09786 \t| test loss: 0.12846\t | time taken: 0.014405250549316406\n",
      "batch_size: 64 \t| epoch: 50 \t| train loss: 0.09796 \t| test loss: 0.12647\t | time taken: 0.014303922653198242\n",
      "batch_size: 64 \t| epoch: 51 \t| train loss: 0.09323 \t| test loss: 0.12447\t | time taken: 0.014832735061645508\n",
      "batch_size: 64 \t| epoch: 52 \t| train loss: 0.09005 \t| test loss: 0.12258\t | time taken: 0.014528989791870117\n",
      "batch_size: 64 \t| epoch: 53 \t| train loss: 0.09402 \t| test loss: 0.12079\t | time taken: 0.014306783676147461\n",
      "batch_size: 64 \t| epoch: 54 \t| train loss: 0.08591 \t| test loss: 0.11898\t | time taken: 0.014236927032470703\n",
      "batch_size: 64 \t| epoch: 55 \t| train loss: 0.08370 \t| test loss: 0.11717\t | time taken: 0.014787912368774414\n",
      "batch_size: 64 \t| epoch: 56 \t| train loss: 0.08325 \t| test loss: 0.11584\t | time taken: 0.01501011848449707\n",
      "batch_size: 64 \t| epoch: 57 \t| train loss: 0.08383 \t| test loss: 0.11406\t | time taken: 0.014719963073730469\n",
      "batch_size: 64 \t| epoch: 58 \t| train loss: 0.08035 \t| test loss: 0.11258\t | time taken: 0.014651298522949219\n",
      "batch_size: 64 \t| epoch: 59 \t| train loss: 0.07731 \t| test loss: 0.11149\t | time taken: 0.014610052108764648\n",
      "batch_size: 64 \t| epoch: 60 \t| train loss: 0.06941 \t| test loss: 0.11016\t | time taken: 0.014545917510986328\n",
      "batch_size: 64 \t| epoch: 61 \t| train loss: 0.07521 \t| test loss: 0.10859\t | time taken: 0.014375925064086914\n",
      "batch_size: 64 \t| epoch: 62 \t| train loss: 0.06990 \t| test loss: 0.10643\t | time taken: 0.014154911041259766\n",
      "batch_size: 64 \t| epoch: 63 \t| train loss: 0.07533 \t| test loss: 0.10515\t | time taken: 0.014325857162475586\n",
      "batch_size: 64 \t| epoch: 64 \t| train loss: 0.07187 \t| test loss: 0.10337\t | time taken: 0.014432907104492188\n",
      "batch_size: 64 \t| epoch: 65 \t| train loss: 0.06643 \t| test loss: 0.10148\t | time taken: 0.014664888381958008\n",
      "batch_size: 64 \t| epoch: 66 \t| train loss: 0.06524 \t| test loss: 0.09989\t | time taken: 0.014435291290283203\n",
      "batch_size: 64 \t| epoch: 67 \t| train loss: 0.06729 \t| test loss: 0.09854\t | time taken: 0.014468908309936523\n",
      "batch_size: 64 \t| epoch: 68 \t| train loss: 0.06230 \t| test loss: 0.09681\t | time taken: 0.01593613624572754\n",
      "batch_size: 64 \t| epoch: 69 \t| train loss: 0.06456 \t| test loss: 0.09553\t | time taken: 0.015532970428466797\n",
      "batch_size: 64 \t| epoch: 70 \t| train loss: 0.06054 \t| test loss: 0.09449\t | time taken: 0.014902114868164062\n",
      "batch_size: 64 \t| epoch: 71 \t| train loss: 0.05739 \t| test loss: 0.09243\t | time taken: 0.014770984649658203\n",
      "batch_size: 64 \t| epoch: 72 \t| train loss: 0.05914 \t| test loss: 0.09069\t | time taken: 0.014492034912109375\n",
      "batch_size: 64 \t| epoch: 73 \t| train loss: 0.05734 \t| test loss: 0.08931\t | time taken: 0.014377832412719727\n",
      "batch_size: 64 \t| epoch: 74 \t| train loss: 0.05238 \t| test loss: 0.08794\t | time taken: 0.014301776885986328\n",
      "batch_size: 64 \t| epoch: 75 \t| train loss: 0.05679 \t| test loss: 0.08631\t | time taken: 0.014302968978881836\n",
      "batch_size: 64 \t| epoch: 76 \t| train loss: 0.05632 \t| test loss: 0.08545\t | time taken: 0.014338970184326172\n",
      "batch_size: 64 \t| epoch: 77 \t| train loss: 0.05547 \t| test loss: 0.08417\t | time taken: 0.014623880386352539\n",
      "batch_size: 64 \t| epoch: 78 \t| train loss: 0.05096 \t| test loss: 0.08261\t | time taken: 0.014642715454101562\n",
      "batch_size: 64 \t| epoch: 79 \t| train loss: 0.04869 \t| test loss: 0.08099\t | time taken: 0.014378070831298828\n",
      "batch_size: 64 \t| epoch: 80 \t| train loss: 0.04481 \t| test loss: 0.07934\t | time taken: 0.014130115509033203\n",
      "batch_size: 64 \t| epoch: 81 \t| train loss: 0.04475 \t| test loss: 0.07885\t | time taken: 0.014688968658447266\n",
      "batch_size: 64 \t| epoch: 82 \t| train loss: 0.04918 \t| test loss: 0.07804\t | time taken: 0.01497197151184082\n",
      "batch_size: 64 \t| epoch: 83 \t| train loss: 0.04994 \t| test loss: 0.07622\t | time taken: 0.014864921569824219\n",
      "batch_size: 64 \t| epoch: 84 \t| train loss: 0.04468 \t| test loss: 0.07470\t | time taken: 0.014387845993041992\n",
      "batch_size: 64 \t| epoch: 85 \t| train loss: 0.04695 \t| test loss: 0.07347\t | time taken: 0.014718055725097656\n",
      "batch_size: 64 \t| epoch: 86 \t| train loss: 0.04711 \t| test loss: 0.07261\t | time taken: 0.014921903610229492\n",
      "batch_size: 64 \t| epoch: 87 \t| train loss: 0.04443 \t| test loss: 0.07159\t | time taken: 0.015245199203491211\n",
      "batch_size: 64 \t| epoch: 88 \t| train loss: 0.03980 \t| test loss: 0.06982\t | time taken: 0.014389991760253906\n",
      "batch_size: 64 \t| epoch: 89 \t| train loss: 0.04490 \t| test loss: 0.06914\t | time taken: 0.014327049255371094\n",
      "batch_size: 64 \t| epoch: 90 \t| train loss: 0.04333 \t| test loss: 0.06822\t | time taken: 0.014429330825805664\n",
      "batch_size: 64 \t| epoch: 91 \t| train loss: 0.03501 \t| test loss: 0.06655\t | time taken: 0.01424717903137207\n",
      "batch_size: 64 \t| epoch: 92 \t| train loss: 0.03566 \t| test loss: 0.06564\t | time taken: 0.014549732208251953\n",
      "batch_size: 64 \t| epoch: 93 \t| train loss: 0.03565 \t| test loss: 0.06569\t | time taken: 0.01429295539855957\n",
      "batch_size: 64 \t| epoch: 94 \t| train loss: 0.03755 \t| test loss: 0.06484\t | time taken: 0.014677047729492188\n",
      "batch_size: 64 \t| epoch: 95 \t| train loss: 0.03261 \t| test loss: 0.06372\t | time taken: 0.015548944473266602\n",
      "batch_size: 64 \t| epoch: 96 \t| train loss: 0.03478 \t| test loss: 0.06178\t | time taken: 0.014672040939331055\n",
      "batch_size: 64 \t| epoch: 97 \t| train loss: 0.02858 \t| test loss: 0.06032\t | time taken: 0.014468908309936523\n",
      "batch_size: 64 \t| epoch: 98 \t| train loss: 0.03208 \t| test loss: 0.06009\t | time taken: 0.014555931091308594\n",
      "batch_size: 64 \t| epoch: 99 \t| train loss: 0.03038 \t| test loss: 0.05961\t | time taken: 0.01428985595703125\n",
      "reach max number of epoch\n",
      "batch_size: 64 \t| epoch: 0 \t| train loss: 0.04712 \t| test loss: 0.05912\t | time taken: 0.014619827270507812\n",
      "batch_size: 64 \t| epoch: 1 \t| train loss: 0.04594 \t| test loss: 0.05477\t | time taken: 0.014539003372192383\n",
      "batch_size: 64 \t| epoch: 2 \t| train loss: 0.04080 \t| test loss: 0.05113\t | time taken: 0.014390945434570312\n",
      "batch_size: 64 \t| epoch: 3 \t| train loss: 0.04124 \t| test loss: 0.04931\t | time taken: 0.014217853546142578\n",
      "batch_size: 64 \t| epoch: 4 \t| train loss: 0.03854 \t| test loss: 0.04746\t | time taken: 0.014182090759277344\n",
      "batch_size: 64 \t| epoch: 5 \t| train loss: 0.03719 \t| test loss: 0.04484\t | time taken: 0.014333963394165039\n",
      "batch_size: 64 \t| epoch: 6 \t| train loss: 0.03536 \t| test loss: 0.04276\t | time taken: 0.01456904411315918\n",
      "batch_size: 64 \t| epoch: 7 \t| train loss: 0.03353 \t| test loss: 0.04186\t | time taken: 0.015146970748901367\n",
      "batch_size: 64 \t| epoch: 8 \t| train loss: 0.03228 \t| test loss: 0.04065\t | time taken: 0.014472007751464844\n",
      "batch_size: 64 \t| epoch: 9 \t| train loss: 0.02846 \t| test loss: 0.03914\t | time taken: 0.014644861221313477\n",
      "batch_size: 64 \t| epoch: 10 \t| train loss: 0.02617 \t| test loss: 0.03719\t | time taken: 0.014323949813842773\n",
      "batch_size: 64 \t| epoch: 11 \t| train loss: 0.02689 \t| test loss: 0.03627\t | time taken: 0.014423131942749023\n",
      "batch_size: 64 \t| epoch: 12 \t| train loss: 0.02934 \t| test loss: 0.03580\t | time taken: 0.014603853225708008\n",
      "batch_size: 64 \t| epoch: 13 \t| train loss: 0.02566 \t| test loss: 0.03510\t | time taken: 0.014428853988647461\n",
      "batch_size: 64 \t| epoch: 14 \t| train loss: 0.03252 \t| test loss: 0.03619\t | time taken: 0.01430201530456543\n",
      "batch_size: 64 \t| epoch: 15 \t| train loss: 0.02660 \t| test loss: 0.03441\t | time taken: 0.014503955841064453\n",
      "batch_size: 64 \t| epoch: 16 \t| train loss: 0.03186 \t| test loss: 0.03544\t | time taken: 0.01834583282470703\n",
      "batch_size: 64 \t| epoch: 17 \t| train loss: 0.02259 \t| test loss: 0.03499\t | time taken: 0.014415979385375977\n",
      "batch_size: 64 \t| epoch: 18 \t| train loss: 0.02168 \t| test loss: 0.03333\t | time taken: 0.014542102813720703\n",
      "batch_size: 64 \t| epoch: 19 \t| train loss: 0.02910 \t| test loss: 0.03281\t | time taken: 0.014412164688110352\n",
      "batch_size: 64 \t| epoch: 20 \t| train loss: 0.02393 \t| test loss: 0.03175\t | time taken: 0.014469146728515625\n",
      "batch_size: 64 \t| epoch: 21 \t| train loss: 0.02359 \t| test loss: 0.03106\t | time taken: 0.015285968780517578\n",
      "batch_size: 64 \t| epoch: 22 \t| train loss: 0.02562 \t| test loss: 0.03057\t | time taken: 0.014998912811279297\n",
      "batch_size: 64 \t| epoch: 23 \t| train loss: 0.02617 \t| test loss: 0.03122\t | time taken: 0.014328718185424805\n",
      "batch_size: 64 \t| epoch: 24 \t| train loss: 0.02506 \t| test loss: 0.03079\t | time taken: 0.014633893966674805\n",
      "batch_size: 64 \t| epoch: 25 \t| train loss: 0.01858 \t| test loss: 0.02905\t | time taken: 0.014497041702270508\n",
      "batch_size: 64 \t| epoch: 26 \t| train loss: 0.02691 \t| test loss: 0.02895\t | time taken: 0.014466047286987305\n",
      "batch_size: 64 \t| epoch: 27 \t| train loss: 0.01862 \t| test loss: 0.02846\t | time taken: 0.014405250549316406\n",
      "batch_size: 64 \t| epoch: 28 \t| train loss: 0.02242 \t| test loss: 0.02843\t | time taken: 0.014259815216064453\n",
      "batch_size: 64 \t| epoch: 29 \t| train loss: 0.02512 \t| test loss: 0.02875\t | time taken: 0.014342069625854492\n",
      "batch_size: 64 \t| epoch: 30 \t| train loss: 0.02062 \t| test loss: 0.02755\t | time taken: 0.014368057250976562\n",
      "batch_size: 64 \t| epoch: 31 \t| train loss: 0.01829 \t| test loss: 0.03102\t | time taken: 0.014300107955932617\n",
      "batch_size: 64 \t| epoch: 32 \t| train loss: 0.02226 \t| test loss: 0.03365\t | time taken: 0.014452934265136719\n",
      "batch_size: 64 \t| epoch: 33 \t| train loss: 0.01505 \t| test loss: 0.03222\t | time taken: 0.01474308967590332\n",
      "early stopping at epoch 33\n",
      "batch_size: 64 \t| epoch: 0 \t| train loss: 0.03042 \t| test loss: 0.02589\t | time taken: 0.01483011245727539\n",
      "batch_size: 64 \t| epoch: 1 \t| train loss: 0.02818 \t| test loss: 0.02163\t | time taken: 0.014374732971191406\n",
      "batch_size: 64 \t| epoch: 2 \t| train loss: 0.02187 \t| test loss: 0.02074\t | time taken: 0.014445781707763672\n",
      "batch_size: 64 \t| epoch: 3 \t| train loss: 0.02786 \t| test loss: 0.02038\t | time taken: 0.014471769332885742\n",
      "batch_size: 64 \t| epoch: 4 \t| train loss: 0.02413 \t| test loss: 0.01858\t | time taken: 0.014393806457519531\n",
      "batch_size: 64 \t| epoch: 5 \t| train loss: 0.02323 \t| test loss: 0.01820\t | time taken: 0.01429295539855957\n",
      "batch_size: 64 \t| epoch: 6 \t| train loss: 0.02282 \t| test loss: 0.01727\t | time taken: 0.014192819595336914\n",
      "batch_size: 64 \t| epoch: 7 \t| train loss: 0.02528 \t| test loss: 0.01640\t | time taken: 0.014338016510009766\n",
      "batch_size: 64 \t| epoch: 8 \t| train loss: 0.02302 \t| test loss: 0.01606\t | time taken: 0.014316082000732422\n",
      "batch_size: 64 \t| epoch: 9 \t| train loss: 0.01687 \t| test loss: 0.01539\t | time taken: 0.014410018920898438\n",
      "batch_size: 64 \t| epoch: 10 \t| train loss: 0.01717 \t| test loss: 0.01489\t | time taken: 0.014345884323120117\n",
      "batch_size: 64 \t| epoch: 11 \t| train loss: 0.02258 \t| test loss: 0.01454\t | time taken: 0.014172077178955078\n",
      "batch_size: 64 \t| epoch: 12 \t| train loss: 0.01751 \t| test loss: 0.01424\t | time taken: 0.01473093032836914\n",
      "batch_size: 64 \t| epoch: 13 \t| train loss: 0.01829 \t| test loss: 0.01452\t | time taken: 0.014948844909667969\n",
      "batch_size: 64 \t| epoch: 14 \t| train loss: 0.02522 \t| test loss: 0.01456\t | time taken: 0.014523029327392578\n",
      "batch_size: 64 \t| epoch: 15 \t| train loss: 0.01847 \t| test loss: 0.01321\t | time taken: 0.014470815658569336\n",
      "batch_size: 64 \t| epoch: 16 \t| train loss: 0.01651 \t| test loss: 0.01168\t | time taken: 0.014312982559204102\n",
      "batch_size: 64 \t| epoch: 17 \t| train loss: 0.02353 \t| test loss: 0.01165\t | time taken: 0.014373779296875\n",
      "batch_size: 64 \t| epoch: 18 \t| train loss: 0.01587 \t| test loss: 0.01196\t | time taken: 0.014238834381103516\n",
      "batch_size: 64 \t| epoch: 19 \t| train loss: 0.01740 \t| test loss: 0.01109\t | time taken: 0.014394044876098633\n",
      "batch_size: 64 \t| epoch: 20 \t| train loss: 0.01354 \t| test loss: 0.01042\t | time taken: 0.014573097229003906\n",
      "batch_size: 64 \t| epoch: 21 \t| train loss: 0.02111 \t| test loss: 0.00856\t | time taken: 0.014278888702392578\n",
      "batch_size: 64 \t| epoch: 22 \t| train loss: 0.01292 \t| test loss: 0.00959\t | time taken: 0.014363765716552734\n",
      "batch_size: 64 \t| epoch: 23 \t| train loss: 0.01673 \t| test loss: 0.00929\t | time taken: 0.014333009719848633\n",
      "batch_size: 64 \t| epoch: 24 \t| train loss: 0.01241 \t| test loss: 0.00875\t | time taken: 0.01405191421508789\n",
      "early stopping at epoch 24\n",
      "batch_size: 64 \t| epoch: 0 \t| train loss: 0.01193 \t| test loss: 0.00797\t | time taken: 0.01508188247680664\n",
      "batch_size: 64 \t| epoch: 1 \t| train loss: 0.01300 \t| test loss: 0.00766\t | time taken: 0.015192985534667969\n",
      "batch_size: 64 \t| epoch: 2 \t| train loss: 0.01631 \t| test loss: 0.00734\t | time taken: 0.014774560928344727\n",
      "batch_size: 64 \t| epoch: 3 \t| train loss: 0.01930 \t| test loss: 0.00653\t | time taken: 0.014536142349243164\n",
      "batch_size: 64 \t| epoch: 4 \t| train loss: 0.01455 \t| test loss: 0.00639\t | time taken: 0.014593124389648438\n",
      "batch_size: 64 \t| epoch: 5 \t| train loss: 0.01779 \t| test loss: 0.00643\t | time taken: 0.014735937118530273\n",
      "batch_size: 64 \t| epoch: 6 \t| train loss: 0.01789 \t| test loss: 0.00629\t | time taken: 0.014759302139282227\n",
      "batch_size: 64 \t| epoch: 7 \t| train loss: 0.01676 \t| test loss: 0.00605\t | time taken: 0.015208959579467773\n",
      "batch_size: 64 \t| epoch: 8 \t| train loss: 0.01333 \t| test loss: 0.00567\t | time taken: 0.014544963836669922\n",
      "batch_size: 64 \t| epoch: 9 \t| train loss: 0.01142 \t| test loss: 0.00554\t | time taken: 0.014411211013793945\n",
      "batch_size: 64 \t| epoch: 10 \t| train loss: 0.01214 \t| test loss: 0.00533\t | time taken: 0.014431953430175781\n",
      "batch_size: 64 \t| epoch: 11 \t| train loss: 0.01359 \t| test loss: 0.00503\t | time taken: 0.01446676254272461\n",
      "batch_size: 64 \t| epoch: 12 \t| train loss: 0.01202 \t| test loss: 0.00524\t | time taken: 0.014132022857666016\n",
      "batch_size: 64 \t| epoch: 13 \t| train loss: 0.01297 \t| test loss: 0.00570\t | time taken: 0.014693260192871094\n",
      "batch_size: 64 \t| epoch: 14 \t| train loss: 0.01300 \t| test loss: 0.00506\t | time taken: 0.022998809814453125\n",
      "early stopping at epoch 14\n",
      "batch_size: 64 \t| epoch: 0 \t| train loss: 0.01141 \t| test loss: 0.00479\t | time taken: 0.014978885650634766\n",
      "batch_size: 64 \t| epoch: 1 \t| train loss: 0.01541 \t| test loss: 0.00472\t | time taken: 0.01501011848449707\n",
      "batch_size: 64 \t| epoch: 2 \t| train loss: 0.01010 \t| test loss: 0.00409\t | time taken: 0.014532089233398438\n",
      "batch_size: 64 \t| epoch: 3 \t| train loss: 0.01438 \t| test loss: 0.00402\t | time taken: 0.014453887939453125\n",
      "batch_size: 64 \t| epoch: 4 \t| train loss: 0.01103 \t| test loss: 0.00431\t | time taken: 0.014599084854125977\n",
      "batch_size: 64 \t| epoch: 5 \t| train loss: 0.01142 \t| test loss: 0.00423\t | time taken: 0.014506816864013672\n",
      "batch_size: 64 \t| epoch: 6 \t| train loss: 0.01279 \t| test loss: 0.00463\t | time taken: 0.014323949813842773\n",
      "early stopping at epoch 6\n",
      "batch_size: 128 \t| epoch: 0 \t| train loss: 0.70515 \t| test loss: 0.68978\t | time taken: 0.00952601432800293\n",
      "batch_size: 128 \t| epoch: 1 \t| train loss: 0.69043 \t| test loss: 0.68087\t | time taken: 0.009540081024169922\n",
      "batch_size: 128 \t| epoch: 2 \t| train loss: 0.68238 \t| test loss: 0.67221\t | time taken: 0.009310007095336914\n",
      "batch_size: 128 \t| epoch: 3 \t| train loss: 0.67407 \t| test loss: 0.66301\t | time taken: 0.009258031845092773\n",
      "batch_size: 128 \t| epoch: 4 \t| train loss: 0.66440 \t| test loss: 0.65282\t | time taken: 0.0094451904296875\n",
      "batch_size: 128 \t| epoch: 5 \t| train loss: 0.65483 \t| test loss: 0.64144\t | time taken: 0.00896906852722168\n",
      "batch_size: 128 \t| epoch: 6 \t| train loss: 0.64364 \t| test loss: 0.62851\t | time taken: 0.009601116180419922\n",
      "batch_size: 128 \t| epoch: 7 \t| train loss: 0.63044 \t| test loss: 0.61399\t | time taken: 0.010150909423828125\n",
      "batch_size: 128 \t| epoch: 8 \t| train loss: 0.61575 \t| test loss: 0.59787\t | time taken: 0.009458065032958984\n",
      "batch_size: 128 \t| epoch: 9 \t| train loss: 0.59825 \t| test loss: 0.57989\t | time taken: 0.009350299835205078\n",
      "batch_size: 128 \t| epoch: 10 \t| train loss: 0.58033 \t| test loss: 0.56055\t | time taken: 0.009258031845092773\n",
      "batch_size: 128 \t| epoch: 11 \t| train loss: 0.56167 \t| test loss: 0.54014\t | time taken: 0.009084939956665039\n",
      "batch_size: 128 \t| epoch: 12 \t| train loss: 0.53954 \t| test loss: 0.51894\t | time taken: 0.009290933609008789\n",
      "batch_size: 128 \t| epoch: 13 \t| train loss: 0.51760 \t| test loss: 0.49713\t | time taken: 0.009242057800292969\n",
      "batch_size: 128 \t| epoch: 14 \t| train loss: 0.49482 \t| test loss: 0.47548\t | time taken: 0.009216070175170898\n",
      "batch_size: 128 \t| epoch: 15 \t| train loss: 0.47315 \t| test loss: 0.45446\t | time taken: 0.009189128875732422\n",
      "batch_size: 128 \t| epoch: 16 \t| train loss: 0.45175 \t| test loss: 0.43457\t | time taken: 0.00927591323852539\n",
      "batch_size: 128 \t| epoch: 17 \t| train loss: 0.43059 \t| test loss: 0.41580\t | time taken: 0.009191274642944336\n",
      "batch_size: 128 \t| epoch: 18 \t| train loss: 0.41626 \t| test loss: 0.39853\t | time taken: 0.009169816970825195\n",
      "batch_size: 128 \t| epoch: 19 \t| train loss: 0.39587 \t| test loss: 0.38279\t | time taken: 0.009290933609008789\n",
      "batch_size: 128 \t| epoch: 20 \t| train loss: 0.37705 \t| test loss: 0.36829\t | time taken: 0.009392023086547852\n",
      "batch_size: 128 \t| epoch: 21 \t| train loss: 0.36433 \t| test loss: 0.35519\t | time taken: 0.009222984313964844\n",
      "batch_size: 128 \t| epoch: 22 \t| train loss: 0.35353 \t| test loss: 0.34322\t | time taken: 0.009656667709350586\n",
      "batch_size: 128 \t| epoch: 23 \t| train loss: 0.34169 \t| test loss: 0.33195\t | time taken: 0.009271860122680664\n",
      "batch_size: 128 \t| epoch: 24 \t| train loss: 0.33076 \t| test loss: 0.32135\t | time taken: 0.009423017501831055\n",
      "batch_size: 128 \t| epoch: 25 \t| train loss: 0.32299 \t| test loss: 0.31133\t | time taken: 0.009107112884521484\n",
      "batch_size: 128 \t| epoch: 26 \t| train loss: 0.30668 \t| test loss: 0.30178\t | time taken: 0.00976109504699707\n",
      "batch_size: 128 \t| epoch: 27 \t| train loss: 0.29985 \t| test loss: 0.29242\t | time taken: 0.009213924407958984\n",
      "batch_size: 128 \t| epoch: 28 \t| train loss: 0.29081 \t| test loss: 0.28344\t | time taken: 0.009555816650390625\n",
      "batch_size: 128 \t| epoch: 29 \t| train loss: 0.28029 \t| test loss: 0.27486\t | time taken: 0.010015010833740234\n",
      "batch_size: 128 \t| epoch: 30 \t| train loss: 0.27235 \t| test loss: 0.26664\t | time taken: 0.009677886962890625\n",
      "batch_size: 128 \t| epoch: 31 \t| train loss: 0.26777 \t| test loss: 0.25888\t | time taken: 0.009177207946777344\n",
      "batch_size: 128 \t| epoch: 32 \t| train loss: 0.25210 \t| test loss: 0.25153\t | time taken: 0.009337902069091797\n",
      "batch_size: 128 \t| epoch: 33 \t| train loss: 0.24732 \t| test loss: 0.24461\t | time taken: 0.00943613052368164\n",
      "batch_size: 128 \t| epoch: 34 \t| train loss: 0.23899 \t| test loss: 0.23805\t | time taken: 0.009470939636230469\n",
      "batch_size: 128 \t| epoch: 35 \t| train loss: 0.22958 \t| test loss: 0.23194\t | time taken: 0.009411096572875977\n",
      "batch_size: 128 \t| epoch: 36 \t| train loss: 0.22684 \t| test loss: 0.22640\t | time taken: 0.009181976318359375\n",
      "batch_size: 128 \t| epoch: 37 \t| train loss: 0.21738 \t| test loss: 0.22115\t | time taken: 0.00924372673034668\n",
      "batch_size: 128 \t| epoch: 38 \t| train loss: 0.21201 \t| test loss: 0.21621\t | time taken: 0.009092092514038086\n",
      "batch_size: 128 \t| epoch: 39 \t| train loss: 0.20098 \t| test loss: 0.21143\t | time taken: 0.009367227554321289\n",
      "batch_size: 128 \t| epoch: 40 \t| train loss: 0.19847 \t| test loss: 0.20696\t | time taken: 0.009340047836303711\n",
      "batch_size: 128 \t| epoch: 41 \t| train loss: 0.19962 \t| test loss: 0.20293\t | time taken: 0.009391069412231445\n",
      "batch_size: 128 \t| epoch: 42 \t| train loss: 0.18871 \t| test loss: 0.19900\t | time taken: 0.009453058242797852\n",
      "batch_size: 128 \t| epoch: 43 \t| train loss: 0.18260 \t| test loss: 0.19524\t | time taken: 0.00937795639038086\n",
      "batch_size: 128 \t| epoch: 44 \t| train loss: 0.18236 \t| test loss: 0.19185\t | time taken: 0.009212970733642578\n",
      "batch_size: 128 \t| epoch: 45 \t| train loss: 0.17408 \t| test loss: 0.18849\t | time taken: 0.009343147277832031\n",
      "batch_size: 128 \t| epoch: 46 \t| train loss: 0.17047 \t| test loss: 0.18541\t | time taken: 0.009604692459106445\n",
      "batch_size: 128 \t| epoch: 47 \t| train loss: 0.16906 \t| test loss: 0.18228\t | time taken: 0.00962519645690918\n",
      "batch_size: 128 \t| epoch: 48 \t| train loss: 0.16765 \t| test loss: 0.17931\t | time taken: 0.009527921676635742\n",
      "batch_size: 128 \t| epoch: 49 \t| train loss: 0.16255 \t| test loss: 0.17662\t | time taken: 0.009702920913696289\n",
      "batch_size: 128 \t| epoch: 50 \t| train loss: 0.16215 \t| test loss: 0.17395\t | time taken: 0.018826961517333984\n",
      "batch_size: 128 \t| epoch: 51 \t| train loss: 0.14831 \t| test loss: 0.17120\t | time taken: 0.009386777877807617\n",
      "batch_size: 128 \t| epoch: 52 \t| train loss: 0.15535 \t| test loss: 0.16883\t | time taken: 0.00964808464050293\n",
      "batch_size: 128 \t| epoch: 53 \t| train loss: 0.14714 \t| test loss: 0.16641\t | time taken: 0.009146928787231445\n",
      "batch_size: 128 \t| epoch: 54 \t| train loss: 0.14634 \t| test loss: 0.16396\t | time taken: 0.009573698043823242\n",
      "batch_size: 128 \t| epoch: 55 \t| train loss: 0.14193 \t| test loss: 0.16155\t | time taken: 0.009396076202392578\n",
      "batch_size: 128 \t| epoch: 56 \t| train loss: 0.13886 \t| test loss: 0.15920\t | time taken: 0.009260177612304688\n",
      "batch_size: 128 \t| epoch: 57 \t| train loss: 0.13511 \t| test loss: 0.15695\t | time taken: 0.00928497314453125\n",
      "batch_size: 128 \t| epoch: 58 \t| train loss: 0.13052 \t| test loss: 0.15473\t | time taken: 0.009207963943481445\n",
      "batch_size: 128 \t| epoch: 59 \t| train loss: 0.12784 \t| test loss: 0.15249\t | time taken: 0.009138107299804688\n",
      "batch_size: 128 \t| epoch: 60 \t| train loss: 0.12767 \t| test loss: 0.15027\t | time taken: 0.0092620849609375\n",
      "batch_size: 128 \t| epoch: 61 \t| train loss: 0.12352 \t| test loss: 0.14814\t | time taken: 0.009270906448364258\n",
      "batch_size: 128 \t| epoch: 62 \t| train loss: 0.12330 \t| test loss: 0.14622\t | time taken: 0.00929713249206543\n",
      "batch_size: 128 \t| epoch: 63 \t| train loss: 0.12161 \t| test loss: 0.14409\t | time taken: 0.009244918823242188\n",
      "batch_size: 128 \t| epoch: 64 \t| train loss: 0.12074 \t| test loss: 0.14204\t | time taken: 0.009210824966430664\n",
      "batch_size: 128 \t| epoch: 65 \t| train loss: 0.11713 \t| test loss: 0.14009\t | time taken: 0.009563922882080078\n",
      "batch_size: 128 \t| epoch: 66 \t| train loss: 0.12127 \t| test loss: 0.13831\t | time taken: 0.010109901428222656\n",
      "batch_size: 128 \t| epoch: 67 \t| train loss: 0.10896 \t| test loss: 0.13644\t | time taken: 0.009814023971557617\n",
      "batch_size: 128 \t| epoch: 68 \t| train loss: 0.11199 \t| test loss: 0.13442\t | time taken: 0.009286165237426758\n",
      "batch_size: 128 \t| epoch: 69 \t| train loss: 0.11364 \t| test loss: 0.13248\t | time taken: 0.009386062622070312\n",
      "batch_size: 128 \t| epoch: 70 \t| train loss: 0.11082 \t| test loss: 0.13069\t | time taken: 0.009515047073364258\n",
      "batch_size: 128 \t| epoch: 71 \t| train loss: 0.10787 \t| test loss: 0.12872\t | time taken: 0.009382009506225586\n",
      "batch_size: 128 \t| epoch: 72 \t| train loss: 0.09982 \t| test loss: 0.12672\t | time taken: 0.009350061416625977\n",
      "batch_size: 128 \t| epoch: 73 \t| train loss: 0.10070 \t| test loss: 0.12498\t | time taken: 0.009443998336791992\n",
      "batch_size: 128 \t| epoch: 74 \t| train loss: 0.10334 \t| test loss: 0.12310\t | time taken: 0.00944209098815918\n",
      "batch_size: 128 \t| epoch: 75 \t| train loss: 0.10575 \t| test loss: 0.12121\t | time taken: 0.009483814239501953\n",
      "batch_size: 128 \t| epoch: 76 \t| train loss: 0.09300 \t| test loss: 0.11940\t | time taken: 0.009158134460449219\n",
      "batch_size: 128 \t| epoch: 77 \t| train loss: 0.10015 \t| test loss: 0.11787\t | time taken: 0.009233713150024414\n",
      "batch_size: 128 \t| epoch: 78 \t| train loss: 0.09459 \t| test loss: 0.11641\t | time taken: 0.009448766708374023\n",
      "batch_size: 128 \t| epoch: 79 \t| train loss: 0.09384 \t| test loss: 0.11500\t | time taken: 0.009280920028686523\n",
      "batch_size: 128 \t| epoch: 80 \t| train loss: 0.08783 \t| test loss: 0.11333\t | time taken: 0.009371042251586914\n",
      "batch_size: 128 \t| epoch: 81 \t| train loss: 0.09289 \t| test loss: 0.11170\t | time taken: 0.009377002716064453\n",
      "batch_size: 128 \t| epoch: 82 \t| train loss: 0.08802 \t| test loss: 0.11015\t | time taken: 0.009559869766235352\n",
      "batch_size: 128 \t| epoch: 83 \t| train loss: 0.08774 \t| test loss: 0.10850\t | time taken: 0.009373903274536133\n",
      "batch_size: 128 \t| epoch: 84 \t| train loss: 0.08365 \t| test loss: 0.10663\t | time taken: 0.009235858917236328\n",
      "batch_size: 128 \t| epoch: 85 \t| train loss: 0.08320 \t| test loss: 0.10507\t | time taken: 0.009767293930053711\n",
      "batch_size: 128 \t| epoch: 86 \t| train loss: 0.07878 \t| test loss: 0.10362\t | time taken: 0.010039806365966797\n",
      "batch_size: 128 \t| epoch: 87 \t| train loss: 0.08004 \t| test loss: 0.10216\t | time taken: 0.009272098541259766\n",
      "batch_size: 128 \t| epoch: 88 \t| train loss: 0.07544 \t| test loss: 0.10073\t | time taken: 0.009504079818725586\n",
      "batch_size: 128 \t| epoch: 89 \t| train loss: 0.07371 \t| test loss: 0.09949\t | time taken: 0.009256839752197266\n",
      "batch_size: 128 \t| epoch: 90 \t| train loss: 0.07768 \t| test loss: 0.09826\t | time taken: 0.009322881698608398\n",
      "batch_size: 128 \t| epoch: 91 \t| train loss: 0.07996 \t| test loss: 0.09700\t | time taken: 0.009425163269042969\n",
      "batch_size: 128 \t| epoch: 92 \t| train loss: 0.07484 \t| test loss: 0.09567\t | time taken: 0.00953817367553711\n",
      "batch_size: 128 \t| epoch: 93 \t| train loss: 0.07307 \t| test loss: 0.09433\t | time taken: 0.009376049041748047\n",
      "batch_size: 128 \t| epoch: 94 \t| train loss: 0.07165 \t| test loss: 0.09296\t | time taken: 0.00974273681640625\n",
      "batch_size: 128 \t| epoch: 95 \t| train loss: 0.07157 \t| test loss: 0.09181\t | time taken: 0.00931096076965332\n",
      "batch_size: 128 \t| epoch: 96 \t| train loss: 0.06684 \t| test loss: 0.09081\t | time taken: 0.009299993515014648\n",
      "batch_size: 128 \t| epoch: 97 \t| train loss: 0.07067 \t| test loss: 0.08949\t | time taken: 0.009341955184936523\n",
      "batch_size: 128 \t| epoch: 98 \t| train loss: 0.06369 \t| test loss: 0.08823\t | time taken: 0.00929117202758789\n",
      "batch_size: 128 \t| epoch: 99 \t| train loss: 0.06164 \t| test loss: 0.08681\t | time taken: 0.009418964385986328\n",
      "reach max number of epoch\n",
      "batch_size: 128 \t| epoch: 0 \t| train loss: 0.07073 \t| test loss: 0.08515\t | time taken: 0.009796142578125\n",
      "batch_size: 128 \t| epoch: 1 \t| train loss: 0.06940 \t| test loss: 0.08340\t | time taken: 0.009366989135742188\n",
      "batch_size: 128 \t| epoch: 2 \t| train loss: 0.07202 \t| test loss: 0.08182\t | time taken: 0.009454727172851562\n",
      "batch_size: 128 \t| epoch: 3 \t| train loss: 0.06573 \t| test loss: 0.08008\t | time taken: 0.009248971939086914\n",
      "batch_size: 128 \t| epoch: 4 \t| train loss: 0.06528 \t| test loss: 0.07830\t | time taken: 0.009354829788208008\n",
      "batch_size: 128 \t| epoch: 5 \t| train loss: 0.06805 \t| test loss: 0.07655\t | time taken: 0.00952601432800293\n",
      "batch_size: 128 \t| epoch: 6 \t| train loss: 0.06053 \t| test loss: 0.07498\t | time taken: 0.009372949600219727\n",
      "batch_size: 128 \t| epoch: 7 \t| train loss: 0.06771 \t| test loss: 0.07354\t | time taken: 0.009840011596679688\n",
      "batch_size: 128 \t| epoch: 8 \t| train loss: 0.06280 \t| test loss: 0.07205\t | time taken: 0.009208917617797852\n",
      "batch_size: 128 \t| epoch: 9 \t| train loss: 0.06558 \t| test loss: 0.07084\t | time taken: 0.00943303108215332\n",
      "batch_size: 128 \t| epoch: 10 \t| train loss: 0.05683 \t| test loss: 0.06961\t | time taken: 0.009285926818847656\n",
      "batch_size: 128 \t| epoch: 11 \t| train loss: 0.05762 \t| test loss: 0.06851\t | time taken: 0.009253978729248047\n",
      "batch_size: 128 \t| epoch: 12 \t| train loss: 0.05879 \t| test loss: 0.06736\t | time taken: 0.009312868118286133\n",
      "batch_size: 128 \t| epoch: 13 \t| train loss: 0.05788 \t| test loss: 0.06592\t | time taken: 0.009337902069091797\n",
      "batch_size: 128 \t| epoch: 14 \t| train loss: 0.05553 \t| test loss: 0.06459\t | time taken: 0.009142160415649414\n",
      "batch_size: 128 \t| epoch: 15 \t| train loss: 0.05573 \t| test loss: 0.06345\t | time taken: 0.009371280670166016\n",
      "batch_size: 128 \t| epoch: 16 \t| train loss: 0.05785 \t| test loss: 0.06231\t | time taken: 0.009304046630859375\n",
      "batch_size: 128 \t| epoch: 17 \t| train loss: 0.05728 \t| test loss: 0.06125\t | time taken: 0.009300947189331055\n",
      "batch_size: 128 \t| epoch: 18 \t| train loss: 0.04940 \t| test loss: 0.06049\t | time taken: 0.009297847747802734\n",
      "batch_size: 128 \t| epoch: 19 \t| train loss: 0.04908 \t| test loss: 0.05938\t | time taken: 0.009337186813354492\n",
      "batch_size: 128 \t| epoch: 20 \t| train loss: 0.04982 \t| test loss: 0.05850\t | time taken: 0.009432792663574219\n",
      "batch_size: 128 \t| epoch: 21 \t| train loss: 0.04627 \t| test loss: 0.05762\t | time taken: 0.00923609733581543\n",
      "batch_size: 128 \t| epoch: 22 \t| train loss: 0.04295 \t| test loss: 0.05652\t | time taken: 0.009596824645996094\n",
      "batch_size: 128 \t| epoch: 23 \t| train loss: 0.04633 \t| test loss: 0.05568\t | time taken: 0.00927281379699707\n",
      "batch_size: 128 \t| epoch: 24 \t| train loss: 0.04416 \t| test loss: 0.05478\t | time taken: 0.009217023849487305\n",
      "batch_size: 128 \t| epoch: 25 \t| train loss: 0.04322 \t| test loss: 0.05397\t | time taken: 0.009390115737915039\n",
      "batch_size: 128 \t| epoch: 26 \t| train loss: 0.05142 \t| test loss: 0.05349\t | time taken: 0.009437084197998047\n",
      "batch_size: 128 \t| epoch: 27 \t| train loss: 0.04241 \t| test loss: 0.05217\t | time taken: 0.009532928466796875\n",
      "batch_size: 128 \t| epoch: 28 \t| train loss: 0.04357 \t| test loss: 0.05120\t | time taken: 0.009577035903930664\n",
      "batch_size: 128 \t| epoch: 29 \t| train loss: 0.04837 \t| test loss: 0.05044\t | time taken: 0.009285926818847656\n",
      "batch_size: 128 \t| epoch: 30 \t| train loss: 0.04093 \t| test loss: 0.04974\t | time taken: 0.009393930435180664\n",
      "batch_size: 128 \t| epoch: 31 \t| train loss: 0.04159 \t| test loss: 0.04881\t | time taken: 0.009525060653686523\n",
      "batch_size: 128 \t| epoch: 32 \t| train loss: 0.03404 \t| test loss: 0.04802\t | time taken: 0.009438037872314453\n",
      "batch_size: 128 \t| epoch: 33 \t| train loss: 0.03704 \t| test loss: 0.04739\t | time taken: 0.00939488410949707\n",
      "batch_size: 128 \t| epoch: 34 \t| train loss: 0.03594 \t| test loss: 0.04670\t | time taken: 0.009193897247314453\n",
      "batch_size: 128 \t| epoch: 35 \t| train loss: 0.04149 \t| test loss: 0.04627\t | time taken: 0.009380817413330078\n",
      "batch_size: 128 \t| epoch: 36 \t| train loss: 0.03511 \t| test loss: 0.04564\t | time taken: 0.009123086929321289\n",
      "batch_size: 128 \t| epoch: 37 \t| train loss: 0.03975 \t| test loss: 0.04534\t | time taken: 0.009352922439575195\n",
      "batch_size: 128 \t| epoch: 38 \t| train loss: 0.03638 \t| test loss: 0.04457\t | time taken: 0.009194135665893555\n",
      "batch_size: 128 \t| epoch: 39 \t| train loss: 0.03112 \t| test loss: 0.04362\t | time taken: 0.009299039840698242\n",
      "batch_size: 128 \t| epoch: 40 \t| train loss: 0.04121 \t| test loss: 0.04315\t | time taken: 0.009137868881225586\n",
      "batch_size: 128 \t| epoch: 41 \t| train loss: 0.03647 \t| test loss: 0.04252\t | time taken: 0.009248018264770508\n",
      "batch_size: 128 \t| epoch: 42 \t| train loss: 0.03269 \t| test loss: 0.04187\t | time taken: 0.009298086166381836\n",
      "batch_size: 128 \t| epoch: 43 \t| train loss: 0.02969 \t| test loss: 0.04119\t | time taken: 0.00939488410949707\n",
      "batch_size: 128 \t| epoch: 44 \t| train loss: 0.03475 \t| test loss: 0.04058\t | time taken: 0.009395837783813477\n",
      "batch_size: 128 \t| epoch: 45 \t| train loss: 0.03587 \t| test loss: 0.04008\t | time taken: 0.009508848190307617\n",
      "batch_size: 128 \t| epoch: 46 \t| train loss: 0.02891 \t| test loss: 0.03971\t | time taken: 0.009614229202270508\n",
      "batch_size: 128 \t| epoch: 47 \t| train loss: 0.02985 \t| test loss: 0.03944\t | time taken: 0.009300947189331055\n",
      "batch_size: 128 \t| epoch: 48 \t| train loss: 0.03073 \t| test loss: 0.03921\t | time taken: 0.009423971176147461\n",
      "batch_size: 128 \t| epoch: 49 \t| train loss: 0.02567 \t| test loss: 0.03890\t | time taken: 0.009665966033935547\n",
      "batch_size: 128 \t| epoch: 50 \t| train loss: 0.03060 \t| test loss: 0.03863\t | time taken: 0.009370803833007812\n",
      "batch_size: 128 \t| epoch: 51 \t| train loss: 0.02258 \t| test loss: 0.03810\t | time taken: 0.009346246719360352\n",
      "batch_size: 128 \t| epoch: 52 \t| train loss: 0.03440 \t| test loss: 0.03754\t | time taken: 0.009253978729248047\n",
      "batch_size: 128 \t| epoch: 53 \t| train loss: 0.03324 \t| test loss: 0.03697\t | time taken: 0.009378671646118164\n",
      "batch_size: 128 \t| epoch: 54 \t| train loss: 0.02577 \t| test loss: 0.03661\t | time taken: 0.009370803833007812\n",
      "batch_size: 128 \t| epoch: 55 \t| train loss: 0.02527 \t| test loss: 0.03632\t | time taken: 0.009305000305175781\n",
      "batch_size: 128 \t| epoch: 56 \t| train loss: 0.03006 \t| test loss: 0.03567\t | time taken: 0.009289264678955078\n",
      "batch_size: 128 \t| epoch: 57 \t| train loss: 0.02992 \t| test loss: 0.03499\t | time taken: 0.009195089340209961\n",
      "batch_size: 128 \t| epoch: 58 \t| train loss: 0.02590 \t| test loss: 0.03458\t | time taken: 0.009397029876708984\n",
      "batch_size: 128 \t| epoch: 59 \t| train loss: 0.03155 \t| test loss: 0.03438\t | time taken: 0.009189128875732422\n",
      "batch_size: 128 \t| epoch: 60 \t| train loss: 0.02737 \t| test loss: 0.03367\t | time taken: 0.009212970733642578\n",
      "batch_size: 128 \t| epoch: 61 \t| train loss: 0.02951 \t| test loss: 0.03324\t | time taken: 0.009210824966430664\n",
      "batch_size: 128 \t| epoch: 62 \t| train loss: 0.02870 \t| test loss: 0.03296\t | time taken: 0.009189128875732422\n",
      "batch_size: 128 \t| epoch: 63 \t| train loss: 0.02468 \t| test loss: 0.03256\t | time taken: 0.00958704948425293\n",
      "batch_size: 128 \t| epoch: 64 \t| train loss: 0.02628 \t| test loss: 0.03245\t | time taken: 0.009059906005859375\n",
      "batch_size: 128 \t| epoch: 65 \t| train loss: 0.03187 \t| test loss: 0.03223\t | time taken: 0.009672880172729492\n",
      "batch_size: 128 \t| epoch: 66 \t| train loss: 0.02270 \t| test loss: 0.03186\t | time taken: 0.009385824203491211\n",
      "batch_size: 128 \t| epoch: 67 \t| train loss: 0.02778 \t| test loss: 0.03142\t | time taken: 0.01022195816040039\n",
      "batch_size: 128 \t| epoch: 68 \t| train loss: 0.02266 \t| test loss: 0.03109\t | time taken: 0.009348869323730469\n",
      "batch_size: 128 \t| epoch: 69 \t| train loss: 0.02637 \t| test loss: 0.03083\t | time taken: 0.009268999099731445\n",
      "batch_size: 128 \t| epoch: 70 \t| train loss: 0.02167 \t| test loss: 0.03012\t | time taken: 0.009252786636352539\n",
      "batch_size: 128 \t| epoch: 71 \t| train loss: 0.01843 \t| test loss: 0.02971\t | time taken: 0.009285926818847656\n",
      "batch_size: 128 \t| epoch: 72 \t| train loss: 0.02264 \t| test loss: 0.02976\t | time taken: 0.00932002067565918\n",
      "batch_size: 128 \t| epoch: 73 \t| train loss: 0.02422 \t| test loss: 0.02961\t | time taken: 0.00948190689086914\n",
      "batch_size: 128 \t| epoch: 74 \t| train loss: 0.02192 \t| test loss: 0.02954\t | time taken: 0.009654045104980469\n",
      "batch_size: 128 \t| epoch: 75 \t| train loss: 0.02398 \t| test loss: 0.02930\t | time taken: 0.009617328643798828\n",
      "batch_size: 128 \t| epoch: 76 \t| train loss: 0.02178 \t| test loss: 0.02899\t | time taken: 0.00913095474243164\n",
      "batch_size: 128 \t| epoch: 77 \t| train loss: 0.01981 \t| test loss: 0.02899\t | time taken: 0.00951695442199707\n",
      "batch_size: 128 \t| epoch: 78 \t| train loss: 0.01707 \t| test loss: 0.02869\t | time taken: 0.009180068969726562\n",
      "batch_size: 128 \t| epoch: 79 \t| train loss: 0.01948 \t| test loss: 0.02849\t | time taken: 0.008990764617919922\n",
      "batch_size: 128 \t| epoch: 80 \t| train loss: 0.02031 \t| test loss: 0.02792\t | time taken: 0.009338140487670898\n",
      "batch_size: 128 \t| epoch: 81 \t| train loss: 0.02311 \t| test loss: 0.02789\t | time taken: 0.009301185607910156\n",
      "batch_size: 128 \t| epoch: 82 \t| train loss: 0.02178 \t| test loss: 0.02803\t | time taken: 0.009110689163208008\n",
      "batch_size: 128 \t| epoch: 83 \t| train loss: 0.02037 \t| test loss: 0.02817\t | time taken: 0.009550809860229492\n",
      "batch_size: 128 \t| epoch: 84 \t| train loss: 0.01893 \t| test loss: 0.02815\t | time taken: 0.009299039840698242\n",
      "early stopping at epoch 84\n",
      "batch_size: 128 \t| epoch: 0 \t| train loss: 0.02958 \t| test loss: 0.02713\t | time taken: 0.009637832641601562\n",
      "batch_size: 128 \t| epoch: 1 \t| train loss: 0.03402 \t| test loss: 0.02529\t | time taken: 0.010100841522216797\n",
      "batch_size: 128 \t| epoch: 2 \t| train loss: 0.02860 \t| test loss: 0.02407\t | time taken: 0.009254217147827148\n",
      "batch_size: 128 \t| epoch: 3 \t| train loss: 0.03165 \t| test loss: 0.02337\t | time taken: 0.00938105583190918\n",
      "batch_size: 128 \t| epoch: 4 \t| train loss: 0.03012 \t| test loss: 0.02248\t | time taken: 0.009200811386108398\n",
      "batch_size: 128 \t| epoch: 5 \t| train loss: 0.02073 \t| test loss: 0.02173\t | time taken: 0.009081840515136719\n",
      "batch_size: 128 \t| epoch: 6 \t| train loss: 0.03335 \t| test loss: 0.02093\t | time taken: 0.00923299789428711\n",
      "batch_size: 128 \t| epoch: 7 \t| train loss: 0.02099 \t| test loss: 0.02012\t | time taken: 0.009337902069091797\n",
      "batch_size: 128 \t| epoch: 8 \t| train loss: 0.02213 \t| test loss: 0.01917\t | time taken: 0.009579181671142578\n",
      "batch_size: 128 \t| epoch: 9 \t| train loss: 0.02309 \t| test loss: 0.01843\t | time taken: 0.009358882904052734\n",
      "batch_size: 128 \t| epoch: 10 \t| train loss: 0.02058 \t| test loss: 0.01807\t | time taken: 0.00946807861328125\n",
      "batch_size: 128 \t| epoch: 11 \t| train loss: 0.01900 \t| test loss: 0.01773\t | time taken: 0.00934290885925293\n",
      "batch_size: 128 \t| epoch: 12 \t| train loss: 0.02019 \t| test loss: 0.01775\t | time taken: 0.009258031845092773\n",
      "batch_size: 128 \t| epoch: 13 \t| train loss: 0.02518 \t| test loss: 0.01733\t | time taken: 0.009279966354370117\n",
      "batch_size: 128 \t| epoch: 14 \t| train loss: 0.02483 \t| test loss: 0.01652\t | time taken: 0.009443044662475586\n",
      "batch_size: 128 \t| epoch: 15 \t| train loss: 0.02287 \t| test loss: 0.01592\t | time taken: 0.009372949600219727\n",
      "batch_size: 128 \t| epoch: 16 \t| train loss: 0.02244 \t| test loss: 0.01564\t | time taken: 0.009369134902954102\n",
      "batch_size: 128 \t| epoch: 17 \t| train loss: 0.01812 \t| test loss: 0.01520\t | time taken: 0.009459257125854492\n",
      "batch_size: 128 \t| epoch: 18 \t| train loss: 0.02111 \t| test loss: 0.01455\t | time taken: 0.00940394401550293\n",
      "batch_size: 128 \t| epoch: 19 \t| train loss: 0.02173 \t| test loss: 0.01419\t | time taken: 0.009306907653808594\n",
      "batch_size: 128 \t| epoch: 20 \t| train loss: 0.02001 \t| test loss: 0.01400\t | time taken: 0.009469270706176758\n",
      "batch_size: 128 \t| epoch: 21 \t| train loss: 0.01791 \t| test loss: 0.01363\t | time taken: 0.009387016296386719\n",
      "batch_size: 128 \t| epoch: 22 \t| train loss: 0.01864 \t| test loss: 0.01333\t | time taken: 0.009459972381591797\n",
      "batch_size: 128 \t| epoch: 23 \t| train loss: 0.01737 \t| test loss: 0.01299\t | time taken: 0.009653091430664062\n",
      "batch_size: 128 \t| epoch: 24 \t| train loss: 0.01826 \t| test loss: 0.01311\t | time taken: 0.009632110595703125\n",
      "batch_size: 128 \t| epoch: 25 \t| train loss: 0.01945 \t| test loss: 0.01278\t | time taken: 0.009497880935668945\n",
      "batch_size: 128 \t| epoch: 26 \t| train loss: 0.02172 \t| test loss: 0.01228\t | time taken: 0.009445905685424805\n",
      "batch_size: 128 \t| epoch: 27 \t| train loss: 0.02285 \t| test loss: 0.01183\t | time taken: 0.009510040283203125\n",
      "batch_size: 128 \t| epoch: 28 \t| train loss: 0.01993 \t| test loss: 0.01149\t | time taken: 0.009324073791503906\n",
      "batch_size: 128 \t| epoch: 29 \t| train loss: 0.01573 \t| test loss: 0.01130\t | time taken: 0.009436845779418945\n",
      "batch_size: 128 \t| epoch: 30 \t| train loss: 0.01415 \t| test loss: 0.01108\t | time taken: 0.009371042251586914\n",
      "batch_size: 128 \t| epoch: 31 \t| train loss: 0.01640 \t| test loss: 0.01094\t | time taken: 0.00925898551940918\n",
      "batch_size: 128 \t| epoch: 32 \t| train loss: 0.01808 \t| test loss: 0.01076\t | time taken: 0.009196996688842773\n",
      "batch_size: 128 \t| epoch: 33 \t| train loss: 0.01857 \t| test loss: 0.01050\t | time taken: 0.009409189224243164\n",
      "batch_size: 128 \t| epoch: 34 \t| train loss: 0.01828 \t| test loss: 0.01000\t | time taken: 0.00945901870727539\n",
      "batch_size: 128 \t| epoch: 35 \t| train loss: 0.02029 \t| test loss: 0.00961\t | time taken: 0.009335994720458984\n",
      "batch_size: 128 \t| epoch: 36 \t| train loss: 0.01888 \t| test loss: 0.00944\t | time taken: 0.009296894073486328\n",
      "batch_size: 128 \t| epoch: 37 \t| train loss: 0.02136 \t| test loss: 0.00979\t | time taken: 0.009528160095214844\n",
      "batch_size: 128 \t| epoch: 38 \t| train loss: 0.01491 \t| test loss: 0.00980\t | time taken: 0.009384393692016602\n",
      "batch_size: 128 \t| epoch: 39 \t| train loss: 0.01928 \t| test loss: 0.00953\t | time taken: 0.009438037872314453\n",
      "early stopping at epoch 39\n",
      "batch_size: 128 \t| epoch: 0 \t| train loss: 0.01526 \t| test loss: 0.00920\t | time taken: 0.009703874588012695\n",
      "batch_size: 128 \t| epoch: 1 \t| train loss: 0.01509 \t| test loss: 0.00881\t | time taken: 0.009699106216430664\n",
      "batch_size: 128 \t| epoch: 2 \t| train loss: 0.02014 \t| test loss: 0.00895\t | time taken: 0.009579181671142578\n",
      "batch_size: 128 \t| epoch: 3 \t| train loss: 0.02024 \t| test loss: 0.00871\t | time taken: 0.00940704345703125\n",
      "batch_size: 128 \t| epoch: 4 \t| train loss: 0.01790 \t| test loss: 0.00816\t | time taken: 0.009217023849487305\n",
      "batch_size: 128 \t| epoch: 5 \t| train loss: 0.01148 \t| test loss: 0.00812\t | time taken: 0.009338140487670898\n",
      "batch_size: 128 \t| epoch: 6 \t| train loss: 0.01773 \t| test loss: 0.00831\t | time taken: 0.009216070175170898\n",
      "batch_size: 128 \t| epoch: 7 \t| train loss: 0.01532 \t| test loss: 0.00785\t | time taken: 0.009695053100585938\n",
      "batch_size: 128 \t| epoch: 8 \t| train loss: 0.01475 \t| test loss: 0.00766\t | time taken: 0.009308815002441406\n",
      "batch_size: 128 \t| epoch: 9 \t| train loss: 0.01291 \t| test loss: 0.00759\t | time taken: 0.009205102920532227\n",
      "batch_size: 128 \t| epoch: 10 \t| train loss: 0.01816 \t| test loss: 0.00761\t | time taken: 0.009316205978393555\n",
      "batch_size: 128 \t| epoch: 11 \t| train loss: 0.01419 \t| test loss: 0.00741\t | time taken: 0.009192943572998047\n",
      "batch_size: 128 \t| epoch: 12 \t| train loss: 0.01186 \t| test loss: 0.00727\t | time taken: 0.009160041809082031\n",
      "batch_size: 128 \t| epoch: 13 \t| train loss: 0.01951 \t| test loss: 0.00730\t | time taken: 0.009422779083251953\n",
      "batch_size: 128 \t| epoch: 14 \t| train loss: 0.01524 \t| test loss: 0.00707\t | time taken: 0.009248018264770508\n",
      "batch_size: 128 \t| epoch: 15 \t| train loss: 0.01663 \t| test loss: 0.00705\t | time taken: 0.00878000259399414\n",
      "batch_size: 128 \t| epoch: 16 \t| train loss: 0.01307 \t| test loss: 0.00664\t | time taken: 0.00914907455444336\n",
      "batch_size: 128 \t| epoch: 17 \t| train loss: 0.01265 \t| test loss: 0.00657\t | time taken: 0.009315729141235352\n",
      "batch_size: 128 \t| epoch: 18 \t| train loss: 0.01398 \t| test loss: 0.00642\t | time taken: 0.00937795639038086\n",
      "batch_size: 128 \t| epoch: 19 \t| train loss: 0.00993 \t| test loss: 0.00582\t | time taken: 0.009288787841796875\n",
      "batch_size: 128 \t| epoch: 20 \t| train loss: 0.01694 \t| test loss: 0.00543\t | time taken: 0.012587785720825195\n",
      "batch_size: 128 \t| epoch: 21 \t| train loss: 0.01852 \t| test loss: 0.00527\t | time taken: 0.010204792022705078\n",
      "batch_size: 128 \t| epoch: 22 \t| train loss: 0.01397 \t| test loss: 0.00516\t | time taken: 0.009217023849487305\n",
      "batch_size: 128 \t| epoch: 23 \t| train loss: 0.01291 \t| test loss: 0.00499\t | time taken: 0.009592771530151367\n",
      "batch_size: 128 \t| epoch: 24 \t| train loss: 0.01698 \t| test loss: 0.00500\t | time taken: 0.009509086608886719\n",
      "batch_size: 128 \t| epoch: 25 \t| train loss: 0.01313 \t| test loss: 0.00532\t | time taken: 0.010736942291259766\n",
      "batch_size: 128 \t| epoch: 26 \t| train loss: 0.01224 \t| test loss: 0.00551\t | time taken: 0.03572416305541992\n",
      "early stopping at epoch 26\n",
      "batch_size: 128 \t| epoch: 0 \t| train loss: 0.01538 \t| test loss: 0.00576\t | time taken: 0.030987977981567383\n",
      "batch_size: 128 \t| epoch: 1 \t| train loss: 0.01039 \t| test loss: 0.00541\t | time taken: 0.00963282585144043\n",
      "batch_size: 128 \t| epoch: 2 \t| train loss: 0.01143 \t| test loss: 0.00515\t | time taken: 0.009482860565185547\n",
      "batch_size: 128 \t| epoch: 3 \t| train loss: 0.01375 \t| test loss: 0.00463\t | time taken: 0.009202241897583008\n",
      "batch_size: 128 \t| epoch: 4 \t| train loss: 0.01230 \t| test loss: 0.00430\t | time taken: 0.010201215744018555\n",
      "batch_size: 128 \t| epoch: 5 \t| train loss: 0.01207 \t| test loss: 0.00441\t | time taken: 0.009984016418457031\n",
      "batch_size: 128 \t| epoch: 6 \t| train loss: 0.00945 \t| test loss: 0.00441\t | time taken: 0.047290802001953125\n",
      "batch_size: 128 \t| epoch: 7 \t| train loss: 0.00970 \t| test loss: 0.00434\t | time taken: 0.009239912033081055\n",
      "early stopping at epoch 7\n",
      "batch_size: 256 \t| epoch: 0 \t| train loss: 0.69438 \t| test loss: 0.68731\t | time taken: 0.10064077377319336\n",
      "batch_size: 256 \t| epoch: 1 \t| train loss: 0.68643 \t| test loss: 0.68268\t | time taken: 0.007071971893310547\n",
      "batch_size: 256 \t| epoch: 2 \t| train loss: 0.68230 \t| test loss: 0.67809\t | time taken: 0.006587028503417969\n",
      "batch_size: 256 \t| epoch: 3 \t| train loss: 0.67747 \t| test loss: 0.67342\t | time taken: 0.0068209171295166016\n",
      "batch_size: 256 \t| epoch: 4 \t| train loss: 0.67339 \t| test loss: 0.66853\t | time taken: 0.006490945816040039\n",
      "batch_size: 256 \t| epoch: 5 \t| train loss: 0.66805 \t| test loss: 0.66342\t | time taken: 0.006521940231323242\n",
      "batch_size: 256 \t| epoch: 6 \t| train loss: 0.66309 \t| test loss: 0.65797\t | time taken: 0.0064220428466796875\n",
      "batch_size: 256 \t| epoch: 7 \t| train loss: 0.65831 \t| test loss: 0.65203\t | time taken: 0.00675511360168457\n",
      "batch_size: 256 \t| epoch: 8 \t| train loss: 0.65161 \t| test loss: 0.64548\t | time taken: 0.006638288497924805\n",
      "batch_size: 256 \t| epoch: 9 \t| train loss: 0.64687 \t| test loss: 0.63826\t | time taken: 0.0064852237701416016\n",
      "batch_size: 256 \t| epoch: 10 \t| train loss: 0.63692 \t| test loss: 0.63030\t | time taken: 0.0066988468170166016\n",
      "batch_size: 256 \t| epoch: 11 \t| train loss: 0.63033 \t| test loss: 0.62151\t | time taken: 0.006648063659667969\n",
      "batch_size: 256 \t| epoch: 12 \t| train loss: 0.62317 \t| test loss: 0.61191\t | time taken: 0.0067539215087890625\n",
      "batch_size: 256 \t| epoch: 13 \t| train loss: 0.61057 \t| test loss: 0.60140\t | time taken: 0.006927013397216797\n",
      "batch_size: 256 \t| epoch: 14 \t| train loss: 0.60191 \t| test loss: 0.58999\t | time taken: 0.006676197052001953\n",
      "batch_size: 256 \t| epoch: 15 \t| train loss: 0.58932 \t| test loss: 0.57782\t | time taken: 0.007211923599243164\n",
      "batch_size: 256 \t| epoch: 16 \t| train loss: 0.57780 \t| test loss: 0.56488\t | time taken: 0.00650787353515625\n",
      "batch_size: 256 \t| epoch: 17 \t| train loss: 0.56392 \t| test loss: 0.55130\t | time taken: 0.007294893264770508\n",
      "batch_size: 256 \t| epoch: 18 \t| train loss: 0.55318 \t| test loss: 0.53724\t | time taken: 0.006803989410400391\n",
      "batch_size: 256 \t| epoch: 19 \t| train loss: 0.53675 \t| test loss: 0.52285\t | time taken: 0.006694793701171875\n",
      "batch_size: 256 \t| epoch: 20 \t| train loss: 0.52463 \t| test loss: 0.50834\t | time taken: 0.006797075271606445\n",
      "batch_size: 256 \t| epoch: 21 \t| train loss: 0.51001 \t| test loss: 0.49383\t | time taken: 0.0066928863525390625\n",
      "batch_size: 256 \t| epoch: 22 \t| train loss: 0.49374 \t| test loss: 0.47949\t | time taken: 0.006592988967895508\n",
      "batch_size: 256 \t| epoch: 23 \t| train loss: 0.47969 \t| test loss: 0.46551\t | time taken: 0.006665945053100586\n",
      "batch_size: 256 \t| epoch: 24 \t| train loss: 0.46565 \t| test loss: 0.45206\t | time taken: 0.0066072940826416016\n",
      "batch_size: 256 \t| epoch: 25 \t| train loss: 0.45285 \t| test loss: 0.43916\t | time taken: 0.0065538883209228516\n",
      "batch_size: 256 \t| epoch: 26 \t| train loss: 0.43880 \t| test loss: 0.42700\t | time taken: 0.006557941436767578\n",
      "batch_size: 256 \t| epoch: 27 \t| train loss: 0.42558 \t| test loss: 0.41564\t | time taken: 0.006457090377807617\n",
      "batch_size: 256 \t| epoch: 28 \t| train loss: 0.41346 \t| test loss: 0.40500\t | time taken: 0.0066030025482177734\n",
      "batch_size: 256 \t| epoch: 29 \t| train loss: 0.40375 \t| test loss: 0.39516\t | time taken: 0.006620883941650391\n",
      "batch_size: 256 \t| epoch: 30 \t| train loss: 0.39506 \t| test loss: 0.38601\t | time taken: 0.007413148880004883\n",
      "batch_size: 256 \t| epoch: 31 \t| train loss: 0.38607 \t| test loss: 0.37747\t | time taken: 0.006853818893432617\n",
      "batch_size: 256 \t| epoch: 32 \t| train loss: 0.37901 \t| test loss: 0.36955\t | time taken: 0.0067348480224609375\n",
      "batch_size: 256 \t| epoch: 33 \t| train loss: 0.37007 \t| test loss: 0.36220\t | time taken: 0.00646209716796875\n",
      "batch_size: 256 \t| epoch: 34 \t| train loss: 0.36253 \t| test loss: 0.35527\t | time taken: 0.006601095199584961\n",
      "batch_size: 256 \t| epoch: 35 \t| train loss: 0.35676 \t| test loss: 0.34866\t | time taken: 0.006680965423583984\n",
      "batch_size: 256 \t| epoch: 36 \t| train loss: 0.34738 \t| test loss: 0.34237\t | time taken: 0.006509065628051758\n",
      "batch_size: 256 \t| epoch: 37 \t| train loss: 0.34276 \t| test loss: 0.33629\t | time taken: 0.006711006164550781\n",
      "batch_size: 256 \t| epoch: 38 \t| train loss: 0.33708 \t| test loss: 0.33037\t | time taken: 0.006745100021362305\n",
      "batch_size: 256 \t| epoch: 39 \t| train loss: 0.32858 \t| test loss: 0.32461\t | time taken: 0.006669759750366211\n",
      "batch_size: 256 \t| epoch: 40 \t| train loss: 0.32353 \t| test loss: 0.31901\t | time taken: 0.006464719772338867\n",
      "batch_size: 256 \t| epoch: 41 \t| train loss: 0.31625 \t| test loss: 0.31358\t | time taken: 0.00690007209777832\n",
      "batch_size: 256 \t| epoch: 42 \t| train loss: 0.31496 \t| test loss: 0.30831\t | time taken: 0.007028818130493164\n",
      "batch_size: 256 \t| epoch: 43 \t| train loss: 0.30711 \t| test loss: 0.30326\t | time taken: 0.00657200813293457\n",
      "batch_size: 256 \t| epoch: 44 \t| train loss: 0.30209 \t| test loss: 0.29830\t | time taken: 0.007258176803588867\n",
      "batch_size: 256 \t| epoch: 45 \t| train loss: 0.29683 \t| test loss: 0.29352\t | time taken: 0.006674051284790039\n",
      "batch_size: 256 \t| epoch: 46 \t| train loss: 0.29080 \t| test loss: 0.28882\t | time taken: 0.006751060485839844\n",
      "batch_size: 256 \t| epoch: 47 \t| train loss: 0.28654 \t| test loss: 0.28426\t | time taken: 0.0065920352935791016\n",
      "batch_size: 256 \t| epoch: 48 \t| train loss: 0.28689 \t| test loss: 0.27977\t | time taken: 0.0069239139556884766\n",
      "batch_size: 256 \t| epoch: 49 \t| train loss: 0.27849 \t| test loss: 0.27551\t | time taken: 0.006485939025878906\n",
      "batch_size: 256 \t| epoch: 50 \t| train loss: 0.26964 \t| test loss: 0.27131\t | time taken: 0.0068209171295166016\n",
      "batch_size: 256 \t| epoch: 51 \t| train loss: 0.26615 \t| test loss: 0.26722\t | time taken: 0.006510019302368164\n",
      "batch_size: 256 \t| epoch: 52 \t| train loss: 0.26135 \t| test loss: 0.26325\t | time taken: 0.006479978561401367\n",
      "batch_size: 256 \t| epoch: 53 \t| train loss: 0.25950 \t| test loss: 0.25935\t | time taken: 0.006822824478149414\n",
      "batch_size: 256 \t| epoch: 54 \t| train loss: 0.25700 \t| test loss: 0.25558\t | time taken: 0.00681304931640625\n",
      "batch_size: 256 \t| epoch: 55 \t| train loss: 0.25037 \t| test loss: 0.25193\t | time taken: 0.006657123565673828\n",
      "batch_size: 256 \t| epoch: 56 \t| train loss: 0.24450 \t| test loss: 0.24839\t | time taken: 0.006529092788696289\n",
      "batch_size: 256 \t| epoch: 57 \t| train loss: 0.24163 \t| test loss: 0.24494\t | time taken: 0.006734132766723633\n",
      "batch_size: 256 \t| epoch: 58 \t| train loss: 0.24176 \t| test loss: 0.24159\t | time taken: 0.0065441131591796875\n",
      "batch_size: 256 \t| epoch: 59 \t| train loss: 0.23395 \t| test loss: 0.23835\t | time taken: 0.006592988967895508\n",
      "batch_size: 256 \t| epoch: 60 \t| train loss: 0.22965 \t| test loss: 0.23514\t | time taken: 0.006518125534057617\n",
      "batch_size: 256 \t| epoch: 61 \t| train loss: 0.22346 \t| test loss: 0.23203\t | time taken: 0.006487131118774414\n",
      "batch_size: 256 \t| epoch: 62 \t| train loss: 0.22502 \t| test loss: 0.22900\t | time taken: 0.006627798080444336\n",
      "batch_size: 256 \t| epoch: 63 \t| train loss: 0.21973 \t| test loss: 0.22602\t | time taken: 0.006883859634399414\n",
      "batch_size: 256 \t| epoch: 64 \t| train loss: 0.21488 \t| test loss: 0.22325\t | time taken: 0.0068509578704833984\n",
      "batch_size: 256 \t| epoch: 65 \t| train loss: 0.21617 \t| test loss: 0.22061\t | time taken: 0.006634950637817383\n",
      "batch_size: 256 \t| epoch: 66 \t| train loss: 0.21306 \t| test loss: 0.21798\t | time taken: 0.006687164306640625\n",
      "batch_size: 256 \t| epoch: 67 \t| train loss: 0.20668 \t| test loss: 0.21539\t | time taken: 0.0065119266510009766\n",
      "batch_size: 256 \t| epoch: 68 \t| train loss: 0.20277 \t| test loss: 0.21285\t | time taken: 0.0064771175384521484\n",
      "batch_size: 256 \t| epoch: 69 \t| train loss: 0.20022 \t| test loss: 0.21040\t | time taken: 0.006763935089111328\n",
      "batch_size: 256 \t| epoch: 70 \t| train loss: 0.19606 \t| test loss: 0.20804\t | time taken: 0.006863117218017578\n",
      "batch_size: 256 \t| epoch: 71 \t| train loss: 0.19888 \t| test loss: 0.20571\t | time taken: 0.00716090202331543\n",
      "batch_size: 256 \t| epoch: 72 \t| train loss: 0.19479 \t| test loss: 0.20344\t | time taken: 0.006579875946044922\n",
      "batch_size: 256 \t| epoch: 73 \t| train loss: 0.18892 \t| test loss: 0.20117\t | time taken: 0.006791353225708008\n",
      "batch_size: 256 \t| epoch: 74 \t| train loss: 0.18570 \t| test loss: 0.19898\t | time taken: 0.006587982177734375\n",
      "batch_size: 256 \t| epoch: 75 \t| train loss: 0.18461 \t| test loss: 0.19685\t | time taken: 0.0067462921142578125\n",
      "batch_size: 256 \t| epoch: 76 \t| train loss: 0.18339 \t| test loss: 0.19477\t | time taken: 0.006758928298950195\n",
      "batch_size: 256 \t| epoch: 77 \t| train loss: 0.18253 \t| test loss: 0.19264\t | time taken: 0.006783962249755859\n",
      "batch_size: 256 \t| epoch: 78 \t| train loss: 0.17676 \t| test loss: 0.19057\t | time taken: 0.006669044494628906\n",
      "batch_size: 256 \t| epoch: 79 \t| train loss: 0.17531 \t| test loss: 0.18857\t | time taken: 0.006737232208251953\n",
      "batch_size: 256 \t| epoch: 80 \t| train loss: 0.17418 \t| test loss: 0.18661\t | time taken: 0.006652116775512695\n",
      "batch_size: 256 \t| epoch: 81 \t| train loss: 0.17341 \t| test loss: 0.18470\t | time taken: 0.00657200813293457\n",
      "batch_size: 256 \t| epoch: 82 \t| train loss: 0.16995 \t| test loss: 0.18283\t | time taken: 0.006762027740478516\n",
      "batch_size: 256 \t| epoch: 83 \t| train loss: 0.16914 \t| test loss: 0.18100\t | time taken: 0.006579399108886719\n",
      "batch_size: 256 \t| epoch: 84 \t| train loss: 0.16870 \t| test loss: 0.17920\t | time taken: 0.006807804107666016\n",
      "batch_size: 256 \t| epoch: 85 \t| train loss: 0.16390 \t| test loss: 0.17741\t | time taken: 0.0066030025482177734\n",
      "batch_size: 256 \t| epoch: 86 \t| train loss: 0.16206 \t| test loss: 0.17567\t | time taken: 0.006630897521972656\n",
      "batch_size: 256 \t| epoch: 87 \t| train loss: 0.15994 \t| test loss: 0.17398\t | time taken: 0.006381034851074219\n",
      "batch_size: 256 \t| epoch: 88 \t| train loss: 0.15658 \t| test loss: 0.17233\t | time taken: 0.006451129913330078\n",
      "batch_size: 256 \t| epoch: 89 \t| train loss: 0.15730 \t| test loss: 0.17072\t | time taken: 0.006740093231201172\n",
      "batch_size: 256 \t| epoch: 90 \t| train loss: 0.15367 \t| test loss: 0.16907\t | time taken: 0.006828784942626953\n",
      "batch_size: 256 \t| epoch: 91 \t| train loss: 0.15300 \t| test loss: 0.16739\t | time taken: 0.006720066070556641\n",
      "batch_size: 256 \t| epoch: 92 \t| train loss: 0.14896 \t| test loss: 0.16576\t | time taken: 0.006683826446533203\n",
      "batch_size: 256 \t| epoch: 93 \t| train loss: 0.15037 \t| test loss: 0.16416\t | time taken: 0.006706953048706055\n",
      "batch_size: 256 \t| epoch: 94 \t| train loss: 0.14723 \t| test loss: 0.16261\t | time taken: 0.00675201416015625\n",
      "batch_size: 256 \t| epoch: 95 \t| train loss: 0.14657 \t| test loss: 0.16111\t | time taken: 0.006611824035644531\n",
      "batch_size: 256 \t| epoch: 96 \t| train loss: 0.14435 \t| test loss: 0.15962\t | time taken: 0.006610870361328125\n",
      "batch_size: 256 \t| epoch: 97 \t| train loss: 0.14238 \t| test loss: 0.15818\t | time taken: 0.006899833679199219\n",
      "batch_size: 256 \t| epoch: 98 \t| train loss: 0.14226 \t| test loss: 0.15678\t | time taken: 0.03314208984375\n",
      "batch_size: 256 \t| epoch: 99 \t| train loss: 0.13807 \t| test loss: 0.15537\t | time taken: 0.03266620635986328\n",
      "reach max number of epoch\n",
      "batch_size: 256 \t| epoch: 0 \t| train loss: 0.13819 \t| test loss: 0.15393\t | time taken: 0.02683115005493164\n",
      "batch_size: 256 \t| epoch: 1 \t| train loss: 0.14272 \t| test loss: 0.15244\t | time taken: 0.014290094375610352\n",
      "batch_size: 256 \t| epoch: 2 \t| train loss: 0.13630 \t| test loss: 0.15096\t | time taken: 0.008022785186767578\n",
      "batch_size: 256 \t| epoch: 3 \t| train loss: 0.14062 \t| test loss: 0.14948\t | time taken: 0.006491184234619141\n",
      "batch_size: 256 \t| epoch: 4 \t| train loss: 0.13673 \t| test loss: 0.14799\t | time taken: 0.006631135940551758\n",
      "batch_size: 256 \t| epoch: 5 \t| train loss: 0.13704 \t| test loss: 0.14655\t | time taken: 0.006399869918823242\n",
      "batch_size: 256 \t| epoch: 6 \t| train loss: 0.13246 \t| test loss: 0.14517\t | time taken: 0.006407022476196289\n",
      "batch_size: 256 \t| epoch: 7 \t| train loss: 0.13405 \t| test loss: 0.14372\t | time taken: 0.006982088088989258\n",
      "batch_size: 256 \t| epoch: 8 \t| train loss: 0.13294 \t| test loss: 0.14228\t | time taken: 0.006451845169067383\n",
      "batch_size: 256 \t| epoch: 9 \t| train loss: 0.12447 \t| test loss: 0.14090\t | time taken: 0.006829023361206055\n",
      "batch_size: 256 \t| epoch: 10 \t| train loss: 0.12498 \t| test loss: 0.13952\t | time taken: 0.006661176681518555\n",
      "batch_size: 256 \t| epoch: 11 \t| train loss: 0.12637 \t| test loss: 0.13813\t | time taken: 0.006617069244384766\n",
      "batch_size: 256 \t| epoch: 12 \t| train loss: 0.12603 \t| test loss: 0.13668\t | time taken: 0.006452083587646484\n",
      "batch_size: 256 \t| epoch: 13 \t| train loss: 0.12248 \t| test loss: 0.13529\t | time taken: 0.006946086883544922\n",
      "batch_size: 256 \t| epoch: 14 \t| train loss: 0.12606 \t| test loss: 0.13397\t | time taken: 0.006494760513305664\n",
      "batch_size: 256 \t| epoch: 15 \t| train loss: 0.11648 \t| test loss: 0.13269\t | time taken: 0.007091999053955078\n",
      "batch_size: 256 \t| epoch: 16 \t| train loss: 0.11870 \t| test loss: 0.13140\t | time taken: 0.007000923156738281\n",
      "batch_size: 256 \t| epoch: 17 \t| train loss: 0.11430 \t| test loss: 0.13005\t | time taken: 0.00685429573059082\n",
      "batch_size: 256 \t| epoch: 18 \t| train loss: 0.11584 \t| test loss: 0.12873\t | time taken: 0.010495185852050781\n",
      "batch_size: 256 \t| epoch: 19 \t| train loss: 0.11548 \t| test loss: 0.12746\t | time taken: 0.006455898284912109\n",
      "batch_size: 256 \t| epoch: 20 \t| train loss: 0.11304 \t| test loss: 0.12622\t | time taken: 0.006853818893432617\n",
      "batch_size: 256 \t| epoch: 21 \t| train loss: 0.11857 \t| test loss: 0.12508\t | time taken: 0.006432056427001953\n",
      "batch_size: 256 \t| epoch: 22 \t| train loss: 0.10996 \t| test loss: 0.12389\t | time taken: 0.006701231002807617\n",
      "batch_size: 256 \t| epoch: 23 \t| train loss: 0.11409 \t| test loss: 0.12265\t | time taken: 0.0065348148345947266\n",
      "batch_size: 256 \t| epoch: 24 \t| train loss: 0.10528 \t| test loss: 0.12143\t | time taken: 0.006700038909912109\n",
      "batch_size: 256 \t| epoch: 25 \t| train loss: 0.10927 \t| test loss: 0.12029\t | time taken: 0.006668806076049805\n",
      "batch_size: 256 \t| epoch: 26 \t| train loss: 0.10546 \t| test loss: 0.11917\t | time taken: 0.0066640377044677734\n",
      "batch_size: 256 \t| epoch: 27 \t| train loss: 0.10713 \t| test loss: 0.11794\t | time taken: 0.006438016891479492\n",
      "batch_size: 256 \t| epoch: 28 \t| train loss: 0.10563 \t| test loss: 0.11664\t | time taken: 0.006486654281616211\n",
      "batch_size: 256 \t| epoch: 29 \t| train loss: 0.10591 \t| test loss: 0.11528\t | time taken: 0.0068662166595458984\n",
      "batch_size: 256 \t| epoch: 30 \t| train loss: 0.10533 \t| test loss: 0.11406\t | time taken: 0.0066640377044677734\n",
      "batch_size: 256 \t| epoch: 31 \t| train loss: 0.10237 \t| test loss: 0.11288\t | time taken: 0.006674766540527344\n",
      "batch_size: 256 \t| epoch: 32 \t| train loss: 0.09951 \t| test loss: 0.11171\t | time taken: 0.006551980972290039\n",
      "batch_size: 256 \t| epoch: 33 \t| train loss: 0.09489 \t| test loss: 0.11052\t | time taken: 0.0065670013427734375\n",
      "batch_size: 256 \t| epoch: 34 \t| train loss: 0.10019 \t| test loss: 0.10931\t | time taken: 0.006685972213745117\n",
      "batch_size: 256 \t| epoch: 35 \t| train loss: 0.10021 \t| test loss: 0.10807\t | time taken: 0.006421089172363281\n",
      "batch_size: 256 \t| epoch: 36 \t| train loss: 0.09538 \t| test loss: 0.10682\t | time taken: 0.0065310001373291016\n",
      "batch_size: 256 \t| epoch: 37 \t| train loss: 0.08841 \t| test loss: 0.10567\t | time taken: 0.006429195404052734\n",
      "batch_size: 256 \t| epoch: 38 \t| train loss: 0.08983 \t| test loss: 0.10457\t | time taken: 0.006595134735107422\n",
      "batch_size: 256 \t| epoch: 39 \t| train loss: 0.08961 \t| test loss: 0.10349\t | time taken: 0.006433010101318359\n",
      "batch_size: 256 \t| epoch: 40 \t| train loss: 0.08852 \t| test loss: 0.10247\t | time taken: 0.006645917892456055\n",
      "batch_size: 256 \t| epoch: 41 \t| train loss: 0.09124 \t| test loss: 0.10143\t | time taken: 0.006862163543701172\n",
      "batch_size: 256 \t| epoch: 42 \t| train loss: 0.08445 \t| test loss: 0.10039\t | time taken: 0.007570981979370117\n",
      "batch_size: 256 \t| epoch: 43 \t| train loss: 0.08754 \t| test loss: 0.09939\t | time taken: 0.006697893142700195\n",
      "batch_size: 256 \t| epoch: 44 \t| train loss: 0.08862 \t| test loss: 0.09840\t | time taken: 0.006527900695800781\n",
      "batch_size: 256 \t| epoch: 45 \t| train loss: 0.08513 \t| test loss: 0.09743\t | time taken: 0.006551027297973633\n",
      "batch_size: 256 \t| epoch: 46 \t| train loss: 0.08835 \t| test loss: 0.09644\t | time taken: 0.0064508914947509766\n",
      "batch_size: 256 \t| epoch: 47 \t| train loss: 0.08356 \t| test loss: 0.09539\t | time taken: 0.006513833999633789\n",
      "batch_size: 256 \t| epoch: 48 \t| train loss: 0.08405 \t| test loss: 0.09437\t | time taken: 0.006775856018066406\n",
      "batch_size: 256 \t| epoch: 49 \t| train loss: 0.08597 \t| test loss: 0.09350\t | time taken: 0.0068721771240234375\n",
      "batch_size: 256 \t| epoch: 50 \t| train loss: 0.07787 \t| test loss: 0.09259\t | time taken: 0.006558895111083984\n",
      "batch_size: 256 \t| epoch: 51 \t| train loss: 0.08022 \t| test loss: 0.09161\t | time taken: 0.0066912174224853516\n",
      "batch_size: 256 \t| epoch: 52 \t| train loss: 0.07997 \t| test loss: 0.09063\t | time taken: 0.006482124328613281\n",
      "batch_size: 256 \t| epoch: 53 \t| train loss: 0.08111 \t| test loss: 0.08968\t | time taken: 0.0065610408782958984\n",
      "batch_size: 256 \t| epoch: 54 \t| train loss: 0.07536 \t| test loss: 0.08880\t | time taken: 0.0067980289459228516\n",
      "batch_size: 256 \t| epoch: 55 \t| train loss: 0.07709 \t| test loss: 0.08790\t | time taken: 0.006542205810546875\n",
      "batch_size: 256 \t| epoch: 56 \t| train loss: 0.07850 \t| test loss: 0.08704\t | time taken: 0.0069849491119384766\n",
      "batch_size: 256 \t| epoch: 57 \t| train loss: 0.07539 \t| test loss: 0.08629\t | time taken: 0.006518125534057617\n",
      "batch_size: 256 \t| epoch: 58 \t| train loss: 0.07192 \t| test loss: 0.08550\t | time taken: 0.0065610408782958984\n",
      "batch_size: 256 \t| epoch: 59 \t| train loss: 0.07560 \t| test loss: 0.08464\t | time taken: 0.006713151931762695\n",
      "batch_size: 256 \t| epoch: 60 \t| train loss: 0.07275 \t| test loss: 0.08374\t | time taken: 0.0068128108978271484\n",
      "batch_size: 256 \t| epoch: 61 \t| train loss: 0.07134 \t| test loss: 0.08286\t | time taken: 0.006509065628051758\n",
      "batch_size: 256 \t| epoch: 62 \t| train loss: 0.07054 \t| test loss: 0.08208\t | time taken: 0.006702899932861328\n",
      "batch_size: 256 \t| epoch: 63 \t| train loss: 0.06869 \t| test loss: 0.08130\t | time taken: 0.006567239761352539\n",
      "batch_size: 256 \t| epoch: 64 \t| train loss: 0.06947 \t| test loss: 0.08050\t | time taken: 0.006459951400756836\n",
      "batch_size: 256 \t| epoch: 65 \t| train loss: 0.05929 \t| test loss: 0.07978\t | time taken: 0.0067636966705322266\n",
      "batch_size: 256 \t| epoch: 66 \t| train loss: 0.06486 \t| test loss: 0.07907\t | time taken: 0.006829023361206055\n",
      "batch_size: 256 \t| epoch: 67 \t| train loss: 0.06902 \t| test loss: 0.07839\t | time taken: 0.00652313232421875\n",
      "batch_size: 256 \t| epoch: 68 \t| train loss: 0.06446 \t| test loss: 0.07783\t | time taken: 0.006946086883544922\n",
      "batch_size: 256 \t| epoch: 69 \t| train loss: 0.06021 \t| test loss: 0.07717\t | time taken: 0.006759166717529297\n",
      "batch_size: 256 \t| epoch: 70 \t| train loss: 0.06352 \t| test loss: 0.07644\t | time taken: 0.00748896598815918\n",
      "batch_size: 256 \t| epoch: 71 \t| train loss: 0.06320 \t| test loss: 0.07559\t | time taken: 0.006526947021484375\n",
      "batch_size: 256 \t| epoch: 72 \t| train loss: 0.06128 \t| test loss: 0.07483\t | time taken: 0.012145042419433594\n",
      "batch_size: 256 \t| epoch: 73 \t| train loss: 0.06431 \t| test loss: 0.07407\t | time taken: 0.006876945495605469\n",
      "batch_size: 256 \t| epoch: 74 \t| train loss: 0.06397 \t| test loss: 0.07345\t | time taken: 0.006563901901245117\n",
      "batch_size: 256 \t| epoch: 75 \t| train loss: 0.05951 \t| test loss: 0.07292\t | time taken: 0.006913900375366211\n",
      "batch_size: 256 \t| epoch: 76 \t| train loss: 0.05358 \t| test loss: 0.07244\t | time taken: 0.0066301822662353516\n",
      "batch_size: 256 \t| epoch: 77 \t| train loss: 0.05524 \t| test loss: 0.07194\t | time taken: 0.0065538883209228516\n",
      "batch_size: 256 \t| epoch: 78 \t| train loss: 0.05753 \t| test loss: 0.07140\t | time taken: 0.006455183029174805\n",
      "batch_size: 256 \t| epoch: 79 \t| train loss: 0.05222 \t| test loss: 0.07077\t | time taken: 0.006417751312255859\n",
      "batch_size: 256 \t| epoch: 80 \t| train loss: 0.05810 \t| test loss: 0.07015\t | time taken: 0.00676417350769043\n",
      "batch_size: 256 \t| epoch: 81 \t| train loss: 0.05838 \t| test loss: 0.06954\t | time taken: 0.0065419673919677734\n",
      "batch_size: 256 \t| epoch: 82 \t| train loss: 0.05515 \t| test loss: 0.06888\t | time taken: 0.006646156311035156\n",
      "batch_size: 256 \t| epoch: 83 \t| train loss: 0.05155 \t| test loss: 0.06819\t | time taken: 0.006587982177734375\n",
      "batch_size: 256 \t| epoch: 84 \t| train loss: 0.05224 \t| test loss: 0.06757\t | time taken: 0.00648808479309082\n",
      "batch_size: 256 \t| epoch: 85 \t| train loss: 0.05242 \t| test loss: 0.06707\t | time taken: 0.0064580440521240234\n",
      "batch_size: 256 \t| epoch: 86 \t| train loss: 0.05545 \t| test loss: 0.06651\t | time taken: 0.006464958190917969\n",
      "batch_size: 256 \t| epoch: 87 \t| train loss: 0.05145 \t| test loss: 0.06593\t | time taken: 0.006793022155761719\n",
      "batch_size: 256 \t| epoch: 88 \t| train loss: 0.04710 \t| test loss: 0.06539\t | time taken: 0.006692171096801758\n",
      "batch_size: 256 \t| epoch: 89 \t| train loss: 0.05680 \t| test loss: 0.06494\t | time taken: 0.00680088996887207\n",
      "batch_size: 256 \t| epoch: 90 \t| train loss: 0.04605 \t| test loss: 0.06444\t | time taken: 0.006497859954833984\n",
      "batch_size: 256 \t| epoch: 91 \t| train loss: 0.04471 \t| test loss: 0.06385\t | time taken: 0.006672859191894531\n",
      "batch_size: 256 \t| epoch: 92 \t| train loss: 0.05149 \t| test loss: 0.06326\t | time taken: 0.006516933441162109\n",
      "batch_size: 256 \t| epoch: 93 \t| train loss: 0.04796 \t| test loss: 0.06283\t | time taken: 0.006489753723144531\n",
      "batch_size: 256 \t| epoch: 94 \t| train loss: 0.04733 \t| test loss: 0.06235\t | time taken: 0.006637096405029297\n",
      "batch_size: 256 \t| epoch: 95 \t| train loss: 0.04971 \t| test loss: 0.06191\t | time taken: 0.006888866424560547\n",
      "batch_size: 256 \t| epoch: 96 \t| train loss: 0.04890 \t| test loss: 0.06156\t | time taken: 0.006648063659667969\n",
      "batch_size: 256 \t| epoch: 97 \t| train loss: 0.04546 \t| test loss: 0.06106\t | time taken: 0.006983757019042969\n",
      "batch_size: 256 \t| epoch: 98 \t| train loss: 0.04129 \t| test loss: 0.06054\t | time taken: 0.006721973419189453\n",
      "batch_size: 256 \t| epoch: 99 \t| train loss: 0.04285 \t| test loss: 0.06012\t | time taken: 0.006500959396362305\n",
      "reach max number of epoch\n",
      "batch_size: 256 \t| epoch: 0 \t| train loss: 0.05886 \t| test loss: 0.05925\t | time taken: 0.006938934326171875\n",
      "batch_size: 256 \t| epoch: 1 \t| train loss: 0.06062 \t| test loss: 0.05762\t | time taken: 0.006614208221435547\n",
      "batch_size: 256 \t| epoch: 2 \t| train loss: 0.06069 \t| test loss: 0.05579\t | time taken: 0.0065708160400390625\n",
      "batch_size: 256 \t| epoch: 3 \t| train loss: 0.05026 \t| test loss: 0.05457\t | time taken: 0.006862163543701172\n",
      "batch_size: 256 \t| epoch: 4 \t| train loss: 0.05707 \t| test loss: 0.05371\t | time taken: 0.007012128829956055\n",
      "batch_size: 256 \t| epoch: 5 \t| train loss: 0.04808 \t| test loss: 0.05288\t | time taken: 0.006582021713256836\n",
      "batch_size: 256 \t| epoch: 6 \t| train loss: 0.05288 \t| test loss: 0.05172\t | time taken: 0.0064697265625\n",
      "batch_size: 256 \t| epoch: 7 \t| train loss: 0.05510 \t| test loss: 0.05045\t | time taken: 0.006497621536254883\n",
      "batch_size: 256 \t| epoch: 8 \t| train loss: 0.04763 \t| test loss: 0.04963\t | time taken: 0.00666499137878418\n",
      "batch_size: 256 \t| epoch: 9 \t| train loss: 0.04965 \t| test loss: 0.04879\t | time taken: 0.006926774978637695\n",
      "batch_size: 256 \t| epoch: 10 \t| train loss: 0.05142 \t| test loss: 0.04791\t | time taken: 0.006474971771240234\n",
      "batch_size: 256 \t| epoch: 11 \t| train loss: 0.05210 \t| test loss: 0.04712\t | time taken: 0.006515026092529297\n",
      "batch_size: 256 \t| epoch: 12 \t| train loss: 0.04572 \t| test loss: 0.04627\t | time taken: 0.0068972110748291016\n",
      "batch_size: 256 \t| epoch: 13 \t| train loss: 0.04606 \t| test loss: 0.04546\t | time taken: 0.007286787033081055\n",
      "batch_size: 256 \t| epoch: 14 \t| train loss: 0.04502 \t| test loss: 0.04481\t | time taken: 0.008675098419189453\n",
      "batch_size: 256 \t| epoch: 15 \t| train loss: 0.04738 \t| test loss: 0.04429\t | time taken: 0.008405923843383789\n",
      "batch_size: 256 \t| epoch: 16 \t| train loss: 0.04355 \t| test loss: 0.04381\t | time taken: 0.008347034454345703\n",
      "batch_size: 256 \t| epoch: 17 \t| train loss: 0.04566 \t| test loss: 0.04306\t | time taken: 0.0077092647552490234\n",
      "batch_size: 256 \t| epoch: 18 \t| train loss: 0.04893 \t| test loss: 0.04226\t | time taken: 0.009970903396606445\n",
      "batch_size: 256 \t| epoch: 19 \t| train loss: 0.03945 \t| test loss: 0.04160\t | time taken: 0.008597850799560547\n",
      "batch_size: 256 \t| epoch: 20 \t| train loss: 0.04597 \t| test loss: 0.04092\t | time taken: 0.009613752365112305\n",
      "batch_size: 256 \t| epoch: 21 \t| train loss: 0.04449 \t| test loss: 0.04043\t | time taken: 0.007205009460449219\n",
      "batch_size: 256 \t| epoch: 22 \t| train loss: 0.04240 \t| test loss: 0.03998\t | time taken: 0.00698399543762207\n",
      "batch_size: 256 \t| epoch: 23 \t| train loss: 0.04258 \t| test loss: 0.03950\t | time taken: 0.014895915985107422\n",
      "batch_size: 256 \t| epoch: 24 \t| train loss: 0.03986 \t| test loss: 0.03922\t | time taken: 0.04156804084777832\n",
      "batch_size: 256 \t| epoch: 25 \t| train loss: 0.04336 \t| test loss: 0.03869\t | time taken: 0.006596803665161133\n",
      "batch_size: 256 \t| epoch: 26 \t| train loss: 0.03793 \t| test loss: 0.03802\t | time taken: 0.0066318511962890625\n",
      "batch_size: 256 \t| epoch: 27 \t| train loss: 0.03855 \t| test loss: 0.03753\t | time taken: 0.006559133529663086\n",
      "batch_size: 256 \t| epoch: 28 \t| train loss: 0.04032 \t| test loss: 0.03729\t | time taken: 0.007057905197143555\n",
      "batch_size: 256 \t| epoch: 29 \t| train loss: 0.04193 \t| test loss: 0.03672\t | time taken: 0.006826877593994141\n",
      "batch_size: 256 \t| epoch: 30 \t| train loss: 0.03930 \t| test loss: 0.03602\t | time taken: 0.006637096405029297\n",
      "batch_size: 256 \t| epoch: 31 \t| train loss: 0.03949 \t| test loss: 0.03542\t | time taken: 0.006495237350463867\n",
      "batch_size: 256 \t| epoch: 32 \t| train loss: 0.04049 \t| test loss: 0.03488\t | time taken: 0.006620168685913086\n",
      "batch_size: 256 \t| epoch: 33 \t| train loss: 0.03759 \t| test loss: 0.03451\t | time taken: 0.006821155548095703\n",
      "batch_size: 256 \t| epoch: 34 \t| train loss: 0.04006 \t| test loss: 0.03412\t | time taken: 0.0064640045166015625\n",
      "batch_size: 256 \t| epoch: 35 \t| train loss: 0.03991 \t| test loss: 0.03372\t | time taken: 0.006597042083740234\n",
      "batch_size: 256 \t| epoch: 36 \t| train loss: 0.04160 \t| test loss: 0.03323\t | time taken: 0.0067310333251953125\n",
      "batch_size: 256 \t| epoch: 37 \t| train loss: 0.04233 \t| test loss: 0.03269\t | time taken: 0.006510019302368164\n",
      "batch_size: 256 \t| epoch: 38 \t| train loss: 0.03740 \t| test loss: 0.03212\t | time taken: 0.006478071212768555\n",
      "batch_size: 256 \t| epoch: 39 \t| train loss: 0.03550 \t| test loss: 0.03177\t | time taken: 0.006774187088012695\n",
      "batch_size: 256 \t| epoch: 40 \t| train loss: 0.03832 \t| test loss: 0.03164\t | time taken: 0.006500959396362305\n",
      "batch_size: 256 \t| epoch: 41 \t| train loss: 0.03641 \t| test loss: 0.03161\t | time taken: 0.006601095199584961\n",
      "batch_size: 256 \t| epoch: 42 \t| train loss: 0.03297 \t| test loss: 0.03148\t | time taken: 0.00708317756652832\n",
      "batch_size: 256 \t| epoch: 43 \t| train loss: 0.03623 \t| test loss: 0.03140\t | time taken: 0.006732940673828125\n",
      "batch_size: 256 \t| epoch: 44 \t| train loss: 0.03398 \t| test loss: 0.03098\t | time taken: 0.007273197174072266\n",
      "batch_size: 256 \t| epoch: 45 \t| train loss: 0.04008 \t| test loss: 0.03043\t | time taken: 0.006802082061767578\n",
      "batch_size: 256 \t| epoch: 46 \t| train loss: 0.03538 \t| test loss: 0.02998\t | time taken: 0.0067217350006103516\n",
      "batch_size: 256 \t| epoch: 47 \t| train loss: 0.03484 \t| test loss: 0.02951\t | time taken: 0.006621122360229492\n",
      "batch_size: 256 \t| epoch: 48 \t| train loss: 0.02847 \t| test loss: 0.02907\t | time taken: 0.0067729949951171875\n",
      "batch_size: 256 \t| epoch: 49 \t| train loss: 0.02909 \t| test loss: 0.02868\t | time taken: 0.006574153900146484\n",
      "batch_size: 256 \t| epoch: 50 \t| train loss: 0.03083 \t| test loss: 0.02846\t | time taken: 0.00680088996887207\n",
      "batch_size: 256 \t| epoch: 51 \t| train loss: 0.03306 \t| test loss: 0.02820\t | time taken: 0.006442070007324219\n",
      "batch_size: 256 \t| epoch: 52 \t| train loss: 0.02710 \t| test loss: 0.02798\t | time taken: 0.006453990936279297\n",
      "batch_size: 256 \t| epoch: 53 \t| train loss: 0.03102 \t| test loss: 0.02784\t | time taken: 0.006479978561401367\n",
      "batch_size: 256 \t| epoch: 54 \t| train loss: 0.03152 \t| test loss: 0.02756\t | time taken: 0.006501913070678711\n",
      "batch_size: 256 \t| epoch: 55 \t| train loss: 0.03263 \t| test loss: 0.02715\t | time taken: 0.006922006607055664\n",
      "batch_size: 256 \t| epoch: 56 \t| train loss: 0.03588 \t| test loss: 0.02667\t | time taken: 0.006810188293457031\n",
      "batch_size: 256 \t| epoch: 57 \t| train loss: 0.03146 \t| test loss: 0.02641\t | time taken: 0.0069239139556884766\n",
      "batch_size: 256 \t| epoch: 58 \t| train loss: 0.02897 \t| test loss: 0.02619\t | time taken: 0.006619930267333984\n",
      "batch_size: 256 \t| epoch: 59 \t| train loss: 0.02705 \t| test loss: 0.02598\t | time taken: 0.0067958831787109375\n",
      "batch_size: 256 \t| epoch: 60 \t| train loss: 0.02707 \t| test loss: 0.02581\t | time taken: 0.006639242172241211\n",
      "batch_size: 256 \t| epoch: 61 \t| train loss: 0.03140 \t| test loss: 0.02563\t | time taken: 0.006571054458618164\n",
      "batch_size: 256 \t| epoch: 62 \t| train loss: 0.03081 \t| test loss: 0.02525\t | time taken: 0.006464958190917969\n",
      "batch_size: 256 \t| epoch: 63 \t| train loss: 0.02739 \t| test loss: 0.02484\t | time taken: 0.006448984146118164\n",
      "batch_size: 256 \t| epoch: 64 \t| train loss: 0.03350 \t| test loss: 0.02431\t | time taken: 0.006997108459472656\n",
      "batch_size: 256 \t| epoch: 65 \t| train loss: 0.02566 \t| test loss: 0.02388\t | time taken: 0.006546974182128906\n",
      "batch_size: 256 \t| epoch: 66 \t| train loss: 0.02503 \t| test loss: 0.02351\t | time taken: 0.006670951843261719\n",
      "batch_size: 256 \t| epoch: 67 \t| train loss: 0.02835 \t| test loss: 0.02327\t | time taken: 0.006468057632446289\n",
      "batch_size: 256 \t| epoch: 68 \t| train loss: 0.02687 \t| test loss: 0.02301\t | time taken: 0.006597757339477539\n",
      "batch_size: 256 \t| epoch: 69 \t| train loss: 0.02346 \t| test loss: 0.02275\t | time taken: 0.006580829620361328\n",
      "batch_size: 256 \t| epoch: 70 \t| train loss: 0.02897 \t| test loss: 0.02239\t | time taken: 0.007054805755615234\n",
      "batch_size: 256 \t| epoch: 71 \t| train loss: 0.02508 \t| test loss: 0.02223\t | time taken: 0.0072820186614990234\n",
      "batch_size: 256 \t| epoch: 72 \t| train loss: 0.02758 \t| test loss: 0.02201\t | time taken: 0.013843297958374023\n",
      "batch_size: 256 \t| epoch: 73 \t| train loss: 0.02641 \t| test loss: 0.02182\t | time taken: 0.009371757507324219\n",
      "batch_size: 256 \t| epoch: 74 \t| train loss: 0.02208 \t| test loss: 0.02172\t | time taken: 0.007041215896606445\n",
      "batch_size: 256 \t| epoch: 75 \t| train loss: 0.02675 \t| test loss: 0.02150\t | time taken: 0.0068438053131103516\n",
      "batch_size: 256 \t| epoch: 76 \t| train loss: 0.02613 \t| test loss: 0.02121\t | time taken: 0.007189035415649414\n",
      "batch_size: 256 \t| epoch: 77 \t| train loss: 0.02725 \t| test loss: 0.02092\t | time taken: 0.006434917449951172\n",
      "batch_size: 256 \t| epoch: 78 \t| train loss: 0.02463 \t| test loss: 0.02075\t | time taken: 0.006763935089111328\n",
      "batch_size: 256 \t| epoch: 79 \t| train loss: 0.02506 \t| test loss: 0.02059\t | time taken: 0.006567955017089844\n",
      "batch_size: 256 \t| epoch: 80 \t| train loss: 0.02334 \t| test loss: 0.02030\t | time taken: 0.006726980209350586\n",
      "batch_size: 256 \t| epoch: 81 \t| train loss: 0.02186 \t| test loss: 0.01995\t | time taken: 0.006756782531738281\n",
      "batch_size: 256 \t| epoch: 82 \t| train loss: 0.02282 \t| test loss: 0.01970\t | time taken: 0.006557941436767578\n",
      "batch_size: 256 \t| epoch: 83 \t| train loss: 0.02223 \t| test loss: 0.01948\t | time taken: 0.006685733795166016\n",
      "batch_size: 256 \t| epoch: 84 \t| train loss: 0.02208 \t| test loss: 0.01937\t | time taken: 0.006459712982177734\n",
      "batch_size: 256 \t| epoch: 85 \t| train loss: 0.02373 \t| test loss: 0.01936\t | time taken: 0.0069010257720947266\n",
      "batch_size: 256 \t| epoch: 86 \t| train loss: 0.02597 \t| test loss: 0.01919\t | time taken: 0.006707906723022461\n",
      "batch_size: 256 \t| epoch: 87 \t| train loss: 0.02677 \t| test loss: 0.01902\t | time taken: 0.006625652313232422\n",
      "batch_size: 256 \t| epoch: 88 \t| train loss: 0.02479 \t| test loss: 0.01873\t | time taken: 0.006510019302368164\n",
      "batch_size: 256 \t| epoch: 89 \t| train loss: 0.01982 \t| test loss: 0.01842\t | time taken: 0.006574153900146484\n",
      "batch_size: 256 \t| epoch: 90 \t| train loss: 0.02703 \t| test loss: 0.01818\t | time taken: 0.006608009338378906\n",
      "batch_size: 256 \t| epoch: 91 \t| train loss: 0.02106 \t| test loss: 0.01805\t | time taken: 0.006474971771240234\n",
      "batch_size: 256 \t| epoch: 92 \t| train loss: 0.02432 \t| test loss: 0.01795\t | time taken: 0.0066030025482177734\n",
      "batch_size: 256 \t| epoch: 93 \t| train loss: 0.02196 \t| test loss: 0.01788\t | time taken: 0.0065021514892578125\n",
      "batch_size: 256 \t| epoch: 94 \t| train loss: 0.02022 \t| test loss: 0.01764\t | time taken: 0.00675511360168457\n",
      "batch_size: 256 \t| epoch: 95 \t| train loss: 0.02350 \t| test loss: 0.01749\t | time taken: 0.006744861602783203\n",
      "batch_size: 256 \t| epoch: 96 \t| train loss: 0.02406 \t| test loss: 0.01733\t | time taken: 0.00724482536315918\n",
      "batch_size: 256 \t| epoch: 97 \t| train loss: 0.02284 \t| test loss: 0.01706\t | time taken: 0.0065898895263671875\n",
      "batch_size: 256 \t| epoch: 98 \t| train loss: 0.01898 \t| test loss: 0.01667\t | time taken: 0.007318019866943359\n",
      "batch_size: 256 \t| epoch: 99 \t| train loss: 0.02284 \t| test loss: 0.01635\t | time taken: 0.0068721771240234375\n",
      "reach max number of epoch\n",
      "batch_size: 256 \t| epoch: 0 \t| train loss: 0.02428 \t| test loss: 0.01605\t | time taken: 0.0068399906158447266\n",
      "batch_size: 256 \t| epoch: 1 \t| train loss: 0.03053 \t| test loss: 0.01574\t | time taken: 0.0066738128662109375\n",
      "batch_size: 256 \t| epoch: 2 \t| train loss: 0.02380 \t| test loss: 0.01552\t | time taken: 0.006779909133911133\n",
      "batch_size: 256 \t| epoch: 3 \t| train loss: 0.03020 \t| test loss: 0.01533\t | time taken: 0.006638050079345703\n",
      "batch_size: 256 \t| epoch: 4 \t| train loss: 0.02490 \t| test loss: 0.01515\t | time taken: 0.007080078125\n",
      "batch_size: 256 \t| epoch: 5 \t| train loss: 0.02639 \t| test loss: 0.01496\t | time taken: 0.0066318511962890625\n",
      "batch_size: 256 \t| epoch: 6 \t| train loss: 0.02548 \t| test loss: 0.01482\t | time taken: 0.006727933883666992\n",
      "batch_size: 256 \t| epoch: 7 \t| train loss: 0.02813 \t| test loss: 0.01467\t | time taken: 0.0068569183349609375\n",
      "batch_size: 256 \t| epoch: 8 \t| train loss: 0.02630 \t| test loss: 0.01457\t | time taken: 0.006478071212768555\n",
      "batch_size: 256 \t| epoch: 9 \t| train loss: 0.02272 \t| test loss: 0.01457\t | time taken: 0.006594181060791016\n",
      "batch_size: 256 \t| epoch: 10 \t| train loss: 0.01967 \t| test loss: 0.01469\t | time taken: 0.00675201416015625\n",
      "batch_size: 256 \t| epoch: 11 \t| train loss: 0.02550 \t| test loss: 0.01466\t | time taken: 0.006750822067260742\n",
      "batch_size: 256 \t| epoch: 12 \t| train loss: 0.02521 \t| test loss: 0.01444\t | time taken: 0.006515026092529297\n",
      "batch_size: 256 \t| epoch: 13 \t| train loss: 0.01989 \t| test loss: 0.01410\t | time taken: 0.0067369937896728516\n",
      "batch_size: 256 \t| epoch: 14 \t| train loss: 0.03055 \t| test loss: 0.01379\t | time taken: 0.006624937057495117\n",
      "batch_size: 256 \t| epoch: 15 \t| train loss: 0.01716 \t| test loss: 0.01354\t | time taken: 0.006540775299072266\n",
      "batch_size: 256 \t| epoch: 16 \t| train loss: 0.02852 \t| test loss: 0.01348\t | time taken: 0.0066950321197509766\n",
      "batch_size: 256 \t| epoch: 17 \t| train loss: 0.02158 \t| test loss: 0.01339\t | time taken: 0.006520986557006836\n",
      "batch_size: 256 \t| epoch: 18 \t| train loss: 0.02358 \t| test loss: 0.01317\t | time taken: 0.006807804107666016\n",
      "batch_size: 256 \t| epoch: 19 \t| train loss: 0.01912 \t| test loss: 0.01298\t | time taken: 0.006522178649902344\n",
      "batch_size: 256 \t| epoch: 20 \t| train loss: 0.02086 \t| test loss: 0.01266\t | time taken: 0.006631135940551758\n",
      "batch_size: 256 \t| epoch: 21 \t| train loss: 0.02046 \t| test loss: 0.01240\t | time taken: 0.006556034088134766\n",
      "batch_size: 256 \t| epoch: 22 \t| train loss: 0.02062 \t| test loss: 0.01217\t | time taken: 0.006526947021484375\n",
      "batch_size: 256 \t| epoch: 23 \t| train loss: 0.01874 \t| test loss: 0.01193\t | time taken: 0.007178068161010742\n",
      "batch_size: 256 \t| epoch: 24 \t| train loss: 0.02618 \t| test loss: 0.01180\t | time taken: 0.006653785705566406\n",
      "batch_size: 256 \t| epoch: 25 \t| train loss: 0.02095 \t| test loss: 0.01192\t | time taken: 0.007395029067993164\n",
      "batch_size: 256 \t| epoch: 26 \t| train loss: 0.02229 \t| test loss: 0.01201\t | time taken: 0.006537914276123047\n",
      "batch_size: 256 \t| epoch: 27 \t| train loss: 0.02336 \t| test loss: 0.01205\t | time taken: 0.007317066192626953\n",
      "early stopping at epoch 27\n",
      "batch_size: 256 \t| epoch: 0 \t| train loss: 0.02067 \t| test loss: 0.01202\t | time taken: 0.006792783737182617\n",
      "batch_size: 256 \t| epoch: 1 \t| train loss: 0.01854 \t| test loss: 0.01207\t | time taken: 0.006856203079223633\n",
      "batch_size: 256 \t| epoch: 2 \t| train loss: 0.02159 \t| test loss: 0.01199\t | time taken: 0.006887912750244141\n",
      "batch_size: 256 \t| epoch: 3 \t| train loss: 0.01721 \t| test loss: 0.01181\t | time taken: 0.006695985794067383\n",
      "batch_size: 256 \t| epoch: 4 \t| train loss: 0.01934 \t| test loss: 0.01172\t | time taken: 0.0069348812103271484\n",
      "batch_size: 256 \t| epoch: 5 \t| train loss: 0.01896 \t| test loss: 0.01173\t | time taken: 0.00651097297668457\n",
      "batch_size: 256 \t| epoch: 6 \t| train loss: 0.01621 \t| test loss: 0.01178\t | time taken: 0.006543159484863281\n",
      "batch_size: 256 \t| epoch: 7 \t| train loss: 0.01955 \t| test loss: 0.01168\t | time taken: 0.006529092788696289\n",
      "batch_size: 256 \t| epoch: 8 \t| train loss: 0.02153 \t| test loss: 0.01142\t | time taken: 0.006825923919677734\n",
      "batch_size: 256 \t| epoch: 9 \t| train loss: 0.02514 \t| test loss: 0.01106\t | time taken: 0.0067331790924072266\n",
      "batch_size: 256 \t| epoch: 10 \t| train loss: 0.01889 \t| test loss: 0.01089\t | time taken: 0.006600141525268555\n",
      "batch_size: 256 \t| epoch: 11 \t| train loss: 0.02181 \t| test loss: 0.01079\t | time taken: 0.006551980972290039\n",
      "batch_size: 256 \t| epoch: 12 \t| train loss: 0.01779 \t| test loss: 0.01036\t | time taken: 0.006759166717529297\n",
      "batch_size: 256 \t| epoch: 13 \t| train loss: 0.01943 \t| test loss: 0.01034\t | time taken: 0.006799936294555664\n",
      "batch_size: 256 \t| epoch: 14 \t| train loss: 0.02372 \t| test loss: 0.01030\t | time taken: 0.00656580924987793\n",
      "batch_size: 256 \t| epoch: 15 \t| train loss: 0.01713 \t| test loss: 0.01007\t | time taken: 0.006573915481567383\n",
      "batch_size: 256 \t| epoch: 16 \t| train loss: 0.02073 \t| test loss: 0.00978\t | time taken: 0.0065059661865234375\n",
      "batch_size: 256 \t| epoch: 17 \t| train loss: 0.02249 \t| test loss: 0.00971\t | time taken: 0.006530046463012695\n",
      "batch_size: 256 \t| epoch: 18 \t| train loss: 0.02127 \t| test loss: 0.00986\t | time taken: 0.006657838821411133\n",
      "batch_size: 256 \t| epoch: 19 \t| train loss: 0.01877 \t| test loss: 0.00982\t | time taken: 0.0065860748291015625\n",
      "batch_size: 256 \t| epoch: 20 \t| train loss: 0.02282 \t| test loss: 0.00964\t | time taken: 0.006569862365722656\n",
      "batch_size: 256 \t| epoch: 21 \t| train loss: 0.02131 \t| test loss: 0.00947\t | time taken: 0.006498098373413086\n",
      "batch_size: 256 \t| epoch: 22 \t| train loss: 0.02150 \t| test loss: 0.00937\t | time taken: 0.006527900695800781\n",
      "batch_size: 256 \t| epoch: 23 \t| train loss: 0.02156 \t| test loss: 0.00924\t | time taken: 0.006593942642211914\n",
      "batch_size: 256 \t| epoch: 24 \t| train loss: 0.02186 \t| test loss: 0.00915\t | time taken: 0.007103919982910156\n",
      "batch_size: 256 \t| epoch: 25 \t| train loss: 0.01461 \t| test loss: 0.00904\t | time taken: 0.006554126739501953\n",
      "batch_size: 256 \t| epoch: 26 \t| train loss: 0.01696 \t| test loss: 0.00888\t | time taken: 0.007097959518432617\n",
      "batch_size: 256 \t| epoch: 27 \t| train loss: 0.01662 \t| test loss: 0.00868\t | time taken: 0.0067980289459228516\n",
      "batch_size: 256 \t| epoch: 28 \t| train loss: 0.01469 \t| test loss: 0.00851\t | time taken: 0.006849050521850586\n",
      "batch_size: 256 \t| epoch: 29 \t| train loss: 0.01753 \t| test loss: 0.00845\t | time taken: 0.006613969802856445\n",
      "batch_size: 256 \t| epoch: 30 \t| train loss: 0.01871 \t| test loss: 0.00841\t | time taken: 0.006821155548095703\n",
      "batch_size: 256 \t| epoch: 31 \t| train loss: 0.01654 \t| test loss: 0.00846\t | time taken: 0.006714820861816406\n",
      "batch_size: 256 \t| epoch: 32 \t| train loss: 0.01446 \t| test loss: 0.00853\t | time taken: 0.006855964660644531\n",
      "batch_size: 256 \t| epoch: 33 \t| train loss: 0.01574 \t| test loss: 0.00869\t | time taken: 0.006396055221557617\n",
      "early stopping at epoch 33\n"
     ]
    }
   ],
   "source": [
    "# TODO: Enter your code here\n",
    "\n",
    "time_train_dict = {batch_size: [] for batch_size in batch_sizes}\n",
    "mean_cv_acc_dict = {batch_size: [] for batch_size in batch_sizes}\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    model = model_dict[batch_size]\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    loss_fn = nn.BCELoss()\n",
    "\n",
    "    n_splits = 5        \n",
    "\n",
    "    for i in range(n_splits):\n",
    "        X_train_ndarray = X_train_scaled_dict[batch_size][i]\n",
    "        X_val_ndarray = X_val_scaled_dict[batch_size][i]\n",
    "        y_train_ndarray = y_train_dict[batch_size][i]\n",
    "        y_val_ndarray = y_val_dict[batch_size][i]\n",
    "        train_dataset = AudioDataset(X_train_ndarray, y_train_ndarray)\n",
    "        val_dataset = AudioDataset(X_val_ndarray, y_val_ndarray)\n",
    "        train_loader = AudioDataLoader(train_dataset, batch_size)\n",
    "        val_loader = AudioDataLoader(test_dataset, batch_size)\n",
    "\n",
    "        loss_train = []\n",
    "        acc_train = []\n",
    "        time_train = []\n",
    "        loss_val = []\n",
    "        acc_val = []\n",
    "\n",
    "        early_stopper = EarlyStopper(patience=3, min_delta=1e-5)\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            start_time = time.time()\n",
    "            model.train()\n",
    "            with torch.enable_grad():\n",
    "                y_pred_list = []\n",
    "                y_true_list = []\n",
    "                for batch in train_loader:\n",
    "                    X, y = batch\n",
    "                    y_pred = model(X)\n",
    "                    loss = loss_fn(y_pred, y)\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    y_pred_list.append(y_pred)\n",
    "                    y_true_list.append(y)\n",
    "                y_pred_list = torch.cat(y_pred_list)\n",
    "                y_true_list= torch.cat(y_true_list)\n",
    "                loss_train.append(loss_fn(y_pred_list, y_true_list))\n",
    "                y_pred_list = torch.round(y_pred_list)\n",
    "                acc_train.append((y_pred_list == y_true_list).sum().item() / len(y_true_list))\n",
    "            end_time = time.time()\n",
    "            time_train.append(end_time - start_time)\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                y_pred_list = []\n",
    "                y_true_list = []\n",
    "                for batch in val_loader:\n",
    "                    X, y = batch\n",
    "                    y_pred = model(X)\n",
    "                    loss = loss_fn(y_pred, y)\n",
    "                    y_pred_list.append(y_pred)\n",
    "                    y_true_list.append(y)\n",
    "                y_pred_list = torch.cat(y_pred_list)\n",
    "                y_true_list= torch.cat(y_true_list)\n",
    "                loss_val.append(loss_fn(y_pred_list, y_true_list))\n",
    "                y_pred_list = torch.round(y_pred_list)\n",
    "                acc_val.append((y_pred_list == y_true_list).sum().item() / len(y_true_list))\n",
    "\n",
    "            log_info = f\"batch_size: {batch_size} \\t| epoch: {epoch} \\t| train loss: {loss_train[-1]:.5f} \\t| test loss: {loss_val[-1]:.5f}\\t | time taken: {time_train[-1]}\"\n",
    "            print(log_info)\n",
    "\n",
    "            if early_stopper.early_stop(loss_val[-1]):\n",
    "                print(f\"early stopping at epoch {epoch}\")\n",
    "                break\n",
    "            \n",
    "            if epoch == n_epochs - 1:\n",
    "                print(\"reach max number of epoch\")\n",
    "\n",
    "        time_train_dict[batch_size].append(time_train[-1])\n",
    "        mean_cv_acc_dict[batch_size].append(acc_val[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b224f8-3b69-425f-bebb-5d15d16e9596",
   "metadata": {},
   "source": [
    "3. Plot scatterplot of mean cross validation accuracies on the final epoch for the different batch sizes. Limit search space to batch sizes {32, 64, 128, 256}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f044488a-88f9-434e-8b66-f6518392daf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Enter your code here\n",
    "\n",
    "mean_cv_accs_plot = [np.mean(mean_cv_acc_dict[batch_size]) for batch_size in batch_sizes]\n",
    "\n",
    "plt.scatter(batch_sizes, mean_cv_accs_plot)\n",
    "plt.xlabel(\"Batch Size\")\n",
    "plt.ylabel(\"Mean Cross Validation Accuracy\")\n",
    "plt.title(\"Mean Cross Validation Accuracy vs Batch Size\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa030c54-1812-482c-8662-289318457a3b",
   "metadata": {},
   "source": [
    "4. Create a table of time taken to train the network on the last epoch against different batch sizes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad41399d-a7fa-40b4-8d56-4f2fd500f36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Split 1   Split 2   Split 3   Split 4   Split 5      Mean\n",
      "32   0.023396  0.023439  0.023593  0.023433  0.023148  0.023402\n",
      "64    0.01429  0.014743  0.014052  0.022999  0.014324  0.016082\n",
      "128  0.009419  0.009299  0.009438  0.035724   0.00924  0.014624\n",
      "256  0.032666  0.006501  0.006872  0.007317  0.006396   0.01195\n"
     ]
    }
   ],
   "source": [
    "# TODO: Enter your code here\n",
    "\n",
    "batch_sizes = list(time_train_dict.keys())\n",
    "split_numbers = [f\"Split {i+1}\" for i in range(len(time_train_dict[batch_sizes[0]]))]\n",
    "columns = split_numbers + [\"Mean\"]\n",
    "\n",
    "results = pd.DataFrame(index=batch_sizes, columns=columns)\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    times = time_train_dict[batch_size]\n",
    "    mean_time = np.mean(times)\n",
    "\n",
    "    for i, time in enumerate(times):\n",
    "        results.at[batch_size, f\"Split {i+1}\"] = time\n",
    "\n",
    "    results.at[batch_size, \"Mean\"] = mean_time\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6750ff86-6c16-4522-a858-5f533d8b0353",
   "metadata": {},
   "source": [
    "5. Select the optimal batch size and state a reason for your selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238d470c-c73a-4c83-a09e-695b5669b736",
   "metadata": {},
   "source": [
    "\\# TODO: \\<Enter your answer here\\>\n",
    "\n",
    "From the experimental chart, we can see that no matter how the batch size is set, a high cross-validation accuracy can be achieved. It is also worth noting that the cross-validation accuracy tends to decrease as the batch size increases. From the perspective of time consumption, a larger batch size consumes less time. Empirically, doubling the batch size will reduce the time consumption to about three-quarters of the original. Therefore, this is a decision-making problem. If we want the model to train faster, we can choose a larger batch. If we want the model to have better accuracy, we should set a smaller batch. In the scenario of this project, the absolute value of the change in accuracy is not obvious, so we can accept the slight decrease in accuracy caused by a larger batch. Therefore, I will choose a batch size of 128, because it brings faster training time and acceptable accuracy at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2ea730-e6f9-4e9e-ad30-905d62436d37",
   "metadata": {},
   "source": [
    "Part A, Q3 (10 marks)\n",
    "---\n",
    "In this question, we will find the optimal number of hidden neurons for first hidden layer of the 4-layer network (3 hidden layers, output layer) designed in Q1 and Q2.\n",
    "\n",
    "To reduce repeated code, you may need to import the network (MLP defined in QA1) from common_utils.py. You will not be repenalised for any error in QA1 here as the code in QA1 will not be remarked. The following code cell will not be marked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f995b5b-1c8a-4a50-a615-c11b5fa66586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cc14b0-eb67-4b18-a38f-69a32e08a9ae",
   "metadata": {},
   "source": [
    "> Plot the mean cross-validation accuracies on the final epoch for different numbers of hidden-layer neurons using a scatter plot. Limit the search space of the number of neurons to {64, 128, 256}. Continue using 5-fold cross validation on training dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ced740-f749-4312-be7b-f747d3eea0c1",
   "metadata": {},
   "source": [
    "1. Perform hyperparameter tuning for the different neurons with 5-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a044015-bf37-4673-8456-e20dfe0b52c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad9f217-fa68-4a7f-bb37-dcaaba438bd1",
   "metadata": {},
   "source": [
    "2. Plot the mean cross-validation accuracies on the final epoch for different numbers of hidden-layer neurons using a scatter plot. Limit the search space of the number of neurons to {64, 128, 256}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6920ca9-2886-432b-914d-65f6e5d7f128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5787c1e-ec8f-43eb-9809-ff46ecf7077b",
   "metadata": {},
   "source": [
    "> Select the optimal number of neurons for the hidden layer. State the rationale for your selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3916a7ad-f9fd-4a0f-acf6-aa33f804e42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2a1270-5499-45fe-a28c-4f3416336eeb",
   "metadata": {},
   "source": [
    "\\# TODO: \\<Enter your answer here\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e382726-ffc0-42bf-8b42-16448c1b9f58",
   "metadata": {},
   "source": [
    "> Plot the train and test accuracies against training epochs with the optimal number of neurons using a line plot.\n",
    "Note: use this optimal number of neurons for the rest of the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3adb95-dbeb-486c-a481-321eb166351d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5a1393-12e7-4820-a4aa-a2d6f765cf3d",
   "metadata": {},
   "source": [
    "Part A, Q4 (10 marks)\n",
    "---\n",
    "In this section, we will understand the utility of such a neural network for a test audio. \n",
    "\n",
    "Do a model prediction on the sample test audio and obtain the predicted label using a threshold of 0.5. The model used is the optimized pretrained model using the selected optimal batch size and optimal number of neurons. \n",
    "Find the most important features on the model prediction for the test sample using SHAP. Plot the local feature importance with a force plot and explain your observations.  (Refer to the documentation and these three useful references:\n",
    "https://christophm.github.io/interpretable-ml-book/shap.html#examples-5, \n",
    "https://towardsdatascience.com/deep-learning-model-interpretation-using-shap-a21786e91d16,  \n",
    "https://medium.com/mlearning-ai/shap-force-plots-for-classification-d30be430e195) \n",
    "\n",
    "To reduce repeated code, you may need to import the network (MLP defined in QA1) from **common_utils.py**. You will not be repenalised for any error in QA1 here as the code in QA1 will not be remarked. The following code cell will not be marked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b701b7fa-7f8b-40ed-8bff-a0f315781c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989a8de7-7394-4c9a-a7f4-c9dd9e818455",
   "metadata": {},
   "source": [
    "> Install and import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a769f3e-8748-4385-a2ce-e190fb2c32f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfffe05d-9e17-4329-8009-c4cdb94595ff",
   "metadata": {},
   "source": [
    "> Preprocess 'audio_test.wav' using the function 'extract_features' in common_utils.py. Please make sure the features are stored in a pandas dataframe, using variable name 'df', and fill the size of 'df' in 'size_row' and 'size_column'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc4e942-811e-4878-9842-5bfa0af86f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.display.Audio(\"./audio_test.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c8df59-5e09-41cf-ba25-22144ee676af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enter your code here\n",
    "df = \n",
    "size_row = \n",
    "size_column = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fc2c3a-7eb5-474c-a6a4-5d288b470955",
   "metadata": {},
   "source": [
    "> Do a model prediction on the sample test audio and obtain the predicted label using a threshold of 0.5. The model used is the optimized pretrained model using the selected optimal batch size and optimal number of neurons. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda1f722-5bea-4127-b9ab-1a03782d77a4",
   "metadata": {},
   "source": [
    " 1.  Preprocess to obtain the test data, save the test data as numpy array, print the shape of the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6add5c3c-6bf4-48da-9600-b66de1eb74f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4a9090-0f9c-4e19-b609-37e46bbc4a31",
   "metadata": {},
   "source": [
    "2. Do a model prediction on the sample test audio and obtain the predicted label using a threshold of 0.5. The model used is the optimized pretrained model using the selected optimal batch size and optimal number of neurons. Note: Please define the variable of your final predicted label as 'pred_label'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fdd28d-7662-4f75-8430-68e3868faf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enter your code here\n",
    "\n",
    "print(pred_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca46f49-8dc9-423c-aadf-56c0a60a740b",
   "metadata": {},
   "source": [
    "> Find the most important features on the model prediction for your test sample using SHAP. Create an instance of the DeepSHAP which is called DeepExplainer using traianing dataset: https://shap-lrjball.readthedocs.io/en/latest/generated/shap.DeepExplainer.html.\n",
    "\n",
    "Plot the local feature importance with a force plot and explain your observations.  (Refer to the documentation and these three useful references:\n",
    "https://christophm.github.io/interpretable-ml-book/shap.html#examples-5, \n",
    "https://towardsdatascience.com/deep-learning-model-interpretation-using-shap-a21786e91d16,  \n",
    "https://medium.com/mlearning-ai/shap-force-plots-for-classification-d30be430e195) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03406410-0615-4c8d-ba89-5b430019516c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Fit the explainer on a subset of the data (you can try all but then gets slower)\n",
    "Return approximate SHAP values for the model applied to the data given by X.\n",
    "Plot the local feature importance with a force plot and explain your observations.\n",
    "'''\n",
    "# TODO: Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696dc88a-5095-4554-9523-03a7614606e6",
   "metadata": {},
   "source": [
    "\\# TODO: \\<Enter your answer here\\>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
